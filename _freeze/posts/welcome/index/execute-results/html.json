{
  "hash": "f4ed0d89d84f781c832a0077e08b9665",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Pulsar Star Classification\"\nauthor: \"Arnab Das\"\ndate: \"2021-05-1\"\ncategories: [R]\noutput:\n  html_document:\n    css: styles.css\nformat:\n  html:\n    code-fold: true\n    code-summary: \"Show the code\"\n    code-tools: true\n    code-block-bg: true\n    code-block-border-left: \"#31BAE9\"\n    toc: true\n    toc-depth: 3\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(kknn)\nlibrary(cowplot)\nlibrary(rsample)\nlibrary(themis)\nlibrary(parsnip)\nlibrary(tune)\nlibrary(workflows)\nlibrary(yardstick)\n```\n:::\n\n\n## Introduction\n\nNeutron stars are high-density celestial bodies caused by a large mass star collapsing on itself due to its own gravity (Goldberger, 2019). Pulsars are a classification of neutron stars which \"pulse\" or emit electromagnetic radiation periodically because of its high rotational velocity and strong magnetic fields. Telescopes on Earth can measure the continuous pulses of radio frequencies emitted by pulsars (Rodriguez, 2019). Identifying pulsar signals among interference and noise can be difficult, so machine learning can be used to predict target pulsars based on their recorded metrics.\n\nBecause the pulsar produces periodic pulsation signals they are often too weak to detect (Liu, 2017). Preprocessing this time series by folding the data with respect to the rotational period of the pulsar, yields a unique and distinguishable pulsar fingerprint referred to as an integrated pulse profile. The profile shape has an increased signal quality and form, and through long term observations are considered generally stable, as opposed to the highly variable single pulses (Liu, 2017).\n\nHowever, before signals reach earth, they must pass through interstellar medium (ISM). This medium consists of everything that could possibly exist between a pulsar and the Earth (Liu, 2017). There are several major effects on pulsar radio signals, namely dispersion, scintillation, and scattering (Grootjans 2016), but for the purpose of this report only dispersion will be explored. The pulsar signal can be considered a plane wave whose frequency depends on interactions between ionized components of the ISM (Grootjans, 2016).The frequency dependence of these waves causes higher frequency waves to arrive earlier and lower frequency waves to arrive later, by an amount defined by a dispersion measure (DM) (Liu, 2017). The DM of any frequency is also proportional to the distance of the pulsar. All these factors result in the broadening and reducing the peak values for the frequency distribution curve of a pulsar. When a sufficient amount of dispersion is present in the signal, a signal-to-noise ratio (SNR) preprocessing (taking into consideration antenna aperture and other properties relevant to noise) must be applied to further analyze the pulsar curve by fitting the data (Grootjans, 2016). By applying this filter, a DM-SNR curve can be produced which has removed dispersion distortions in pulse shapes.\n\nIn the \"Predicted Pulsar Star\" Kaggle dataset, an integrated profile and a DM-SNR curve is considered. These two preprocessed measurements have tabulated results for the following statistics: mean, standard deviation, excess kurtosis, and skewness of these measurements. Excess kurtosis describes the tails on a normal distribution to measure the probability of events occurring outside of the normal range, unlike skewness which refers to the symmetry of a normal distribution. In our dataset, our target_class has been identified by binary classification, where a value of 1 represents a pulsar, and the value of 0 represents a non-pulsar.\n\nIn this project we will explore how a star's integrated and DM-SNR profiles affect the accuracy of K-Nearest Neighbors classification models based on these two profiles. The question we aim to answer is: can DM-SNR curve and integrated profile measurements predict pulsar stars? If so, which preprocessing technique yields the most accurate model? We hypothesize that the model built using DM-SNR curve measurements will yield the most accurate model, because it takes into account the noise of space, which is a common source of error when star signals are recorded.\n\n## Methods and Results\n\nWe'll start by downloading the data set and reading it into a dataframe.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load training data from Kaggle\npulsar_data <- read_csv(\"pulsar_data_train.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Update column names to be more 'code-friendly'\ncolnames(pulsar_data) <- c(\"mean_integrated\", \"stdev_integrated\", \"kurtosis_integrated\", \"skew_integrated\", \"mean_DMSNR\", \"stdev_DMSNR\", \"kurtosis_DMSNR\", \"skew_DMSNR\", \"class\")\n# Covert target class to factor\npulsar_data <- pulsar_data %>% mutate(class=as_factor(class))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load testing file from Kaggle\npulsar_test_unclass <- read_csv(\"pulsar_data_test.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Update column names to be more 'code friendly'\ncolnames(pulsar_test_unclass) <- c(\"mean_integrated\", \"stdev_integrated\", \"kurtosis_integrated\", \"skew_integrated\", \"mean_DMSNR\", \"stdev_DMSNR\", \"kurtosis_DMSNR\", \"skew_DMSNR\", \"class\")\nsummary(pulsar_test_unclass)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n mean_integrated  stdev_integrated kurtosis_integrated skew_integrated  \n Min.   :  6.18   Min.   :24.79    Min.   :-1.8760     Min.   :-1.7647  \n 1st Qu.:101.04   1st Qu.:42.41    1st Qu.: 0.0306     1st Qu.:-0.1896  \n Median :114.76   Median :47.03    Median : 0.2273     Median : 0.1865  \n Mean   :111.17   Mean   :46.62    Mean   : 0.4837     Mean   : 1.7513  \n 3rd Qu.:127.02   3rd Qu.:51.13    3rd Qu.: 0.4751     3rd Qu.: 0.9188  \n Max.   :192.62   Max.   :98.78    Max.   : 7.6084     Max.   :65.3860  \n                                   NA's   :767                          \n   mean_DMSNR        stdev_DMSNR     kurtosis_DMSNR     skew_DMSNR      \n Min.   :  0.2132   Min.   :  7.37   Min.   :-2.722   Min.   :  -1.965  \n 1st Qu.:  1.9565   1st Qu.: 14.56   1st Qu.: 5.700   1st Qu.:  33.817  \n Median :  2.8307   Median : 18.55   Median : 8.384   Median :  81.392  \n Mean   : 12.4736   Mean   : 26.43   Mean   : 8.234   Mean   : 102.869  \n 3rd Qu.:  5.5903   3rd Qu.: 28.68   3rd Qu.:10.632   3rd Qu.: 136.893  \n Max.   :223.3921   Max.   :109.71   Max.   :34.540   Max.   :1191.001  \n                    NA's   :524                       NA's   :244       \n  class        \n Mode:logical  \n NA's:5370     \n               \n               \n               \n               \n               \n```\n\n\n:::\n:::\n\n\nWe can see that the test dataset doesn't have any target classes. So, this dataset is not useful to us in training/testing our model, and we will need to split the training dataset.\n\nWe will split the pulsar data set into training and testing sets. The testing set will not be touched until the model is ready for evaluation. We are splitting the data into 75% training and 25% testing because the dataset is sufficiently large (\\~9,000 observations) that leaving 25% of the data for testing is enough to accurately assess the performance of our model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split into training and testing set\nset.seed(111)\n# Determine how many rows have missing values.\nnum_rows_missing_values <- sum(apply(pulsar_data, 1, anyNA))\n# Number of rows with missing values: 3183\n# For our predictions down the road, we need to remove rows with NA values from the test set\npulsar_omit_na <- na.omit(pulsar_data) #this is so our KNN model will run properly later\npulsar_split <- initial_split(pulsar_omit_na, prop = 0.75, strata = class)\npulsar_train <- training(pulsar_split)\npulsar_test <- testing(pulsar_split)\n```\n:::\n\n\n### Exploratory Analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(pulsar_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 9\n  mean_integrated stdev_integrated kurtosis_integrated skew_integrated\n            <dbl>            <dbl>               <dbl>           <dbl>\n1           121.              48.4              0.375          -0.0132\n2            77.0             36.2              0.713           3.39  \n3           131.              53.2              0.133          -0.297 \n4           109.              55.9              0.565           0.0562\n5            95.0             40.2              0.348           1.15  \n6           130.              46.4             -0.0466         -0.0345\n# ℹ 5 more variables: mean_DMSNR <dbl>, stdev_DMSNR <dbl>,\n#   kurtosis_DMSNR <dbl>, skew_DMSNR <dbl>, class <fct>\n```\n\n\n:::\n:::\n\n\nTable 1: Raw pulsar data (training set)\n\nThere are eight predictive variables and one target variable. The predictors are composed of two groups of readings: the integrated profile and the DMSNR profile. Each group has the same four measurements: mean, standard deviation, kurtosis, and skew. We will use descriptive statistics and visualizations to explore the differences between the two profiles in order to determine which predictors will be most useful for our model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the number and percentage of observations that are pulsar stars.\nnum_obs <- nrow(pulsar_train)\nclass_dist <- pulsar_train %>% group_by(class) %>% summarize(n=n(), percentage=n()/num_obs*100)\nclass_dist\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  class     n percentage\n  <fct> <int>      <dbl>\n1 0      6301      90.6 \n2 1       653       9.39\n```\n\n\n:::\n\n```{.r .cell-code}\n#class 0 (non-pulsar stars): 90.8% of the data\n#class 1 (pulsar stars): 9.2% of the data\n```\n:::\n\n\nTable 2: Frequencies of target classes\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# We will also calculate the mean of the predictors in the dataset.\n#mean of the predictors\npredictor_means <- pulsar_train %>% select(-class) %>% map_df(~mean(., na.rm=TRUE))\npredictor_means\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 8\n  mean_integrated stdev_integrated kurtosis_integrated skew_integrated\n            <dbl>            <dbl>               <dbl>           <dbl>\n1            111.             46.5               0.482            1.80\n# ℹ 4 more variables: mean_DMSNR <dbl>, stdev_DMSNR <dbl>,\n#   kurtosis_DMSNR <dbl>, skew_DMSNR <dbl>\n```\n\n\n:::\n\n```{.r .cell-code}\n#now, let's omit rows with NA values.\npulsar_train <- na.omit(pulsar_train)\n```\n:::\n\n\nTable 3: Mean of predictor variables\n\nThe predictors' scale varies significantly, as evidenced by the calculated means. Since the k-nn classification algorithm is particularly sensitive to predictors with larger values, we need to scale and center the data before building the model. This ensures that predictors with larger values (e.g. mean of the integrated profile) don't have a disproportionate effect on the classification model.\n\n### Data Visualization\n\nVisualizing the data can give us an idea of the distribution of predictors, and may give us an indication of which predictors will be most useful in the creation of our model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_long <- pulsar_train %>%\npivot_longer(cols = -c(class, mean_integrated, stdev_integrated, kurtosis_integrated, skew_integrated), names_to = \"Variable\", values_to = \"Value\")\ndata_long$Variable <- str_replace(data_long$Variable, \"_DMSNR\", \"\")\n\noptions(repr.plot.width = 15, repr.plot.height = 10)\nintdensity_plot <- ggplot(data_long, aes(x = Value, fill = Variable)) +\n  geom_density(alpha = 0.7) +\n  facet_wrap(~ Variable, scales = \"free\", nrow = 2, ncol = 2) +\n  theme_minimal() +\n  ggtitle(\"DMSNR\") +\n  theme(text = element_text(size = 15),\n       axis.title.x = element_text(hjust = 1)) +\n  guides(fill = \"none\") +\n  scale_fill_manual(values = c(\"#e64186\", \"#672e8c\", \"#2d3569\", \"#c68dd8\"))\n                                                        \ndata_long1 <- pulsar_train %>%\n  pivot_longer(cols = -c(class, mean_DMSNR, stdev_DMSNR, kurtosis_DMSNR, skew_DMSNR), names_to = \"Variable\", values_to = \"Value\")\ndata_long1$Variable <- str_replace(data_long1$Variable, \"_integrated\", \"\")\n\nDMSNRdensity_plot <- ggplot(data_long1, aes(x = Value, fill = Variable)) +\n  geom_density(alpha = 0.7) +\n  facet_wrap(~ Variable, scales = \"free\", nrow = 2, ncol = 2) +\n  theme_minimal() +\n  ggtitle(\"Integrated\") +\n  theme(text = element_text(size = 15)) +\n  ylab(\"\") +\n  xlab(\"\") +\n  scale_fill_manual(values = c(\"#e64186\", \"#672e8c\", \"#2d3569\", \"#c68dd8\"), labels = c(\"Kurtosis\", \"Mean\", \"Skewness\", \"Standard Deviation\"))\n\ncombine_plot <- plot_grid(intdensity_plot, DMSNRdensity_plot, nrow = 1, ncol = 2)\nplot_grid(ggdraw() + draw_label(\"Density plots\", fontface='bold', size = 25), combine_plot, ncol=1, rel_heights=c(0.1, 1))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# To visualize the distribution of pulsar vs non-pulsar stars, we can use a bar chart.\nclass_plot <- subset(class_dist, !is.na(class)) %>% ggplot(aes(x=class, y=n, fill=class)) +\n  geom_bar(stat=\"identity\") +\n  ylab(\"Count\") +\n  xlab(\"Class\")+\n  labs(title=\"Frequencies of target classes\") +\n  theme_classic()+\n  theme(text=element_text(size=20)) +\n  scale_fill_brewer(palette = \"Purples\", labels = c(\"0\" = \"non-pulsar\", \"1\" = \"pulsar\"))\nclass_plot\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nThere are significantly more observations in the non-pulsar (0) than pulsar (1) class. Imbalanced data can result in the model inappropriately preferring the more common class. In order to account for this imbalance, we will need to upsample the pulsar class to produce an accurate model (see methods for more details on how this is done).\n\nTo visualize the relationship between predictors and the target class, we can create violin plots of each predictor variable stratified by target class. Unlike boxplots, violin plots allow us to visualize both the distribution and density of the underlying data. This additional detail about the data will help with our intuition when it comes to selecting predictors for our model.\n\nUsing those plots, we can see which predictors vary most between the two target classes, and between integrated and DM-SNR.\n\nThe data needs to be tidied before the violin plots can be created. Specifically, we will be moving the values of each of the predictors from their own columns into key-value pairs using the gather function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_stars <- pulsar_train %>%\ngather(key=profile, value=value, mean_integrated, stdev_integrated, kurtosis_integrated, skew_integrated,\nmean_DMSNR, stdev_DMSNR, kurtosis_DMSNR, skew_DMSNR)\nhead(tidy_stars)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  class profile         value\n  <fct> <chr>           <dbl>\n1 0     mean_integrated 121. \n2 0     mean_integrated  77.0\n3 0     mean_integrated 131. \n4 0     mean_integrated 109. \n5 0     mean_integrated  95.0\n6 0     mean_integrated 130. \n```\n\n\n:::\n:::\n\n\nTable 4: Tidied pulsar data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  options(repr.plot.width= 15, repr.plot.height = 10)\n  # make violin plots comparing DM-SNR Curve and Integrated Profile variables, to see how they differ\n  # between our target classes\n  # tidy the data to generate a profile column\n\n# mean violin\nmean_violin <-  tidy_stars %>%\n  filter(profile==\"mean_integrated\"|profile==\"mean_DMSNR\")%>%\n  ggplot(aes(x=profile, y=value), trim=TRUE)+\n  geom_violin(aes(fill=class), draw_quantiles=c(0.5), colour = \"red\")+\n  xlab(\"Target class\") +\n  ylab(\"Mean Value\")+\n  labs(fill=\"Class\", title=\"Mean\")+\n  theme_minimal()+\n  theme(text=element_text(size=20)) +\n  scale_fill_brewer(palette = \"Reds\", labels = c(\"0\" = \"non-pulsar\", \"1\" = \"pulsar\"))\n\n# sd violin\n  sd_violin <-  tidy_stars %>%\n  filter(profile==\"stdev_integrated\"|profile==\"stdev_DMSNR\")%>%\n  ggplot(aes(x=profile, y=value), trim=TRUE)+\n  geom_violin(aes(fill=class), draw_quantiles=c(0.5), colour = \"purple\")+\n  xlab(\"Profile\") +\n  ylab(\"SD Value\")+\n  labs(fill=\"Class\", title=\"Standard Deviation\")+\n  theme_minimal()+\n  theme(text=element_text(size=20)) +\n  scale_fill_brewer(palette = \"PuRd\", labels = c(\"0\" = \"non-pulsar\", \"1\" = \"pulsar\"))\n\n# kurtosis violin\nkurtosis_violin <-  tidy_stars %>%\n  filter(profile==\"kurtosis_integrated\"|profile==\"kurtosis_DMSNR\")%>%\n    ggplot(aes(x=profile, y=value))+\n  geom_violin(aes(fill=class), draw_quantiles=c(0.5), colour = \"blue\")+\n  xlab(\"Profile\") +\n  ylab(\"Kurtosis Value\")+\n  labs(fill=\"Class\", title=\"Kurtosis\")+\n  theme_minimal()+\n  theme(text=element_text(size=20)) +\n  scale_fill_brewer(palette = \"Blues\", labels = c(\"0\" = \"non-pulsar\", \"1\" = \"pulsar\"))\n\n# skew_violin\nskew_violin <-  tidy_stars %>%\n  filter(profile==\"skew_integrated\"|profile==\"skew_DMSNR\")%>%\n  ggplot(aes(x=profile, y=value), draw_quantiles=c(0.5))+\n  geom_violin(aes(fill=class), colour = \"darkgreen\")+\n  xlab(\"Profile\") +\n  ylab(\"Skew Value\")+\n  labs(fill=\"Class\", title=\"Skew\")+\n  theme_minimal()+\n  theme(text=element_text(size=20)) +\n  scale_fill_brewer(palette = \"Greens\", labels = c(\"0\" = \"non-pulsar\", \"1\" = \"pulsar\"))\n\ncombined_plot <- plot_grid(mean_violin, sd_violin, kurtosis_violin, skew_violin, nrow = 2, ncol = 2)\nplot_grid(ggdraw() + draw_label(\"Class Across Key Factors\", fontface='bold', size = 25), combined_plot, ncol=1, rel_heights=c(0.1, 1))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nFigures 2-5 are violin plots of the four types of measurements that were collected (mean, standard deviation, kurtosis, and skew), stratified by target class and profile (integrated vs DMSNR). These plots allow us to determine which predictors will be most useful for building our classification model.\n\nFirstly, it demonstrates how the predictors vary between the two target classes. A predictor whose distribution varies significantly between the target classes (for example, in Figure 3), is likely to be more useful for our classification model than a predictor with very similar distributions.\n\nSecondly, these plots demonstrate the difference between the integrated and DMSNR profiles. We can see which profile varies more between the two target classes, thus allowing us to predict which profile will be more useful for building classification models.\n\nWe can see that while each predictor is extremely different depending on whether it is from the integrated or DM-SNR profile, all groups show a difference in the distribution and mean of the target classes within the same variable. So, each would be a useful addition to their respective models. As well, from a broader perspective, we want to use all of the scientific data available to us, since it was painstakingly collected and cleaned, and omission of a particular variable without reasonable grounds could cause us to lose accuracy in our models.\n\n### K-Nearest Neighbors Analysis\n\nAfter our preliminary analysis, our next step will be to use our integrated and DM-SNR variables to predict pulsar stars using the K-Nearest Neighbors classification algorithm.\n\nThis will help us determine whether one of these two recording techniques is better than the other at distinguishing pulsar stars from the noise of space.\n\nOur first step is to set up our KNN model, using the integrated variables. We will do this in several steps. 1. Make a recipe. The recipe will preprocess our data by centering and scaling our predictors (since the ranges of our predictors differ), and upsample our target class (since it is \\~90% target class 0). 2. Make a KNN spec, to tell the computer to run KNN (and with what parameters). This will include the neighbors=tune() function so we can find the value of K that gives us the highest accuracy. 3. Make our cross validation sets, so we can test our tuning model and find accuracies of different K's. 4. Make a tuning workflow, to tune the KNN model and find the value of K giving the highest accuracy.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#1. First, we make our recipe, which will preprocess our data.\nrecipe_integrated <- recipe(class ~ mean_integrated + stdev_integrated + kurtosis_integrated +\nskew_integrated, data = pulsar_train) %>%\n    step_scale(all_predictors()) %>%\n    step_center(all_predictors())%>%\n    step_upsample(class, over_ratio=1, skip=TRUE)\n\nrecipe_integrated\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#2. Next, we will create our model, so the computer knows how to run the KNN\nknn_tune <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) %>%\n            set_engine(\"kknn\") %>%\n            set_mode(\"classification\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#3. next, we make our cross-validation set to test iterations of K\nvfold <- vfold_cv(pulsar_train, v = 5, strata = class)\n\n#4. now, we make a workflow to find the best value of K, to give our training model the best accuracy.\nknn_fit <- workflow() %>%\n        add_recipe(recipe_integrated) %>%\n        add_model(knn_tune) %>%\n        tune_grid(resamples = vfold, grid = 20) %>%\n        collect_metrics()\n        #note: grid=20 to test 20 values of K, rather than the usual 10\n        #This is so we can confidently find the value of K with the highest accuracy by being more rigorous with our testing\n```\n:::\n\n\nNow that our model has been tuned, we can pull the accuracy of our tuning model for each iteration of K, and plot it to show how accuracy changes depending on K.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#pull accuracies\naccuracies <- knn_fit %>%\n       filter(.metric == \"accuracy\")\n#plot accuracy against K\ncross_val_plot <- ggplot(accuracies, aes(x = neighbors , y = mean)) +\n       geom_point() +\n       geom_line() +\n       labs(x = \"Neighbours\", y = \"Accuracy Estimate\", title = \"Integrated profile model accuracy\")  +\n       theme(text = element_text(size = 20))\n\ncross_val_plot\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nAccording to the plot above, the value of K that gives the highest accuracy is K=2. We will now make another model that uses this value of K to predict the class of a testing set. This will require us to make a new KNN spec and workflow- but we can reuse our old recipe, since it isn't affected by choosing K.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#pull the value of K giving the highest accuracy (2)\nkn_integrated <- knn_fit %>%\n       filter(.metric == \"accuracy\") %>%\n       arrange(desc(mean)) %>%\n       head(1) %>%\n       pull(neighbors)\nkn_integrated\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#make another KNN spec, this time with neighbors= our chosen value of K\nint_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = kn_integrated) %>%\n              set_engine(\"kknn\") %>%\n              set_mode(\"classification\")\n#make the workflow to build our final model\nint_fit <- workflow() %>%\n             add_recipe(recipe_integrated) %>%\n             add_model(int_spec) %>%\n             fit(data = pulsar_train)\n```\n:::\n\n\nNow that we've build our final model, we can test it using data it hasn't seen before, i.e. our testing set. This will tell us its overall accuracy in predicting the class of an unknown star, and how good it is at predicting pulsar stars specifically. Once we have predicted the testing set, we can gather its metrics to see its accuracy, and plot it as a confusion matrix to see how many times it predicted each class correctly, and incorrectly.\n\nTable 5: Integrated profile model metrics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#predict the testing set using our integrated model\nint_pred <- predict(int_fit, pulsar_test) %>% bind_cols(pulsar_test)\n#get the accuracy of our model at predicting the test class\nint_metrics <- int_pred %>% metrics(truth = class, estimate=.pred_class)\n\n#make a confusion matrix\nint_mat <- int_pred %>%\n      conf_mat(truth = class, estimate = .pred_class)\nint_mat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction    0    1\n         0 2071   36\n         1   51  161\n```\n\n\n:::\n\n```{.r .cell-code}\ntidy_int_mat <- tidy(int_mat) #this is for later\n```\n:::\n\n\nTable 6: Confusion matrix for the integrated profile model\n\nWe are finished with our integrated model, and can begin making our DM-SNR model. This will follow the same steps as above. Firstly, we will set up a recipe to tune the value of K to that which gives the highest accuracy of our training model. To do this, we will: 1. Make a new recipe. 2. Reuse our old KNN spec (since it doesn't depend on our variables) 3. Reuse our old cross validation sets (since they don't depend on our variables) 4. Make a tuning workflow, to find the best value of K.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#make the recipe\nrecipe_DMSNR <- recipe(class ~ mean_DMSNR + stdev_DMSNR + kurtosis_DMSNR +\nskew_DMSNR, data = pulsar_train) %>%\n    step_scale(all_predictors()) %>%\n    step_center(all_predictors())%>%\n    step_upsample(class, over_ratio=1, skip=TRUE)\n\n#reuse KNN spec and cross validation sets\n#make our tuning workflow\nknn_fit <- workflow() %>%\n        add_recipe(recipe_DMSNR) %>%\n        add_model(knn_tune) %>%\n        tune_grid(resamples = vfold, grid = 20) %>% #note: grid=20 to test 20 values of K, rather than the usual 10\n        collect_metrics()\n```\n:::\n\n\nWith our DM-SNR tuning workflow, we can pull the accuracies of each K in our tuning model, and plot them to see how accuracy depends on K.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#pull accuracies\naccuracies <- knn_fit %>%\n       filter(.metric == \"accuracy\")\n#make the plot\ncross_val_plot <- ggplot(accuracies, aes(x = neighbors , y = mean)) +\n       geom_point() +\n       geom_line() +\n       labs(x = \"Neighbours\", y = \"Accuracy Estimate\", title = \"DM-SNR profile model accuracy\")  +\n       theme(text = element_text(size = 20))\n\ncross_val_plot\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\nThe plot tells that the value of K giving the highest accuracy of our training model is K=1. We will pull that value, and use it to build a new model to predict our testing set (and other unknown stars). We can reuse our old recipe and build a new KNN spec for our best value of K (value of K giving the highest accuracy), and a new workflow for our final DM-SNR model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkn_DMSNR <- knn_fit %>%\n       filter(.metric == \"accuracy\") %>%\n       arrange(desc(mean)) %>%\n       head(1) %>%\n       pull(neighbors)\n\n#make new KNN spec with best value of K\nDMSNR_int_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = kn_DMSNR) %>%\n              set_engine(\"kknn\") %>%\n              set_mode(\"classification\")\n#make final workflow\nDMSNR_int_fit <- workflow() %>%\n             add_recipe(recipe_DMSNR) %>%\n             add_model(DMSNR_int_spec) %>%\n             fit(data = pulsar_train)\n```\n:::\n\n\nNow that our final DM-SNR model is built, we will test it on data it hasn't seen before (the testing set) to find its accuracy. We can pull the overall accuracy of the model using the metrics() function, and use a confusion matrix to show how many times our model correctly and incorrectly identified the class of the stars by comparing their true and predicted class.\n\nTable 7: DM-SNR model metrics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Predict the testing data\nDMSNR_int_pred <- predict(DMSNR_int_fit, pulsar_test) %>%\n                    bind_cols(pulsar_test)\n#get the accuracy of our model at predicting the classes of the test data\nDMSNR_int_metrics <- DMSNR_int_pred %>%\nmetrics(truth = class, estimate=.pred_class)\n\n#make a confusion matrix\nDMSNR_mat <- DMSNR_int_pred %>%\n      conf_mat(truth = class, estimate = .pred_class)\nDMSNR_mat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction    0    1\n         0 2038   88\n         1   84  109\n```\n\n\n:::\n\n```{.r .cell-code}\ntidy_DMSNR_mat <- tidy(DMSNR_mat)#this is for later\n```\n:::\n\n\nTable 8: DM-SNR model confusion matrix\n\nFinally, to display our data we will make a grouped bar chart, showing both the overall accuracy and the accuracy of predicting pulsar stars (the true positive rate). It is important for us to look at overall accuracy and pulsar star classification accuracy because while we do want a model that has a high overall accuracy, our goal is to find the model that is the best at predicting pular stars (not just non-pulsars), so we need to compare both metrics of our models. We will also calculate a false positive rate, as the goal of our project is to correctly identify pulsar stars - so while a false negative (classifying a pulsar as a non-pulsar star) is not ideal, it is not as detrimental as false positives (classifying a non-pulsar as a pulsar) as this affects our ability to gain accurate knowledge on pulsar stars using our model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#gather the values from our accuracy estimates\n#gather overall accuracy estimates from our two models\nDMSNR_accuracy <- as.numeric(DMSNR_int_metrics[1, 3])*100 #92.3%\nintegrated_accuracy <- as.numeric(int_metrics[1, 3])*100 #96.8%\n\n#gather accuracy estimates of pulsar stars only from our confusian matrices\n#this is correctly identified pulsars/ total number of true pulsars *100\npulsar_integrated <- as.numeric(tidy_int_mat[4,2])/\n(as.numeric(tidy_int_mat[3,2])+as.numeric(tidy_int_mat[4,2]))*100\npulsar_integrated #77.5%\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 81.72589\n```\n\n\n:::\n\n```{.r .cell-code}\npulsar_DMSNR <- as.numeric(tidy_DMSNR_mat[4,2])/\n(as.numeric(tidy_DMSNR_mat[3,2])+as.numeric(tidy_DMSNR_mat[4,2]))*100\npulsar_DMSNR #58.0%\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 55.32995\n```\n\n\n:::\n\n```{.r .cell-code}\n#gather accuracy estimates of pulsar stars from confusion matrix\n#calculate false positive rate\n#this is false positive rate (0's classified as 1)/total negatives (all 0's) *100\nintegrated_falsepos <- as.numeric(tidy_int_mat[2, 2])/\n(as.numeric(tidy_int_mat[1, 2])+as.numeric(tidy_int_mat[2, 2]))*100\nintegrated_falsepos #1.2%\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.403393\n```\n\n\n:::\n\n```{.r .cell-code}\nDMSNR_falsepos <- as.numeric(tidy_DMSNR_mat[2,2])/\n((as.numeric(tidy_DMSNR_mat[1,2])+as.numeric(tidy_DMSNR_mat[2,2])))*100\nDMSNR_falsepos #4.2%\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.95853\n```\n\n\n:::\n:::\n\n\nNow we can plot our accuracy estimates as a grouped bar chart.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#make a new dataframe for our plot\naccuracies <- data.frame(\"percent_accuracy\"=c(DMSNR_accuracy, integrated_accuracy, pulsar_integrated, pulsar_DMSNR))\nnames <- data.frame(\"type\"=c(\"DMSNR\", \"Integrated\", \"Integrated\", \"DMSNR\"))\nsubgroup <- data.frame(\"subgroup\"=c(\"Overall Accuracy\", \"Overall Accuracy\", \"Pulsar Identification Accuracy\", \"Pulsar Identification Accuracy\"))\naccuracy_table <- bind_cols(names, accuracies, subgroup)\n\n#make the plot\naccuracy_plot <- ggplot(accuracy_table, aes(x=type, y=percent_accuracy, fill=subgroup))+\ngeom_bar(aes(fill=subgroup),stat=\"identity\", position=\"dodge\")+\n  labs(x=\"Type of Star Recording\", y=\"Percent Accuracy of KNN Model\", fill=\"Accuracy Type\", title=\"Overall and true positive accuracy\")+\n  theme_classic()+\ntheme(text = element_text(size=20)) +\nscale_fill_brewer(palette = \"BuGn\")\n\n\naccuracy_plot\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\nFrom this barchart, we observe that the overall accuracy and accuracy of pulsar star identification differs between our two models. In both, the integrated model shows higher accuracy.\n\n## Discussion\n\nWe created two models using the provided metrics for the integrated and DM-SNR profiles. The first model using integrated profile predictors yielded an accuracy of 96% for all predicted classes. The second model using the DM-SNR curve predictors had a lower accuracy at 91% for all predicted classes. In addition to better overall accuracy, the integrated curve model was also more accurate when predicting true positives (i.e. correctly identifying pulsar stars). Out of 219 pulsar stars in our data set, the integrated curve model correctly identified 178 (78%), compared with the DM-SNR curve model which only identified 127 (58%). Clearly, the integrated model is better than the DM-SNR curve model at predicting pulsar stars from the noise of space. However, considering how rare pulsar stars are an 81% accuracy of our best model may not be good enough if it is to be used in scientific research. To elucidate this, we calculated a false positive rate for each model, as falsely identifying a non-pulsar as a pulsar star affects the quality of future research far more than missing a pulsar star due to a false negative. Our integrated model had a false positive rate of 1.2%, while our DM-SNR model had a false positive rate of 4.2%. So again we see our integrated model is better at distinguishing pulsar stars, would give better prediction accuracy for our key metrics, and is a better choice for use in classifying new stars.\n\nBy predicting the target class for each observation in the testing data set using the integrated model, we accurately differentiated pulsars from non-pulsars for each observation in the testing data set and could predict the class of recently recorded pulsar observations.\n\nContrary to our hypothesis that the DM-SNR curve model would be more accurate, the integrated profile model proved to be better at predicting pulsars. The reasoning for our hypothesis was that the DM-SNR curve would be a better preprocessing method which removed distortions in pulse shapes due to dispersion, in comparison to the integrated pulse profile which preprocessed the time series by folding the data with respect to the rotational period. Analyzing the integrated vs DM-SNR model's accuracies tells us how well these recording methods can identify a pulsar star and the best variables for filtering pulsar stars from the noise of space. We used the integrated model to predict the target class for the observations in the testing dataset, differentiating pulsar stars from non-pulsars. Prediction of pulsars will be beneficial for scientists and astronomers for celestial research. Using pulsar stars, scientists can measure cosmic distances, time and search for planets beyond Earth's solar system. Moreover, it is possible to measure how the presence of massive bodies curves space-time. By observing pulsars, the researchers have shown repeatedly that close double neutron star systems send out strong gravitational waves (Cofield 2016). Helping scientists efficiently and accurately identify pulsar stars will ultimately help advance research areas that depend on pulsar stars.\n\nNow that we have demonstrated a reasonably effective model using the provided data set, the next step would be to test our model out in the real world. This will give us a better idea of how useful our model might actually be to scientists interested in identifying pulsar stars, as the competition data set we used for our model may not be entirely representative of the data typically produced by radio telescopes. We may find that additional cleaning or wrangling is needed in order to deploy our model on real-world data.\n\nAt the end of the data analysis, several questions may arise: - Are either of our models accurate enough for scientific research? - Is another classification model better suited to this data (e.g. Random Forest)? - Which predictors need more scientific advancement until they are as useful as the others? - Our best values of K were quite low. Do we need to omit variables that are adding extra variation into our models? - How accurate are the predicted labels from the model? - Can we further improve the model or method of classification?\n\n## Works cited\n\nCofield,C.(2016, April 22). What Are Pulsars? . https://www.space.com/32661-pulsars.html\n\nGrootjans et al. (2016, August). Detection of Dispersed Pulsars in a Time Series by Using a Matched Filtering Approach. https://essay.utwente.nl/71435/1/GROOTJANS_MA_EWI.pdf\n\nGoldberger, A. (2019, January 04). Classifying pulsar stars using AI techniques. https://medium.com/duke-ai-society-blog/classifying-pulsar-stars-using-ai-techniques-d2be70c0f691\n\nMax Planck Institute, PULSE: The Impact of European Pulsar Science on Modern Physics, https://phys.org/news/2005-12-pulse-impact-european-pulsar-science.html#:\\~:text=By%20observing%20pulsars%2C%20the%20researchers,confirm%20Einstein's%20General%20Relativity%20Theory.\n\n(n.d.). Ggplot2 violin plot : Quick start guide - R software and data visualization. STHDA. http://www.sthda.com/english/wiki/ggplot2-violin-plot-quick-start-guide-r-software-and-data-visualization\n\n(n.d.). Violin Plot. Ggplot 2. https://ggplot2.tidyverse.org/reference/geom_violin.html\n\nRodriguez, F. (2019, October 07). Pulsar Stars Detection. https://datauab.github.io/pulsar_stars/\n\nLiu K., (2017, June 26).Introduction to Pulsar, Pulsar Timing, and measuring of Pulse Time-of-Arrivals. http://ipta.phys.wvu.edu/files/student-week-2017/IPTA2017_KuoLiu_pulsartiming.pdf\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}