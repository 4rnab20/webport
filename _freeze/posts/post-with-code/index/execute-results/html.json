{
  "hash": "45264c40b6e1ea9683e5f46476b70826",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Diabetes Prediction Modeling\"\nauthor: \"Arnab Das\"\ndate: \"2024-04-21\"\ncategories: [R]\noutput:\n  html_document:\n    css: styles.css\nformat:\n  html:\n    code-fold: true\n    code-summary: \"Show the code\"\n    code-tools: true\n    code-block-bg: true\n    code-block-border-left: \"#31BAE9\"\n    toc: true\n    toc-depth: 3\n---\n\n\n## Introduction:\n\nDiabetes is a prevalent chronic metabolic disorder posing significant health and economic burdens globally, particularly with the recent rise in type 2 diabetes cases. Predictive modeling offers a valuable approach for identifying individuals at risk and intervening early. The dataset we analyze in this project exclusively consists of female patients aged 21 years or above, all of whom are of Pima Indian heritage. These demographic constraints ensure a focused examination of diabetes within this specific population subset. Diagnostic measurements crucial for diabetes prediction, including glucose levels, blood pressure, insulin levels, and BMI were collected through medical examinations and tests conducted by healthcare professionals. Our aim is to develop an effective tool for diabetes risk assessment to gain insights into the factors contributing to its onset, ultimately improving health outcomes and quality of life for individuals vulnerable to diabetes.\n\n#### Data Overview:\n\nThe dataset originates from the National Institute of Diabetes and Digestive and Kidney Diseases and is utilized to predict the probability of diabetes diagnosis in female subjects aged 21 and above. There are a total of 768 observations and 9 variables in the dataset. The target variable is `Outcome` which indicates the presence of diabetes. The 8 explanatory variables are: `Pregnancies`, `Glucose`, `BloodPressure`, `Skin Thickness`, `Insulin`, `BMI`, `DiabetesPredigreeFunction` and `Age`. Below are the detailed description of each explanatory varibles:\n\n-   `Pregnancies`: Integer variable indicating the number of pregnancies the individual has experienced.\n-   `Glucose`: Numeric variable representing plasma glucose concentration at 2 hours in an oral glucose tolerance test, measured in mg/dL.\n-   `BloodPressure`: Numeric variable denoting the diastolic blood pressure, measured in mmHg.\n-   `Skin Thickness`: Numeric variable indicating the thickness of the triceps skin fold, measured in mm.\n-   `Insulin`: Numeric variable representing insulin levels in the bloodstream two hours after a specific event (such as the administration of glucose), measured in micro-units per milliliter of serum.\n-   `BMI`: Numeric variable representing Body Mass Index (BMI), a measure of body fat based on height and weight, measured in kg/m\\^2.\n-   `DiabetesPedigreeFunction`: Numeric variable representing a function which scores the likelihood of diabetes based on family history.\n-   `Age`: Integer variable indicating the age of the individual.\n-   `Outcome`: Categorical (binary) variable, where 0 represents absence of diabetes and 1 represents presence of diabetes. This variable is the target variable for prediction.\n\n#### Project Objective:\n\nThe primary objective of this project is to develop a predictive model capable for predicting the probability of a subject having diabetes based on their diagnostic measurements. By variable and model selection, we aim to build a **\"best\"** model for prediction among all candidate models. Through this exploration, we seek to gain insights into the underlying factors contributing to diabetes onset and create a valuable tool for diabetes risk assessment. Further analysis, such as correlation analysis, could contribute to ensuring the reliability and robustness of the observed relationships.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(GGally)\nlibrary(glmnet)\nlibrary(caret)\nlibrary(MASS)\nlibrary(pROC)\nlibrary(cowplot)\n```\n:::\n\n\n#### Loading data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes <- read.csv(\"diabetes.csv\")\nhead(diabetes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Pregnancies Glucose BloodPressure SkinThickness Insulin  BMI\n1           6     148            72            35       0 33.6\n2           1      85            66            29       0 26.6\n3           8     183            64             0       0 23.3\n4           1      89            66            23      94 28.1\n5           0     137            40            35     168 43.1\n6           5     116            74             0       0 25.6\n  DiabetesPedigreeFunction Age Outcome\n1                    0.627  50       1\n2                    0.351  31       0\n3                    0.672  32       1\n4                    0.167  21       0\n5                    2.288  33       1\n6                    0.201  30       0\n```\n\n\n:::\n\n```{.r .cell-code}\nnrow(diabetes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 768\n```\n\n\n:::\n:::\n\n\n#### Removing missing values:\n\nMissing values can introduce bias in parameter estimates and reduce their precision. Upon observing that several attributes in our dataset contain missing values, we opted to clean the data by removing these rows.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes_clean <- diabetes[!(diabetes$Glucose == 0 | diabetes$BloodPressure == 0 | diabetes$SkinThickness == 0 | diabetes$Insulin == 0 | diabetes$BMI == 0 | diabetes$DiabetesPedigreeFunction == 0 | diabetes$Age == 0), ]\nhead(diabetes_clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Pregnancies Glucose BloodPressure SkinThickness Insulin  BMI\n4            1      89            66            23      94 28.1\n5            0     137            40            35     168 43.1\n7            3      78            50            32      88 31.0\n9            2     197            70            45     543 30.5\n14           1     189            60            23     846 30.1\n15           5     166            72            19     175 25.8\n   DiabetesPedigreeFunction Age Outcome\n4                     0.167  21       0\n5                     2.288  33       1\n7                     0.248  26       1\n9                     0.158  53       1\n14                    0.398  59       1\n15                    0.587  51       1\n```\n\n\n:::\n\n```{.r .cell-code}\nnrow(diabetes_clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 392\n```\n\n\n:::\n\n```{.r .cell-code}\noutcome_counts <- table(diabetes_clean$Outcome)\nprint(outcome_counts)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  0   1 \n262 130 \n```\n\n\n:::\n:::\n\n\n#### Post data-cleaning overview:\n\nThe **number of rows** in our dataset after removing the 0 values is **392**. Although the dataset size has decreased, the remaining data still provides sufficient information to explore relationships, trends, and patterns. By excluding rows with unreliable physiological measurements, we ensure the integrity and accuracy of the dataset, allowing for more reliable insights and interpretations from subsequent analyses.\n\nFrom some basic exploratory data analysis we see the dataset contains around **one-third** **positive (1)** outcomes, and **two-thirds** **negative (0)** outomes, they are generally balanced enough. However, it's important to remain vigilant for potential issues related to class imbalance and to employ appropriate techniques if imbalance becomes problematic during analysis.\n\n## Methods and Results:\n\n### Exploratory Data Analysis (EDA):\n\nBefore delving into specifics, it's essential to examine the overall distribution of outcomes across variables. This exploration provides insight into how outcomes vary in response to changes in each variable.\n\n#### Explore the multicollinearity:\n\nAccording to regression assumptions, multicollinearity among explanatory variables should be avoided. If a multicollinearity problem exists in the dataset, the standard errors of estimated coefficients will be inflated, and coefficient estimates will be unstable, making it difficult to determine variable significance. Additionally, the interpretation of coefficients will be misleading. We can explore the correlation matrix for better insights.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# getting correlation values between variables\ncorr_matrix <- diabetes_clean %>%\n  dplyr::select(- Outcome) %>%\n  cor() %>%\n  as.data.frame() %>%\n  rownames_to_column(\"var1\") %>%\n  pivot_longer(-var1, names_to = \"var2\", values_to = \"corr\")\n\n# plotting a correlation matrix\noptions(repr.plot.width = 15, repr.plot.height = 15)\ncorr_matrix %>%\n  ggplot(aes(var1, var2)) +\n  geom_tile(aes(fill = corr), color = \"white\") +\n  scale_fill_distiller(\"Correlation Coefficient \\n\",\n    palette =  \"Spectral\",\n    direction = 1, limits = c(-1,1)\n  ) +\n    theme(\n        axis.text.x = element_text(\n          angle = 45, vjust = 1,\n          size = 18, hjust = 1\n        ),\n        axis.text.y = element_text(\n          vjust = 1,\n          size = 18, hjust = 1\n        ),\n        title = element_text(size = 20, face = \"bold\"),\n        legend.title = element_text(size = 18, face = \"bold\"),\n        legend.text = element_text(size = 20),\n        legend.key.size = unit(2, \"cm\"),\n        text = element_text(size = 20),\n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 25)\n      ) +\n      coord_fixed() +\n      geom_text(aes(var1, var2, label = round(corr, 2)), color = \"black\", size = 6) +\n    labs(title = \"Correlation Matrix\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=1440}\n:::\n:::\n\n\n##### Understanding variables which show correlation:\n\nIn the dataset analysis, several pairs of variables show significant correlations. Notably, `Glucose` and `Insulin` correlate at **0.58**, indicating a regulatory response to blood sugar levels. `Age` and `Pregnancies` exhibit a correlation of **0.68**, reflecting reproductive aging. `BMI` and `SkinThickness` correlate at **0.66**, suggesting a link between body fat and skin thickness. `BloodPressure` and `BMI` show a correlation of **0.30**, indicating a connection between hypertension and obesity. Lastly, `Glucose` and `Age` correlate at **0.34**, potentially indicating age-related changes in glucose metabolism and diabetes risk. The above all shows the potential issue of multicollinearity in the dataset.\n\nConversely, the correlation between other variables appears to be within acceptable ranges, suggesting that they are not significantly affected by multicollinearity. Therefore, we need to addresse multicollinearity issue by some techniques such as variable selection or regularization methods,improving the robustness of the regression model.\n\n#### Distribution of predictors:\n\nThe density plots for the variables in this dataset illustrate the distribution of each variable's values. This visualization helps in understanding the spread, central tendency, and shape of the data for variables such as Pregnancies, Glucose, BloodPressure, Skin Thickness, Insulin, BMI, DiabetesPedigreeFunction, and Age. These plots offer insights into the prevalence and distribution of key factors associated with diabetes diagnosis in the female subjects aged 21 and above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_long <- diabetes_clean %>%\n  pivot_longer(cols = -Outcome, names_to = \"Variable\", values_to = \"Value\")\n\noptions(repr.plot.width = 15, repr.plot.height = 10)\ndensity_plot <- ggplot(data_long, aes(x = Value, fill = Variable)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~ Variable, scales = \"free\", nrow = 2, ncol = 4) +\n  theme_minimal() +\n  ggtitle(\"Density Plots of Factors\") +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\", size = 25),\n        text = element_text(size = 15)) +\n  guides(fill = \"none\")\n\ndensity_plot\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=1440}\n:::\n:::\n\n\n##### Understanding the distributions:\n\nAge skews right, indicating a younger population. Blood Pressure and BMI are normally distributed, representing the population. DiabetesPedigreeFunction and Insulin skew right, with low values prevalent; Glucose is normally distributed; Pregnancies skew right, suggesting fewer are common; Skin Thickness is nearly normal, peaking at lower values. These patterns aid in understanding population demographics and physiological factors influencing diabetes prediction.\n\n#### Observing the relationship between each predictor variable and the outcome:\n\nWe aim to gain insights into the relationship between each explanatory variable and the response variable before conducting regression analysis. Given the binary nature of the response variable, utilizing boxplots to visualize the relationship between each explanatory variable and the response variable offers a convenient approach.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfunction_plot <- ggplot(data = diabetes_clean, aes(x = factor(Outcome), y =DiabetesPedigreeFunction, fill = factor(Outcome))) +\n  geom_boxplot(colour = \"red\") +\n  labs(title = \"DiabetesPedigreeFunction vs. Outcome\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Diabetes Pedigree Function\") + theme(\n    text = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.title = element_text(size = 15)\n  ) +\n  scale_fill_brewer(palette = \"Reds\", labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\"))+\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  theme(text = element_text(size = 15))\n\nSkin_plot <- ggplot(data = diabetes_clean, aes(x = as.factor(Outcome), y =SkinThickness, fill = factor(Outcome))) +\n  geom_boxplot(colour = \"purple\") +\n  labs(title = \"Skin Thickness vs. Outcome\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Skin Thickness\") + theme(\n    text = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.title = element_text(size = 15)\n) +\n  scale_fill_brewer(palette = \"PuRd\", labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\"))+\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  theme(text = element_text(size = 15))\n\nglucose_plot <- ggplot(data = diabetes_clean, aes(x = as.factor(Outcome), y =Glucose, fill = factor(Outcome))) +\n  geom_boxplot(colour = \"blue\") +\n  labs(title = \"Glucose vs. Outcome\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Glucose Level\") + theme(\n    text = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.title = element_text(size = 15)\n  ) +\n  scale_fill_brewer(palette = \"Blues\", labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\"))+\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  theme(text = element_text(size = 15))\n\nbloodPressure_plot <- ggplot(data = diabetes_clean, aes(x = as.factor(Outcome), y =BloodPressure, fill = factor(Outcome))) +\n  geom_boxplot(colour = \"red\") +\n  labs(title = \"Blood Pressure vs. Outcome\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Blood Pressure\") + theme(\n    text = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.title = element_text(size = 15)\n  ) +\n  scale_fill_brewer(palette = \"RdPu\", labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\"))+\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  theme(text = element_text(size = 15))\n\nInsulin_plot <- ggplot(data = diabetes_clean, aes(x = as.factor(Outcome), y =Insulin, fill = factor(Outcome))) +\n  geom_boxplot(colour = \"darkgreen\") +\n  labs(title = \"Insulin vs. Outcome\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Insulin\") + theme(\n    text = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.title = element_text(size = 15)\n  )  +\n  scale_fill_brewer(palette = \"BuGn\", labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\"))+\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  theme(text = element_text(size = 15))\n\nBMI_plot <- ggplot(data = diabetes_clean, aes(x = as.factor(Outcome), y =BMI, fill = factor(Outcome))) +\n  geom_boxplot(colour = \"darkgreen\") +\n  labs(title = \"BMI vs. Outcome\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Body Mass Index\")  + theme(\n    text = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.title = element_text(size = 15)\n  )  +\n  scale_fill_brewer(palette = \"Greens\", labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  theme(text = element_text(size = 15))\n\nAge_plot <- ggplot(data = diabetes_clean, aes(x = as.factor(Outcome), y =Age, fill = factor(Outcome))) +\n  geom_boxplot(colour = \"red\") +\n  labs(title = \"Age vs. Outcome\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Age\")  + theme(\n    text = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.title = element_text(size = 15)\n  )  +\n  scale_fill_brewer(palette = \"OrRd\", labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  theme(text = element_text(size = 15))\n\nPregnancies_plot <- ggplot(data = diabetes_clean, aes(x = as.factor(Outcome), y =Pregnancies, fill = factor(Outcome))) +\n  geom_boxplot(colour = \"DarkBlue\") +\n  labs(title = \"Pregnancies vs. Outcome\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Number of Pregnancies\") + theme(\n    text = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.title = element_text(size = 15)\n  )  +\n  scale_fill_brewer(palette = \"BuPu\", labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  theme(text = element_text(size = 15))\n\ncombined_plot <- plot_grid(function_plot, Skin_plot, glucose_plot, bloodPressure_plot, Insulin_plot, BMI_plot, Age_plot, Pregnancies_plot, nrow = 2, ncol = 4)\n\noptions(repr.plot.width = 20, repr.plot.height = 15)\nplot_grid(ggdraw() + draw_label(\"Diabetes Outcome Across Key Factors\", fontface='bold', size = 25), combined_plot, ncol=1, rel_heights=c(0.1, 1))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=1920}\n:::\n:::\n\n\n##### Conclusion from boxplots:\n\n-   From the eight boxplots above, a notable disparity emerges in the mean glucose levels between individuals with and without diabetes. Specifically, the mean glucose level appears markedly higher among those with diabetes compared to those without, suggesting a positive association between glucose level and diabetes. Given this observation, further investigation into the relationship between glucose level and diabetes outcome is warranted.\n\n-   Moreover, upon inspecting the boxplot depicting diabetes status against age, a similar pattern emerges. Individuals diagnosed with diabetes have a higher mean age compared to those without. Consequently, it can be inferred that both age and glucose level are potentially significant explanatory variables associated with diabetes outcome.\n\n-   Additionally, it's noteworthy that the mean values of other variables exhibit slight variations based on whether individuals have diabetes or not. Specifically, when an individual has diabetes, the mean values of all eight predictor variables are higher compared to when the person doesn't have diabetes, suggesting a potentially positive relationship between each X and Y to some extent.\n\n### Methods (plan):\n\n#### Model selection methods:\n\n-   We will begin with a full model incorporating all eight variables, then use backward selection based on AIC and BIC to refine our model selection. This process yields two models: an AIC-selected model and a BIC-selected model. Backward selection eliminates a non-significant predictor from the model in each interaction, resulting in an interpretable final model. AIC and BIC serve as suitable selection criteria due to the binary nature of the response variable in our dataset. Unlike adjusted $R^2$ or residual mean square, AIC and BIC focus on maximizing the likelihood of the data while penalizing model complexity. For comparision, AIC emphasizes maximizing the likelihood, BIC adds a higher penalty for decreasing model complexity, favoring a more straightforward and simpler model.\n\n-   Given the objective of setting up a model for prediction, avoiding model overfitting and reducing the variance of estimated cofficients are important concerns we need to consider. To address this concern, LASSO regression gives us a great advantage in terms of effectively shrinking some coefficients to zero, thereby increasing the model's generalizability to out-of-sample data. Therefore, we will also incorporate a LASSO regression model into our analysis to serve as another candidate model.\n\n-   We will compare the predictive performance of four candidate models: the full model, AIC-selected model, BIC-selected model, and LASSO model. After splitting the data into training and testing subsets, we will build up each model using the training dataset and evaluate their performance based on AUC values. The model with the highest AUC value was selected as the final model. We will then assess the generalization ability of the final model by fitting it to the testing dataset and computing the AUC value. Using a probability threshold of 0.5, we classify individuals as \"1\" or \"0\" accordingly. Additionally, we will compute confusion matrices and evaluated metrics such as Accuracy and Precision to determine the best model for predicting diabetes status. This comprehensive approach allows us to identify the most effective model for our predictive task.\n\n#### Implementation of a proposed model:\n\n-   **Spliting the data into training set and testing set:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\ntraining.samples <- diabetes_clean$Outcome %>%\n  createDataPartition(p = 0.7, list = FALSE)\ndiabetes_train  <- diabetes_clean[training.samples, ]\ndiabetes_test <- diabetes_clean[-training.samples, ]\nnrow(diabetes_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 275\n```\n\n\n:::\n\n```{.r .cell-code}\nnrow(diabetes_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 117\n```\n\n\n:::\n:::\n\n\n-   **Fit the full logistic regression model using training dataset:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_model <- glm(formula = Outcome ~ ., family = binomial, data = diabetes_train)\nsummary(full_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = Outcome ~ ., family = binomial, data = diabetes_train)\n\nCoefficients:\n                           Estimate Std. Error z value Pr(>|z|)    \n(Intercept)              -1.050e+01  1.482e+00  -7.087 1.37e-12 ***\nPregnancies              -1.740e-02  6.880e-02  -0.253   0.8004    \nGlucose                   4.394e-02  7.526e-03   5.838 5.27e-09 ***\nBloodPressure            -7.669e-03  1.558e-02  -0.492   0.6225    \nSkinThickness             3.943e-03  2.082e-02   0.189   0.8498    \nInsulin                  -6.144e-04  1.651e-03  -0.372   0.7097    \nBMI                       8.170e-02  3.462e-02   2.360   0.0183 *  \nDiabetesPedigreeFunction  1.121e+00  5.238e-01   2.139   0.0324 *  \nAge                       4.648e-02  2.312e-02   2.010   0.0444 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 354.52  on 274  degrees of freedom\nResidual deviance: 238.11  on 266  degrees of freedom\nAIC: 256.11\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n:::\n\n\n-   **Backward selection based on AIC to get a AIC-selected model:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAIC_selection <- stepAIC(full_model, method = \"backward\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStart:  AIC=256.11\nOutcome ~ Pregnancies + Glucose + BloodPressure + SkinThickness + \n    Insulin + BMI + DiabetesPedigreeFunction + Age\n\n                           Df Deviance    AIC\n- SkinThickness             1   238.15 254.15\n- Pregnancies               1   238.18 254.18\n- Insulin                   1   238.25 254.25\n- BloodPressure             1   238.35 254.35\n<none>                          238.11 256.11\n- Age                       1   242.40 258.40\n- DiabetesPedigreeFunction  1   243.01 259.01\n- BMI                       1   244.01 260.01\n- Glucose                   1   282.11 298.11\n\nStep:  AIC=254.15\nOutcome ~ Pregnancies + Glucose + BloodPressure + Insulin + BMI + \n    DiabetesPedigreeFunction + Age\n\n                           Df Deviance    AIC\n- Pregnancies               1   238.21 252.21\n- Insulin                   1   238.29 252.29\n- BloodPressure             1   238.40 252.40\n<none>                          238.15 254.15\n- Age                       1   242.62 256.62\n- DiabetesPedigreeFunction  1   243.09 257.10\n- BMI                       1   247.97 261.97\n- Glucose                   1   282.15 296.15\n\nStep:  AIC=252.21\nOutcome ~ Glucose + BloodPressure + Insulin + BMI + DiabetesPedigreeFunction + \n    Age\n\n                           Df Deviance    AIC\n- Insulin                   1   238.34 250.34\n- BloodPressure             1   238.46 250.46\n<none>                          238.21 252.21\n- DiabetesPedigreeFunction  1   243.20 255.20\n- Age                       1   244.89 256.89\n- BMI                       1   248.19 260.19\n- Glucose                   1   282.19 294.19\n\nStep:  AIC=250.34\nOutcome ~ Glucose + BloodPressure + BMI + DiabetesPedigreeFunction + \n    Age\n\n                           Df Deviance    AIC\n- BloodPressure             1   238.56 248.56\n<none>                          238.34 250.34\n- DiabetesPedigreeFunction  1   243.28 253.28\n- Age                       1   244.93 254.93\n- BMI                       1   248.29 258.29\n- Glucose                   1   293.71 303.71\n\nStep:  AIC=248.56\nOutcome ~ Glucose + BMI + DiabetesPedigreeFunction + Age\n\n                           Df Deviance    AIC\n<none>                          238.56 248.56\n- DiabetesPedigreeFunction  1   243.82 251.82\n- Age                       1   245.01 253.01\n- BMI                       1   249.26 257.26\n- Glucose                   1   293.72 301.72\n```\n\n\n:::\n\n```{.r .cell-code}\nAIC_selection\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  glm(formula = Outcome ~ Glucose + BMI + DiabetesPedigreeFunction + \n    Age, family = binomial, data = diabetes_train)\n\nCoefficients:\n             (Intercept)                   Glucose                       BMI  \n               -10.62017                   0.04244                   0.07836  \nDiabetesPedigreeFunction                       Age  \n                 1.14649                   0.04054  \n\nDegrees of Freedom: 274 Total (i.e. Null);  270 Residual\nNull Deviance:\t    354.5 \nResidual Deviance: 238.6 \tAIC: 248.6\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nAIC_model <- glm(formula = Outcome ~ DiabetesPedigreeFunction + Age + BMI + Glucose, family = binomial, data = diabetes_train)\nsummary(AIC_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = Outcome ~ DiabetesPedigreeFunction + Age + BMI + \n    Glucose, family = binomial, data = diabetes_train)\n\nCoefficients:\n                           Estimate Std. Error z value Pr(>|z|)    \n(Intercept)              -10.620170   1.335864  -7.950 1.86e-15 ***\nDiabetesPedigreeFunction   1.146492   0.520234   2.204  0.02754 *  \nAge                        0.040539   0.016146   2.511  0.01205 *  \nBMI                        0.078362   0.025096   3.122  0.00179 ** \nGlucose                    0.042436   0.006673   6.360 2.02e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 354.52  on 274  degrees of freedom\nResidual deviance: 238.56  on 270  degrees of freedom\nAIC: 248.56\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n:::\n\n\n-   **Backward selection based on BIC to get a BIC-selected model:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBIC_selection <- step(full_model, direction = \"backward\", k = log(nrow(diabetes_clean)), trace = FALSE)\nBIC_selection\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  glm(formula = Outcome ~ Glucose + BMI + Age, family = binomial, \n    data = diabetes_train)\n\nCoefficients:\n(Intercept)      Glucose          BMI          Age  \n  -10.19977      0.04212      0.08450      0.04100  \n\nDegrees of Freedom: 274 Total (i.e. Null);  271 Residual\nNull Deviance:\t    354.5 \nResidual Deviance: 243.8 \tAIC: 251.8\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nBIC_model <- glm(formula = Outcome ~ Age + BMI + Glucose, family = binomial, data = diabetes_train)\nsummary(BIC_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = Outcome ~ Age + BMI + Glucose, family = binomial, \n    data = diabetes_train)\n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -10.199770   1.287171  -7.924 2.30e-15 ***\nAge           0.040997   0.015832   2.589 0.009612 ** \nBMI           0.084503   0.024857   3.400 0.000675 ***\nGlucose       0.042124   0.006539   6.442 1.18e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 354.52  on 274  degrees of freedom\nResidual deviance: 243.82  on 271  degrees of freedom\nAIC: 251.82\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n:::\n\n\n-   **Compute three confusion matrics for full model, AIC-selected model and BIC-selected model seperately:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes_pred_class_full_model <-\n  round(predict(full_model, type = \"response\"), 0)\ndiabetes_confusion_matrix <-\n    confusionMatrix(\n    data = as.factor(diabetes_pred_class_full_model),\n    reference = as.factor(diabetes_train$Outcome),\n    positive = '1'\n)\n\ndiabetes_confusion_matrix\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 159  35\n         1  21  60\n                                          \n               Accuracy : 0.7964          \n                 95% CI : (0.7439, 0.8424)\n    No Information Rate : 0.6545          \n    P-Value [Acc > NIR] : 1.8e-07         \n                                          \n                  Kappa : 0.5335          \n                                          \n Mcnemar's Test P-Value : 0.08235         \n                                          \n            Sensitivity : 0.6316          \n            Specificity : 0.8833          \n         Pos Pred Value : 0.7407          \n         Neg Pred Value : 0.8196          \n             Prevalence : 0.3455          \n         Detection Rate : 0.2182          \n   Detection Prevalence : 0.2945          \n      Balanced Accuracy : 0.7575          \n                                          \n       'Positive' Class : 1               \n                                          \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes_pred_class_AIC_model <-\n  round(predict(AIC_model, type = \"response\"), 0)\n\ndiabetes_confusion_matrix_AIC_model <-\n    confusionMatrix(\n    data = as.factor(diabetes_pred_class_AIC_model),\n    reference = as.factor(diabetes_train$Outcome),\n    positive = '1'\n)\n\ndiabetes_confusion_matrix_AIC_model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 160  35\n         1  20  60\n                                          \n               Accuracy : 0.8             \n                 95% CI : (0.7478, 0.8456)\n    No Information Rate : 0.6545          \n    P-Value [Acc > NIR] : 8.513e-08       \n                                          \n                  Kappa : 0.5406          \n                                          \n Mcnemar's Test P-Value : 0.05906         \n                                          \n            Sensitivity : 0.6316          \n            Specificity : 0.8889          \n         Pos Pred Value : 0.7500          \n         Neg Pred Value : 0.8205          \n             Prevalence : 0.3455          \n         Detection Rate : 0.2182          \n   Detection Prevalence : 0.2909          \n      Balanced Accuracy : 0.7602          \n                                          \n       'Positive' Class : 1               \n                                          \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes_pred_class_BIC_model <-\n  round(predict(BIC_model, type = \"response\"), 0)\n\ndiabetes_confusion_matrix_BIC_model <-\n    confusionMatrix(\n    data = as.factor(diabetes_pred_class_BIC_model),\n    reference = as.factor(diabetes_train$Outcome),\n    positive = '1'\n)\n\ndiabetes_confusion_matrix_BIC_model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 160  36\n         1  20  59\n                                          \n               Accuracy : 0.7964          \n                 95% CI : (0.7439, 0.8424)\n    No Information Rate : 0.6545          \n    P-Value [Acc > NIR] : 1.8e-07         \n                                          \n                  Kappa : 0.5311          \n                                          \n Mcnemar's Test P-Value : 0.04502         \n                                          \n            Sensitivity : 0.6211          \n            Specificity : 0.8889          \n         Pos Pred Value : 0.7468          \n         Neg Pred Value : 0.8163          \n             Prevalence : 0.3455          \n         Detection Rate : 0.2145          \n   Detection Prevalence : 0.2873          \n      Balanced Accuracy : 0.7550          \n                                          \n       'Positive' Class : 1               \n                                          \n```\n\n\n:::\n:::\n\n\n-   **Get AUC values for these 3 candidate models:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nROC_full_log <- roc(\n  response = diabetes_train$Outcome,\n  predictor = predict(full_model, type = \"response\")\n)\ncat(\"Full model AUC value:\", ROC_full_log$auc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFull model AUC value: 0.8774269\n```\n\n\n:::\n\n```{.r .cell-code}\nROC_AIC_log <- roc(\n  response = diabetes_train$Outcome,\n  predictor = predict(AIC_model, type = \"response\")\n)\ncat(\"AIC-selected model AUC value:\", ROC_AIC_log$auc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAIC-selected model AUC value: 0.8776023\n```\n\n\n:::\n\n```{.r .cell-code}\nROC_BIC_log <- roc(\n  response = diabetes_train$Outcome,\n  predictor = predict(BIC_model, type = \"response\")\n)\ncat(\"BIC-selected model AUC value:\", ROC_BIC_log$auc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBIC-selected model AUC value: 0.8598246\n```\n\n\n:::\n:::\n\n\n-   **Using LASSO to get a logistic regression model and compare the prediction performance of LASSO model with those 3 models above.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_matrix_X_train <-\n    as.matrix(diabetes_train[, -9])\n\nmatrix_Y_train <-\n    as.matrix(diabetes_train[, 9], ncol = 1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#set.seed(271)\ndiabetes_cv_lambda_LASSO <-\n  cv.glmnet(\n  x = model_matrix_X_train, y = matrix_Y_train,\n  alpha = 1,\n  family = 'binomial',\n  type.measure = 'auc',\n  nfolds = 5)\n\ndiabetes_cv_lambda_LASSO\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  cv.glmnet(x = model_matrix_X_train, y = matrix_Y_train, type.measure = \"auc\",      nfolds = 5, alpha = 1, family = \"binomial\") \n\nMeasure: AUC \n\n     Lambda Index Measure      SE Nonzero\nmin 0.01601    31  0.8510 0.02857       5\n1se 0.07092    15  0.8241 0.03988       3\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes_lambda_1se_AUC_LASSO <- round(diabetes_cv_lambda_LASSO$lambda.1se, 4)\n\ndiabetes_lambda_1se_AUC_LASSO\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0709\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes_LASSO_1se_AUC <- glmnet(\n  x = model_matrix_X_train, y = matrix_Y_train,\n  alpha = 1,\n  family = 'binomial',\n  lambda = diabetes_lambda_1se_AUC_LASSO\n)\ncoef(diabetes_LASSO_1se_AUC)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n9 x 1 sparse Matrix of class \"dgCMatrix\"\n                                   s0\n(Intercept)              -5.220454794\nPregnancies               .          \nGlucose                   0.028193519\nBloodPressure             .          \nSkinThickness             .          \nInsulin                   .          \nBMI                       0.021872562\nDiabetesPedigreeFunction  .          \nAge                       0.008939217\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nROC_lasso <-\n    roc(\n        response = diabetes_train$Outcome,\n        predictor = predict(diabetes_LASSO_1se_AUC,\n                     newx = model_matrix_X_train)[,\"s0\"] )\nROC_lasso\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nroc.default(response = diabetes_train$Outcome, predictor = predict(diabetes_LASSO_1se_AUC,     newx = model_matrix_X_train)[, \"s0\"])\n\nData: predict(diabetes_LASSO_1se_AUC, newx = model_matrix_X_train)[, \"s0\"] in 180 controls (diabetes_train$Outcome 0) < 95 cases (diabetes_train$Outcome 1).\nArea under the curve: 0.8496\n```\n\n\n:::\n\n```{.r .cell-code}\nAUC_lasso <- pROC::auc(ROC_lasso)\n```\n:::\n\n\n-   **Compare the AUC values for our 4 candidate models, full model, AIC-selected model, BIC-selected model and LASSO model.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_names <- c(\"Full Model\", \"AIC-selected Model\", \"BIC-selected Model\", \"LASSO Model\")\nAUC_values <- c(ROC_full_log$auc, ROC_AIC_log$auc, ROC_BIC_log$auc, as.double(AUC_lasso))\ncomparison_table <- data.frame(Model = model_names, AUC = AUC_values)\n\ncomparison_table\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Model       AUC\n1         Full Model 0.8774269\n2 AIC-selected Model 0.8776023\n3 BIC-selected Model 0.8598246\n4        LASSO Model 0.8496491\n```\n\n\n:::\n:::\n\n\n#### Results:\n\nAfter comparing the AUC values for the four candidate models, we observed that the AIC-selected model had the best prediction performance on the training dataset. Consequently, we determined to adopt the AIC-selected model as our final predictive model. Next, we will assess the out-of-sample performance of our final model by applying it to the testing dataset, thereby generating the ROC curve and computing the corresponding AUC value.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(repr.plot.width = 15, repr.plot.height = 10)\n\nmodel_X_test <- diabetes_test[, -which(names(diabetes_test) == \"Outcome\")]\n\npredicted_prob_AIC <- predict(AIC_model, newdata = model_X_test, type = \"response\")\npredicted_fullmodel <- predict(full_model, newdata = model_X_test, type = \"response\")\npredicted_prob_BIC <- predict(BIC_model, newdata = model_X_test, type = \"response\")\npredicted_lasso <- predict(diabetes_LASSO_1se_AUC, newx = as.matrix(model_X_test))[,\"s0\"]\n\nROC_AIC_model_in_testdata <- roc(response = diabetes_test$Outcome, predictor = predicted_prob_AIC)\nplot(ROC_AIC_model_in_testdata,  print.auc = TRUE, print.auc.x = 0.5, print.auc.y = 0.5, col = \"steelblue\", lwd = 3, lty = 2, main = \"ROC Curve of Models using Test Dataset\", cex.main = 2)\n\nROC_full_model_in_testdata <- roc(response = diabetes_test$Outcome, predictor = predicted_fullmodel)\nplot(ROC_full_model_in_testdata, print.auc = TRUE, print.auc.x = 0.5, print.auc.y = 0.45, col = \"black\", lwd = 3, lty = 2, add = TRUE)\n\nROC_BIC_model_in_testdata <- roc(response = diabetes_test$Outcome, predictor = predicted_prob_BIC)\nplot(ROC_BIC_model_in_testdata, print.auc = TRUE, print.auc.x = 0.5, print.auc.y = 0.40, col = \"springgreen\", lwd = 3, lty = 2, add = TRUE)\n\nROC_lasso_in_testdata <- roc(response = diabetes_test$Outcome, predictor = predicted_lasso)\nplot(ROC_lasso_in_testdata, print.auc = TRUE, print.auc.x = 0.5, print.auc.y = 0.35, col = \"red\", lwd = 3, lty = 2, add = TRUE)\n\n\nlegend(\"bottomright\", legend = model_names, col = c(\"black\", \"steelblue\", \"springgreen\", \"red\"), lty = 2, lwd = 3)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){width=1440}\n:::\n:::\n\n\n#### Result summary and conclusion:\n\nBased on our analysis, the AIC-selected model has the best prediction performance among the four models we compared. Upon fitting the AIC-selected model to the testing dataset, we obtained an AUC value of 0.808, which underscores the robust predictive capability of our AIC-selected model when applied to out-of-sample data.\n\nTherefore, we have concluded that the AIC-selected model aligns most effectively with our project's objective of establishing a predictive model for determining the probabilty of an individual having diabetes. Below is a summary of the selected model:\n\n$$\n\\begin{align*}\n\\log\\left(\\frac{p_i}{1-p_i}\\right) &= -10.620170 +1.146492 \\cdot \\text{DiabetesPedigreeFunction} + 0.040539 \\cdot \\text{Age} \\\\\n&\\quad + 0.078362 \\cdot \\text{BMI} + 0.042436 \\cdot \\text{Glucose}\n\\end{align*}\n$$\n\n\nwhere $p_i$ is the probability of the $i{\\text{th}}$ individual having diabetes.\n\nGiven an individual's diabetes percentage, age, BMI, and glucose level, this model can be used to predictive the probability of diabetes as\n\n$$p_{i} = \\frac{1}{1+e^{-(-10.620170 + 1.146492 \\times \\text{DiabetesPedigreeFunction} + 0.040539 \\times \\text{Age} + 0.078362 \\times \\text{BMI} + 0.042436 \\times \\text{Glucose})}}$$\n\n## Discussion:\n\nThrough this project, we've developed a predictive model to estimate the probability of an individual having diabetes. In our final model, we've left with four key variables, making the model has the best prediction performance: DiabetesPedigreeFunction, Age, BMI, and Glucose. Notably, all coefficients associated with these variables are positive, indicating that higher values for these factors correlate with an increased prbability of diabetes. Following model comparison and selection processes, the AIC-selected model has been determined as the optimal choice, demonstrating best predictive performance both in-sample and out-of-sample prediction among our 4 candidate models with AUC value of approximately 0.8, indicating its robust predictive capabilities.\n\nThe outcome of our analysis was surprising. While AIC-based stepwise selection is commonly used to explore predictor-response relationships, we opted to assess the efficacy of a LASSO model, known for predictive power and overfitting avoidance. We expected the LASSO model to outperform the AIC-selected model in out-of-sample prediction accuracy. However, results showed the AIC-selected model not only offered good interpretability but also outperformed the LASSO model in terms of AUC values. This outcome is better than we expected as it signifies a balance between model interpretability and predictive powerness in our final model.\n\nWhile the AIC-selected model performed best on the testing dataset, its superiority on out-of-sample data is not guaranteed. Implementing k-fold cross-validation and calculating CV-AUC values for our four candidate models can enhance our methodology. This approach assesses models across various data partitions, reducing reliance on chance results from a single train-test split. CV-AUC values may not always align with initial model selection; for instance, the Lasso model might show the highest CV-AUC value, indicating superior prediction performance. Integrating k-fold cross-validation into our model evaluation enhances our methodology's robustness, ensuring our final predictive model is well-suited for generalization to unseen data.\n\nA key area for future exploration is identifying additional predictors beyond those in our dataset that could influence diabetes risk. Factors like other medical histories and pharmaceutical supplements may provide valuable insights. Additionally, we should investigate how different parameters, such as lambda values, affect the LASSO model's performance. In our analysis we use lambda.1se value, and explore the performance with the lambda.min value. In summary, exploring new predictors and optimizing regularization parameters can enhance our predictive models and improve our ability to predict and manage diabetes.\n\n## References:\n\n1.  **Related Study 1**: Joshi, Ram D, and Chandra K Dhakal. \"Predicting Type 2 Diabetes Using Logistic Regression and Machine Learning Approaches.\" International Journal of Environmental Research and Public Health, U.S. National Library of Medicine, 9 July 2021, www.ncbi.nlm.nih.gov/pmc/articles/PMC8306487/.\n\n2.  **Related Study 2**: Chang, Victor, et al. \"Pima Indians Diabetes Mellitus Classification Based on Machine Learning (ML) Algorithms.\" Neural Computing & Applications, U.S. National Library of Medicine, 24 Mar. 2022, www.ncbi.nlm.nih.gov/pmc/articles/PMC8943493/.\n\n3.  **Data source:** National Institute of Diabetes and Digestive and Kidney Diseases. \"Predict Diabetes.\" Kaggle, 9 Nov. 2022, www.kaggle.com/datasets/whenamancodes/predict-diabities?resource=download.\n\n4.  **More Details on data:** \"Pima Indians Diabetes Database - Dataset by Data-Society.\" Data.World, 13 Dec. 2016, www.data.world/data-society/pima-indians-diabetes-database.\n\n5.  **ROC in health data:** Nahm, Francis Sahngun. \"Receiver Operating Characteristic Curve: Overview and Practical Use for Clinicians.\" Korean Journal of Anesthesiology, U.S. National Library of Medicine, Feb. 2022, www.ncbi.nlm.nih.gov/pmc/articles/PMC8831439/.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}