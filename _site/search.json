[
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Projects",
    "section": "",
    "text": "Diabetes Prediction Modeling\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nApr 21, 2024\n\n\nArnab Das\n\n\n\n\n\n\n\n\n\n\n\n\nModeling for flagging potential cancellations\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nMay 4, 2023\n\n\nArnab Das\n\n\n\n\n\n\n\n\n\n\n\n\nPulsar Star Classification\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 1, 2021\n\n\nArnab Das\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html#introduction",
    "href": "posts/post-with-code/index.html#introduction",
    "title": "Diabetes Prediction Modeling",
    "section": "",
    "text": "Diabetes is a prevalent chronic metabolic disorder posing significant health and economic burdens globally, particularly with the recent rise in type 2 diabetes cases. Predictive modeling offers a valuable approach for identifying individuals at risk and intervening early. The dataset we analyze in this project exclusively consists of female patients aged 21 years or above, all of whom are of Pima Indian heritage. These demographic constraints ensure a focused examination of diabetes within this specific population subset. Diagnostic measurements crucial for diabetes prediction, including glucose levels, blood pressure, insulin levels, and BMI were collected through medical examinations and tests conducted by healthcare professionals. Our aim is to develop an effective tool for diabetes risk assessment to gain insights into the factors contributing to its onset, ultimately improving health outcomes and quality of life for individuals vulnerable to diabetes.\n\n\nThe dataset originates from the National Institute of Diabetes and Digestive and Kidney Diseases and is utilized to predict the probability of diabetes diagnosis in female subjects aged 21 and above. There are a total of 768 observations and 9 variables in the dataset. The target variable is Outcome which indicates the presence of diabetes. The 8 explanatory variables are: Pregnancies, Glucose, BloodPressure, Skin Thickness, Insulin, BMI, DiabetesPredigreeFunction and Age. Below are the detailed description of each explanatory varibles:\n\nPregnancies: Integer variable indicating the number of pregnancies the individual has experienced.\nGlucose: Numeric variable representing plasma glucose concentration at 2 hours in an oral glucose tolerance test, measured in mg/dL.\nBloodPressure: Numeric variable denoting the diastolic blood pressure, measured in mmHg.\nSkin Thickness: Numeric variable indicating the thickness of the triceps skin fold, measured in mm.\nInsulin: Numeric variable representing insulin levels in the bloodstream two hours after a specific event (such as the administration of glucose), measured in micro-units per milliliter of serum.\nBMI: Numeric variable representing Body Mass Index (BMI), a measure of body fat based on height and weight, measured in kg/m^2.\nDiabetesPedigreeFunction: Numeric variable representing a function which scores the likelihood of diabetes based on family history.\nAge: Integer variable indicating the age of the individual.\nOutcome: Categorical (binary) variable, where 0 represents absence of diabetes and 1 represents presence of diabetes. This variable is the target variable for prediction.\n\n\n\n\nThe primary objective of this project is to develop a predictive model capable for predicting the probability of a subject having diabetes based on their diagnostic measurements. By variable and model selection, we aim to build a “best” model for prediction among all candidate models. Through this exploration, we seek to gain insights into the underlying factors contributing to diabetes onset and create a valuable tool for diabetes risk assessment. Further analysis, such as correlation analysis, could contribute to ensuring the reliability and robustness of the observed relationships.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(GGally)\nlibrary(glmnet)\nlibrary(caret)\nlibrary(MASS)\nlibrary(pROC)\nlibrary(cowplot)\n\n\n\n\n\n\n\nShow the code\ndiabetes &lt;- read.csv(\"diabetes.csv\")\nhead(diabetes)\n\n\n  Pregnancies Glucose BloodPressure SkinThickness Insulin  BMI\n1           6     148            72            35       0 33.6\n2           1      85            66            29       0 26.6\n3           8     183            64             0       0 23.3\n4           1      89            66            23      94 28.1\n5           0     137            40            35     168 43.1\n6           5     116            74             0       0 25.6\n  DiabetesPedigreeFunction Age Outcome\n1                    0.627  50       1\n2                    0.351  31       0\n3                    0.672  32       1\n4                    0.167  21       0\n5                    2.288  33       1\n6                    0.201  30       0\n\n\nShow the code\nnrow(diabetes)\n\n\n[1] 768\n\n\n\n\n\nMissing values can introduce bias in parameter estimates and reduce their precision. Upon observing that several attributes in our dataset contain missing values, we opted to clean the data by removing these rows.\n\n\nShow the code\ndiabetes_clean &lt;- diabetes[!(diabetes$Glucose == 0 | diabetes$BloodPressure == 0 | diabetes$SkinThickness == 0 | diabetes$Insulin == 0 | diabetes$BMI == 0 | diabetes$DiabetesPedigreeFunction == 0 | diabetes$Age == 0), ]\nhead(diabetes_clean)\n\n\n   Pregnancies Glucose BloodPressure SkinThickness Insulin  BMI\n4            1      89            66            23      94 28.1\n5            0     137            40            35     168 43.1\n7            3      78            50            32      88 31.0\n9            2     197            70            45     543 30.5\n14           1     189            60            23     846 30.1\n15           5     166            72            19     175 25.8\n   DiabetesPedigreeFunction Age Outcome\n4                     0.167  21       0\n5                     2.288  33       1\n7                     0.248  26       1\n9                     0.158  53       1\n14                    0.398  59       1\n15                    0.587  51       1\n\n\nShow the code\nnrow(diabetes_clean)\n\n\n[1] 392\n\n\nShow the code\noutcome_counts &lt;- table(diabetes_clean$Outcome)\nprint(outcome_counts)\n\n\n\n  0   1 \n262 130 \n\n\n\n\n\nThe number of rows in our dataset after removing the 0 values is 392. Although the dataset size has decreased, the remaining data still provides sufficient information to explore relationships, trends, and patterns. By excluding rows with unreliable physiological measurements, we ensure the integrity and accuracy of the dataset, allowing for more reliable insights and interpretations from subsequent analyses.\nFrom some basic exploratory data analysis we see the dataset contains around one-third positive (1) outcomes, and two-thirds negative (0) outomes, they are generally balanced enough. However, it’s important to remain vigilant for potential issues related to class imbalance and to employ appropriate techniques if imbalance becomes problematic during analysis."
  },
  {
    "objectID": "posts/post-with-code/index.html#methods-and-results",
    "href": "posts/post-with-code/index.html#methods-and-results",
    "title": "Diabetes Prediction Modeling",
    "section": "Methods and Results:",
    "text": "Methods and Results:\n\nExploratory Data Analysis (EDA):\nBefore delving into specifics, it’s essential to examine the overall distribution of outcomes across variables. This exploration provides insight into how outcomes vary in response to changes in each variable.\n\nExplore the multicollinearity:\nAccording to regression assumptions, multicollinearity among explanatory variables should be avoided. If a multicollinearity problem exists in the dataset, the standard errors of estimated coefficients will be inflated, and coefficient estimates will be unstable, making it difficult to determine variable significance. Additionally, the interpretation of coefficients will be misleading. We can explore the correlation matrix for better insights.\n\n\nShow the code\n# getting correlation values between variables\ncorr_matrix &lt;- diabetes_clean %&gt;%\n  dplyr::select(- Outcome) %&gt;%\n  cor() %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"var1\") %&gt;%\n  pivot_longer(-var1, names_to = \"var2\", values_to = \"corr\")\n\n# plotting a correlation matrix\noptions(repr.plot.width = 15, repr.plot.height = 15)\ncorr_matrix %&gt;%\n  ggplot(aes(var1, var2)) +\n  geom_tile(aes(fill = corr), color = \"white\") +\n  scale_fill_distiller(\"Correlation Coefficient \\n\",\n    palette =  \"Spectral\",\n    direction = 1, limits = c(-1,1)\n  ) +\n    theme(\n        axis.text.x = element_text(\n          angle = 45, vjust = 1,\n          size = 18, hjust = 1\n        ),\n        axis.text.y = element_text(\n          vjust = 1,\n          size = 18, hjust = 1\n        ),\n        title = element_text(size = 20, face = \"bold\"),\n        legend.title = element_text(size = 18, face = \"bold\"),\n        legend.text = element_text(size = 20),\n        legend.key.size = unit(2, \"cm\"),\n        text = element_text(size = 20),\n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 25)\n      ) +\n      coord_fixed() +\n      geom_text(aes(var1, var2, label = round(corr, 2)), color = \"black\", size = 6) +\n    labs(title = \"Correlation Matrix\")\n\n\n\n\n\n\n\n\n\n\nUnderstanding variables which show correlation:\nIn the dataset analysis, several pairs of variables show significant correlations. Notably, Glucose and Insulin correlate at 0.58, indicating a regulatory response to blood sugar levels. Age and Pregnancies exhibit a correlation of 0.68, reflecting reproductive aging. BMI and SkinThickness correlate at 0.66, suggesting a link between body fat and skin thickness. BloodPressure and BMI show a correlation of 0.30, indicating a connection between hypertension and obesity. Lastly, Glucose and Age correlate at 0.34, potentially indicating age-related changes in glucose metabolism and diabetes risk. The above all shows the potential issue of multicollinearity in the dataset.\nConversely, the correlation between other variables appears to be within acceptable ranges, suggesting that they are not significantly affected by multicollinearity. Therefore, we need to addresse multicollinearity issue by some techniques such as variable selection or regularization methods,improving the robustness of the regression model.\n\n\n\nDistribution of predictors:\nThe density plots for the variables in this dataset illustrate the distribution of each variable’s values. This visualization helps in understanding the spread, central tendency, and shape of the data for variables such as Pregnancies, Glucose, BloodPressure, Skin Thickness, Insulin, BMI, DiabetesPedigreeFunction, and Age. These plots offer insights into the prevalence and distribution of key factors associated with diabetes diagnosis in the female subjects aged 21 and above.\n\n\nShow the code\ndata_long &lt;- diabetes_clean %&gt;%\n  pivot_longer(cols = -Outcome, names_to = \"Variable\", values_to = \"Value\")\n\noptions(repr.plot.width = 15, repr.plot.height = 10)\ndensity_plot &lt;- ggplot(data_long, aes(x = Value, fill = Variable)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~ Variable, scales = \"free\", nrow = 2, ncol = 4) +\n  theme_minimal() +\n  ggtitle(\"Density Plots of Factors\") +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\", size = 25),\n        text = element_text(size = 15)) +\n  guides(fill = \"none\")\n\ndensity_plot\n\n\n\n\n\n\n\n\n\n\nUnderstanding the distributions:\nAge skews right, indicating a younger population. Blood Pressure and BMI are normally distributed, representing the population. DiabetesPedigreeFunction and Insulin skew right, with low values prevalent; Glucose is normally distributed; Pregnancies skew right, suggesting fewer are common; Skin Thickness is nearly normal, peaking at lower values. These patterns aid in understanding population demographics and physiological factors influencing diabetes prediction.\n\n\n\nObserving the relationship between each predictor variable and the outcome:\nWe aim to gain insights into the relationship between each explanatory variable and the response variable before conducting regression analysis. Given the binary nature of the response variable, utilizing boxplots to visualize the relationship between each explanatory variable and the response variable offers a convenient approach.\n\n\nShow the code\nfunction_plot &lt;- ggplot(data = diabetes_clean, aes(x = factor(Outcome), y =DiabetesPedigreeFunction, fill = factor(Outcome))) +\n  geom_boxplot(colour = \"red\") +\n  labs(title = \"DiabetesPedigreeFunction vs. Outcome\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Diabetes Pedigree Function\") + theme(\n    text = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.title = element_text(size = 15)\n  ) +\n  scale_fill_brewer(palette = \"Reds\", labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\"))+\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  theme(text = element_text(size = 15))\n\nSkin_plot &lt;- ggplot(data = diabetes_clean, aes(x = as.factor(Outcome), y =SkinThickness, fill = factor(Outcome))) +\n  geom_boxplot(colour = \"purple\") +\n  labs(title = \"Skin Thickness vs. Outcome\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Skin Thickness\") + theme(\n    text = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.title = element_text(size = 15)\n) +\n  scale_fill_brewer(palette = \"PuRd\", labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\"))+\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  theme(text = element_text(size = 15))\n\nglucose_plot &lt;- ggplot(data = diabetes_clean, aes(x = as.factor(Outcome), y =Glucose, fill = factor(Outcome))) +\n  geom_boxplot(colour = \"blue\") +\n  labs(title = \"Glucose vs. Outcome\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Glucose Level\") + theme(\n    text = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.title = element_text(size = 15)\n  ) +\n  scale_fill_brewer(palette = \"Blues\", labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\"))+\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  theme(text = element_text(size = 15))\n\nbloodPressure_plot &lt;- ggplot(data = diabetes_clean, aes(x = as.factor(Outcome), y =BloodPressure, fill = factor(Outcome))) +\n  geom_boxplot(colour = \"red\") +\n  labs(title = \"Blood Pressure vs. Outcome\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Blood Pressure\") + theme(\n    text = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.title = element_text(size = 15)\n  ) +\n  scale_fill_brewer(palette = \"RdPu\", labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\"))+\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  theme(text = element_text(size = 15))\n\nInsulin_plot &lt;- ggplot(data = diabetes_clean, aes(x = as.factor(Outcome), y =Insulin, fill = factor(Outcome))) +\n  geom_boxplot(colour = \"darkgreen\") +\n  labs(title = \"Insulin vs. Outcome\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Insulin\") + theme(\n    text = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.title = element_text(size = 15)\n  )  +\n  scale_fill_brewer(palette = \"BuGn\", labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\"))+\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  theme(text = element_text(size = 15))\n\nBMI_plot &lt;- ggplot(data = diabetes_clean, aes(x = as.factor(Outcome), y =BMI, fill = factor(Outcome))) +\n  geom_boxplot(colour = \"darkgreen\") +\n  labs(title = \"BMI vs. Outcome\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Body Mass Index\")  + theme(\n    text = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.title = element_text(size = 15)\n  )  +\n  scale_fill_brewer(palette = \"Greens\", labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  theme(text = element_text(size = 15))\n\nAge_plot &lt;- ggplot(data = diabetes_clean, aes(x = as.factor(Outcome), y =Age, fill = factor(Outcome))) +\n  geom_boxplot(colour = \"red\") +\n  labs(title = \"Age vs. Outcome\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Age\")  + theme(\n    text = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.title = element_text(size = 15)\n  )  +\n  scale_fill_brewer(palette = \"OrRd\", labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  theme(text = element_text(size = 15))\n\nPregnancies_plot &lt;- ggplot(data = diabetes_clean, aes(x = as.factor(Outcome), y =Pregnancies, fill = factor(Outcome))) +\n  geom_boxplot(colour = \"DarkBlue\") +\n  labs(title = \"Pregnancies vs. Outcome\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Number of Pregnancies\") + theme(\n    text = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.title = element_text(size = 15)\n  )  +\n  scale_fill_brewer(palette = \"BuPu\", labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  theme(text = element_text(size = 15))\n\ncombined_plot &lt;- plot_grid(function_plot, Skin_plot, glucose_plot, bloodPressure_plot, Insulin_plot, BMI_plot, Age_plot, Pregnancies_plot, nrow = 2, ncol = 4)\n\noptions(repr.plot.width = 20, repr.plot.height = 15)\nplot_grid(ggdraw() + draw_label(\"Diabetes Outcome Across Key Factors\", fontface='bold', size = 25), combined_plot, ncol=1, rel_heights=c(0.1, 1))\n\n\n\n\n\n\n\n\n\n\nConclusion from boxplots:\n\nFrom the eight boxplots above, a notable disparity emerges in the mean glucose levels between individuals with and without diabetes. Specifically, the mean glucose level appears markedly higher among those with diabetes compared to those without, suggesting a positive association between glucose level and diabetes. Given this observation, further investigation into the relationship between glucose level and diabetes outcome is warranted.\nMoreover, upon inspecting the boxplot depicting diabetes status against age, a similar pattern emerges. Individuals diagnosed with diabetes have a higher mean age compared to those without. Consequently, it can be inferred that both age and glucose level are potentially significant explanatory variables associated with diabetes outcome.\nAdditionally, it’s noteworthy that the mean values of other variables exhibit slight variations based on whether individuals have diabetes or not. Specifically, when an individual has diabetes, the mean values of all eight predictor variables are higher compared to when the person doesn’t have diabetes, suggesting a potentially positive relationship between each X and Y to some extent.\n\n\n\n\n\nMethods (plan):\n\nModel selection methods:\n\nWe will begin with a full model incorporating all eight variables, then use backward selection based on AIC and BIC to refine our model selection. This process yields two models: an AIC-selected model and a BIC-selected model. Backward selection eliminates a non-significant predictor from the model in each interaction, resulting in an interpretable final model. AIC and BIC serve as suitable selection criteria due to the binary nature of the response variable in our dataset. Unlike adjusted \\(R^2\\) or residual mean square, AIC and BIC focus on maximizing the likelihood of the data while penalizing model complexity. For comparision, AIC emphasizes maximizing the likelihood, BIC adds a higher penalty for decreasing model complexity, favoring a more straightforward and simpler model.\nGiven the objective of setting up a model for prediction, avoiding model overfitting and reducing the variance of estimated cofficients are important concerns we need to consider. To address this concern, LASSO regression gives us a great advantage in terms of effectively shrinking some coefficients to zero, thereby increasing the model’s generalizability to out-of-sample data. Therefore, we will also incorporate a LASSO regression model into our analysis to serve as another candidate model.\nWe will compare the predictive performance of four candidate models: the full model, AIC-selected model, BIC-selected model, and LASSO model. After splitting the data into training and testing subsets, we will build up each model using the training dataset and evaluate their performance based on AUC values. The model with the highest AUC value was selected as the final model. We will then assess the generalization ability of the final model by fitting it to the testing dataset and computing the AUC value. Using a probability threshold of 0.5, we classify individuals as “1” or “0” accordingly. Additionally, we will compute confusion matrices and evaluated metrics such as Accuracy and Precision to determine the best model for predicting diabetes status. This comprehensive approach allows us to identify the most effective model for our predictive task.\n\n\n\nImplementation of a proposed model:\n\nSpliting the data into training set and testing set:\n\n\n\nShow the code\nset.seed(123)\ntraining.samples &lt;- diabetes_clean$Outcome %&gt;%\n  createDataPartition(p = 0.7, list = FALSE)\ndiabetes_train  &lt;- diabetes_clean[training.samples, ]\ndiabetes_test &lt;- diabetes_clean[-training.samples, ]\nnrow(diabetes_train)\n\n\n[1] 275\n\n\nShow the code\nnrow(diabetes_test)\n\n\n[1] 117\n\n\n\nFit the full logistic regression model using training dataset:\n\n\n\nShow the code\nfull_model &lt;- glm(formula = Outcome ~ ., family = binomial, data = diabetes_train)\nsummary(full_model)\n\n\n\nCall:\nglm(formula = Outcome ~ ., family = binomial, data = diabetes_train)\n\nCoefficients:\n                           Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              -1.050e+01  1.482e+00  -7.087 1.37e-12 ***\nPregnancies              -1.740e-02  6.880e-02  -0.253   0.8004    \nGlucose                   4.394e-02  7.526e-03   5.838 5.27e-09 ***\nBloodPressure            -7.669e-03  1.558e-02  -0.492   0.6225    \nSkinThickness             3.943e-03  2.082e-02   0.189   0.8498    \nInsulin                  -6.144e-04  1.651e-03  -0.372   0.7097    \nBMI                       8.170e-02  3.462e-02   2.360   0.0183 *  \nDiabetesPedigreeFunction  1.121e+00  5.238e-01   2.139   0.0324 *  \nAge                       4.648e-02  2.312e-02   2.010   0.0444 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 354.52  on 274  degrees of freedom\nResidual deviance: 238.11  on 266  degrees of freedom\nAIC: 256.11\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nBackward selection based on AIC to get a AIC-selected model:\n\n\n\nShow the code\nAIC_selection &lt;- stepAIC(full_model, method = \"backward\")\n\n\nStart:  AIC=256.11\nOutcome ~ Pregnancies + Glucose + BloodPressure + SkinThickness + \n    Insulin + BMI + DiabetesPedigreeFunction + Age\n\n                           Df Deviance    AIC\n- SkinThickness             1   238.15 254.15\n- Pregnancies               1   238.18 254.18\n- Insulin                   1   238.25 254.25\n- BloodPressure             1   238.35 254.35\n&lt;none&gt;                          238.11 256.11\n- Age                       1   242.40 258.40\n- DiabetesPedigreeFunction  1   243.01 259.01\n- BMI                       1   244.01 260.01\n- Glucose                   1   282.11 298.11\n\nStep:  AIC=254.15\nOutcome ~ Pregnancies + Glucose + BloodPressure + Insulin + BMI + \n    DiabetesPedigreeFunction + Age\n\n                           Df Deviance    AIC\n- Pregnancies               1   238.21 252.21\n- Insulin                   1   238.29 252.29\n- BloodPressure             1   238.40 252.40\n&lt;none&gt;                          238.15 254.15\n- Age                       1   242.62 256.62\n- DiabetesPedigreeFunction  1   243.09 257.10\n- BMI                       1   247.97 261.97\n- Glucose                   1   282.15 296.15\n\nStep:  AIC=252.21\nOutcome ~ Glucose + BloodPressure + Insulin + BMI + DiabetesPedigreeFunction + \n    Age\n\n                           Df Deviance    AIC\n- Insulin                   1   238.34 250.34\n- BloodPressure             1   238.46 250.46\n&lt;none&gt;                          238.21 252.21\n- DiabetesPedigreeFunction  1   243.20 255.20\n- Age                       1   244.89 256.89\n- BMI                       1   248.19 260.19\n- Glucose                   1   282.19 294.19\n\nStep:  AIC=250.34\nOutcome ~ Glucose + BloodPressure + BMI + DiabetesPedigreeFunction + \n    Age\n\n                           Df Deviance    AIC\n- BloodPressure             1   238.56 248.56\n&lt;none&gt;                          238.34 250.34\n- DiabetesPedigreeFunction  1   243.28 253.28\n- Age                       1   244.93 254.93\n- BMI                       1   248.29 258.29\n- Glucose                   1   293.71 303.71\n\nStep:  AIC=248.56\nOutcome ~ Glucose + BMI + DiabetesPedigreeFunction + Age\n\n                           Df Deviance    AIC\n&lt;none&gt;                          238.56 248.56\n- DiabetesPedigreeFunction  1   243.82 251.82\n- Age                       1   245.01 253.01\n- BMI                       1   249.26 257.26\n- Glucose                   1   293.72 301.72\n\n\nShow the code\nAIC_selection\n\n\n\nCall:  glm(formula = Outcome ~ Glucose + BMI + DiabetesPedigreeFunction + \n    Age, family = binomial, data = diabetes_train)\n\nCoefficients:\n             (Intercept)                   Glucose                       BMI  \n               -10.62017                   0.04244                   0.07836  \nDiabetesPedigreeFunction                       Age  \n                 1.14649                   0.04054  \n\nDegrees of Freedom: 274 Total (i.e. Null);  270 Residual\nNull Deviance:      354.5 \nResidual Deviance: 238.6    AIC: 248.6\n\n\n\n\nShow the code\nAIC_model &lt;- glm(formula = Outcome ~ DiabetesPedigreeFunction + Age + BMI + Glucose, family = binomial, data = diabetes_train)\nsummary(AIC_model)\n\n\n\nCall:\nglm(formula = Outcome ~ DiabetesPedigreeFunction + Age + BMI + \n    Glucose, family = binomial, data = diabetes_train)\n\nCoefficients:\n                           Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              -10.620170   1.335864  -7.950 1.86e-15 ***\nDiabetesPedigreeFunction   1.146492   0.520234   2.204  0.02754 *  \nAge                        0.040539   0.016146   2.511  0.01205 *  \nBMI                        0.078362   0.025096   3.122  0.00179 ** \nGlucose                    0.042436   0.006673   6.360 2.02e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 354.52  on 274  degrees of freedom\nResidual deviance: 238.56  on 270  degrees of freedom\nAIC: 248.56\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nBackward selection based on BIC to get a BIC-selected model:\n\n\n\nShow the code\nBIC_selection &lt;- step(full_model, direction = \"backward\", k = log(nrow(diabetes_clean)), trace = FALSE)\nBIC_selection\n\n\n\nCall:  glm(formula = Outcome ~ Glucose + BMI + Age, family = binomial, \n    data = diabetes_train)\n\nCoefficients:\n(Intercept)      Glucose          BMI          Age  \n  -10.19977      0.04212      0.08450      0.04100  \n\nDegrees of Freedom: 274 Total (i.e. Null);  271 Residual\nNull Deviance:      354.5 \nResidual Deviance: 243.8    AIC: 251.8\n\n\n\n\nShow the code\nBIC_model &lt;- glm(formula = Outcome ~ Age + BMI + Glucose, family = binomial, data = diabetes_train)\nsummary(BIC_model)\n\n\n\nCall:\nglm(formula = Outcome ~ Age + BMI + Glucose, family = binomial, \n    data = diabetes_train)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -10.199770   1.287171  -7.924 2.30e-15 ***\nAge           0.040997   0.015832   2.589 0.009612 ** \nBMI           0.084503   0.024857   3.400 0.000675 ***\nGlucose       0.042124   0.006539   6.442 1.18e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 354.52  on 274  degrees of freedom\nResidual deviance: 243.82  on 271  degrees of freedom\nAIC: 251.82\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nCompute three confusion matrics for full model, AIC-selected model and BIC-selected model seperately:\n\n\n\nShow the code\ndiabetes_pred_class_full_model &lt;-\n  round(predict(full_model, type = \"response\"), 0)\ndiabetes_confusion_matrix &lt;-\n    confusionMatrix(\n    data = as.factor(diabetes_pred_class_full_model),\n    reference = as.factor(diabetes_train$Outcome),\n    positive = '1'\n)\n\ndiabetes_confusion_matrix\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 159  35\n         1  21  60\n                                          \n               Accuracy : 0.7964          \n                 95% CI : (0.7439, 0.8424)\n    No Information Rate : 0.6545          \n    P-Value [Acc &gt; NIR] : 1.8e-07         \n                                          \n                  Kappa : 0.5335          \n                                          \n Mcnemar's Test P-Value : 0.08235         \n                                          \n            Sensitivity : 0.6316          \n            Specificity : 0.8833          \n         Pos Pred Value : 0.7407          \n         Neg Pred Value : 0.8196          \n             Prevalence : 0.3455          \n         Detection Rate : 0.2182          \n   Detection Prevalence : 0.2945          \n      Balanced Accuracy : 0.7575          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\n\n\nShow the code\ndiabetes_pred_class_AIC_model &lt;-\n  round(predict(AIC_model, type = \"response\"), 0)\n\ndiabetes_confusion_matrix_AIC_model &lt;-\n    confusionMatrix(\n    data = as.factor(diabetes_pred_class_AIC_model),\n    reference = as.factor(diabetes_train$Outcome),\n    positive = '1'\n)\n\ndiabetes_confusion_matrix_AIC_model\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 160  35\n         1  20  60\n                                          \n               Accuracy : 0.8             \n                 95% CI : (0.7478, 0.8456)\n    No Information Rate : 0.6545          \n    P-Value [Acc &gt; NIR] : 8.513e-08       \n                                          \n                  Kappa : 0.5406          \n                                          \n Mcnemar's Test P-Value : 0.05906         \n                                          \n            Sensitivity : 0.6316          \n            Specificity : 0.8889          \n         Pos Pred Value : 0.7500          \n         Neg Pred Value : 0.8205          \n             Prevalence : 0.3455          \n         Detection Rate : 0.2182          \n   Detection Prevalence : 0.2909          \n      Balanced Accuracy : 0.7602          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\n\n\nShow the code\ndiabetes_pred_class_BIC_model &lt;-\n  round(predict(BIC_model, type = \"response\"), 0)\n\ndiabetes_confusion_matrix_BIC_model &lt;-\n    confusionMatrix(\n    data = as.factor(diabetes_pred_class_BIC_model),\n    reference = as.factor(diabetes_train$Outcome),\n    positive = '1'\n)\n\ndiabetes_confusion_matrix_BIC_model\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 160  36\n         1  20  59\n                                          \n               Accuracy : 0.7964          \n                 95% CI : (0.7439, 0.8424)\n    No Information Rate : 0.6545          \n    P-Value [Acc &gt; NIR] : 1.8e-07         \n                                          \n                  Kappa : 0.5311          \n                                          \n Mcnemar's Test P-Value : 0.04502         \n                                          \n            Sensitivity : 0.6211          \n            Specificity : 0.8889          \n         Pos Pred Value : 0.7468          \n         Neg Pred Value : 0.8163          \n             Prevalence : 0.3455          \n         Detection Rate : 0.2145          \n   Detection Prevalence : 0.2873          \n      Balanced Accuracy : 0.7550          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\n\nGet AUC values for these 3 candidate models:\n\n\n\nShow the code\nROC_full_log &lt;- roc(\n  response = diabetes_train$Outcome,\n  predictor = predict(full_model, type = \"response\")\n)\ncat(\"Full model AUC value:\", ROC_full_log$auc)\n\n\nFull model AUC value: 0.8774269\n\n\nShow the code\nROC_AIC_log &lt;- roc(\n  response = diabetes_train$Outcome,\n  predictor = predict(AIC_model, type = \"response\")\n)\ncat(\"AIC-selected model AUC value:\", ROC_AIC_log$auc)\n\n\nAIC-selected model AUC value: 0.8776023\n\n\nShow the code\nROC_BIC_log &lt;- roc(\n  response = diabetes_train$Outcome,\n  predictor = predict(BIC_model, type = \"response\")\n)\ncat(\"BIC-selected model AUC value:\", ROC_BIC_log$auc)\n\n\nBIC-selected model AUC value: 0.8598246\n\n\n\nUsing LASSO to get a logistic regression model and compare the prediction performance of LASSO model with those 3 models above.\n\n\n\nShow the code\nmodel_matrix_X_train &lt;-\n    as.matrix(diabetes_train[, -9])\n\nmatrix_Y_train &lt;-\n    as.matrix(diabetes_train[, 9], ncol = 1)\n\n\n\n\nShow the code\n#set.seed(271)\ndiabetes_cv_lambda_LASSO &lt;-\n  cv.glmnet(\n  x = model_matrix_X_train, y = matrix_Y_train,\n  alpha = 1,\n  family = 'binomial',\n  type.measure = 'auc',\n  nfolds = 5)\n\ndiabetes_cv_lambda_LASSO\n\n\n\nCall:  cv.glmnet(x = model_matrix_X_train, y = matrix_Y_train, type.measure = \"auc\",      nfolds = 5, alpha = 1, family = \"binomial\") \n\nMeasure: AUC \n\n     Lambda Index Measure      SE Nonzero\nmin 0.01601    31  0.8510 0.02857       5\n1se 0.07092    15  0.8241 0.03988       3\n\n\n\n\nShow the code\ndiabetes_lambda_1se_AUC_LASSO &lt;- round(diabetes_cv_lambda_LASSO$lambda.1se, 4)\n\ndiabetes_lambda_1se_AUC_LASSO\n\n\n[1] 0.0709\n\n\n\n\nShow the code\ndiabetes_LASSO_1se_AUC &lt;- glmnet(\n  x = model_matrix_X_train, y = matrix_Y_train,\n  alpha = 1,\n  family = 'binomial',\n  lambda = diabetes_lambda_1se_AUC_LASSO\n)\ncoef(diabetes_LASSO_1se_AUC)\n\n\n9 x 1 sparse Matrix of class \"dgCMatrix\"\n                                   s0\n(Intercept)              -5.220454794\nPregnancies               .          \nGlucose                   0.028193519\nBloodPressure             .          \nSkinThickness             .          \nInsulin                   .          \nBMI                       0.021872562\nDiabetesPedigreeFunction  .          \nAge                       0.008939217\n\n\n\n\nShow the code\nROC_lasso &lt;-\n    roc(\n        response = diabetes_train$Outcome,\n        predictor = predict(diabetes_LASSO_1se_AUC,\n                     newx = model_matrix_X_train)[,\"s0\"] )\nROC_lasso\n\n\n\nCall:\nroc.default(response = diabetes_train$Outcome, predictor = predict(diabetes_LASSO_1se_AUC,     newx = model_matrix_X_train)[, \"s0\"])\n\nData: predict(diabetes_LASSO_1se_AUC, newx = model_matrix_X_train)[, \"s0\"] in 180 controls (diabetes_train$Outcome 0) &lt; 95 cases (diabetes_train$Outcome 1).\nArea under the curve: 0.8496\n\n\nShow the code\nAUC_lasso &lt;- pROC::auc(ROC_lasso)\n\n\n\nCompare the AUC values for our 4 candidate models, full model, AIC-selected model, BIC-selected model and LASSO model.\n\n\n\nShow the code\nmodel_names &lt;- c(\"Full Model\", \"AIC-selected Model\", \"BIC-selected Model\", \"LASSO Model\")\nAUC_values &lt;- c(ROC_full_log$auc, ROC_AIC_log$auc, ROC_BIC_log$auc, as.double(AUC_lasso))\ncomparison_table &lt;- data.frame(Model = model_names, AUC = AUC_values)\n\ncomparison_table\n\n\n               Model       AUC\n1         Full Model 0.8774269\n2 AIC-selected Model 0.8776023\n3 BIC-selected Model 0.8598246\n4        LASSO Model 0.8496491\n\n\n\n\nResults:\nAfter comparing the AUC values for the four candidate models, we observed that the AIC-selected model had the best prediction performance on the training dataset. Consequently, we determined to adopt the AIC-selected model as our final predictive model. Next, we will assess the out-of-sample performance of our final model by applying it to the testing dataset, thereby generating the ROC curve and computing the corresponding AUC value.\n\n\nShow the code\noptions(repr.plot.width = 15, repr.plot.height = 10)\n\nmodel_X_test &lt;- diabetes_test[, -which(names(diabetes_test) == \"Outcome\")]\n\npredicted_prob_AIC &lt;- predict(AIC_model, newdata = model_X_test, type = \"response\")\npredicted_fullmodel &lt;- predict(full_model, newdata = model_X_test, type = \"response\")\npredicted_prob_BIC &lt;- predict(BIC_model, newdata = model_X_test, type = \"response\")\npredicted_lasso &lt;- predict(diabetes_LASSO_1se_AUC, newx = as.matrix(model_X_test))[,\"s0\"]\n\nROC_AIC_model_in_testdata &lt;- roc(response = diabetes_test$Outcome, predictor = predicted_prob_AIC)\nplot(ROC_AIC_model_in_testdata,  print.auc = TRUE, print.auc.x = 0.5, print.auc.y = 0.5, col = \"steelblue\", lwd = 3, lty = 2, main = \"ROC Curve of Models using Test Dataset\", cex.main = 2)\n\nROC_full_model_in_testdata &lt;- roc(response = diabetes_test$Outcome, predictor = predicted_fullmodel)\nplot(ROC_full_model_in_testdata, print.auc = TRUE, print.auc.x = 0.5, print.auc.y = 0.45, col = \"black\", lwd = 3, lty = 2, add = TRUE)\n\nROC_BIC_model_in_testdata &lt;- roc(response = diabetes_test$Outcome, predictor = predicted_prob_BIC)\nplot(ROC_BIC_model_in_testdata, print.auc = TRUE, print.auc.x = 0.5, print.auc.y = 0.40, col = \"springgreen\", lwd = 3, lty = 2, add = TRUE)\n\nROC_lasso_in_testdata &lt;- roc(response = diabetes_test$Outcome, predictor = predicted_lasso)\nplot(ROC_lasso_in_testdata, print.auc = TRUE, print.auc.x = 0.5, print.auc.y = 0.35, col = \"red\", lwd = 3, lty = 2, add = TRUE)\n\n\nlegend(\"bottomright\", legend = model_names, col = c(\"black\", \"steelblue\", \"springgreen\", \"red\"), lty = 2, lwd = 3)\n\n\n\n\n\n\n\n\n\n\n\nResult summary and conclusion:\nBased on our analysis, the AIC-selected model has the best prediction performance among the four models we compared. Upon fitting the AIC-selected model to the testing dataset, we obtained an AUC value of 0.808, which underscores the robust predictive capability of our AIC-selected model when applied to out-of-sample data.\nTherefore, we have concluded that the AIC-selected model aligns most effectively with our project’s objective of establishing a predictive model for determining the probabilty of an individual having diabetes. Below is a summary of the selected model:\n\\[\n\\begin{align*}\n\\log\\left(\\frac{p_i}{1-p_i}\\right) &= -10.620170 +1.146492 \\cdot \\text{DiabetesPedigreeFunction} + 0.040539 \\cdot \\text{Age} \\\\\n&\\quad + 0.078362 \\cdot \\text{BMI} + 0.042436 \\cdot \\text{Glucose}\n\\end{align*}\n\\]\nwhere \\(p_i\\) is the probability of the \\(i{\\text{th}}\\) individual having diabetes.\nGiven an individual’s diabetes percentage, age, BMI, and glucose level, this model can be used to predictive the probability of diabetes as\n\\[p_{i} = \\frac{1}{1+e^{-(-10.620170 + 1.146492 \\times \\text{DiabetesPedigreeFunction} + 0.040539 \\times \\text{Age} + 0.078362 \\times \\text{BMI} + 0.042436 \\times \\text{Glucose})}}\\]"
  },
  {
    "objectID": "posts/post-with-code/index.html#discussion",
    "href": "posts/post-with-code/index.html#discussion",
    "title": "Diabetes Prediction Modeling",
    "section": "Discussion:",
    "text": "Discussion:\nThrough this project, we’ve developed a predictive model to estimate the probability of an individual having diabetes. In our final model, we’ve left with four key variables, making the model has the best prediction performance: DiabetesPedigreeFunction, Age, BMI, and Glucose. Notably, all coefficients associated with these variables are positive, indicating that higher values for these factors correlate with an increased prbability of diabetes. Following model comparison and selection processes, the AIC-selected model has been determined as the optimal choice, demonstrating best predictive performance both in-sample and out-of-sample prediction among our 4 candidate models with AUC value of approximately 0.8, indicating its robust predictive capabilities.\nThe outcome of our analysis was surprising. While AIC-based stepwise selection is commonly used to explore predictor-response relationships, we opted to assess the efficacy of a LASSO model, known for predictive power and overfitting avoidance. We expected the LASSO model to outperform the AIC-selected model in out-of-sample prediction accuracy. However, results showed the AIC-selected model not only offered good interpretability but also outperformed the LASSO model in terms of AUC values. This outcome is better than we expected as it signifies a balance between model interpretability and predictive powerness in our final model.\nWhile the AIC-selected model performed best on the testing dataset, its superiority on out-of-sample data is not guaranteed. Implementing k-fold cross-validation and calculating CV-AUC values for our four candidate models can enhance our methodology. This approach assesses models across various data partitions, reducing reliance on chance results from a single train-test split. CV-AUC values may not always align with initial model selection; for instance, the Lasso model might show the highest CV-AUC value, indicating superior prediction performance. Integrating k-fold cross-validation into our model evaluation enhances our methodology’s robustness, ensuring our final predictive model is well-suited for generalization to unseen data.\nA key area for future exploration is identifying additional predictors beyond those in our dataset that could influence diabetes risk. Factors like other medical histories and pharmaceutical supplements may provide valuable insights. Additionally, we should investigate how different parameters, such as lambda values, affect the LASSO model’s performance. In our analysis we use lambda.1se value, and explore the performance with the lambda.min value. In summary, exploring new predictors and optimizing regularization parameters can enhance our predictive models and improve our ability to predict and manage diabetes."
  },
  {
    "objectID": "posts/post-with-code/index.html#references",
    "href": "posts/post-with-code/index.html#references",
    "title": "Diabetes Prediction Modeling",
    "section": "References:",
    "text": "References:\n\nRelated Study 1: Joshi, Ram D, and Chandra K Dhakal. “Predicting Type 2 Diabetes Using Logistic Regression and Machine Learning Approaches.” International Journal of Environmental Research and Public Health, U.S. National Library of Medicine, 9 July 2021, www.ncbi.nlm.nih.gov/pmc/articles/PMC8306487/.\nRelated Study 2: Chang, Victor, et al. “Pima Indians Diabetes Mellitus Classification Based on Machine Learning (ML) Algorithms.” Neural Computing & Applications, U.S. National Library of Medicine, 24 Mar. 2022, www.ncbi.nlm.nih.gov/pmc/articles/PMC8943493/.\nData source: National Institute of Diabetes and Digestive and Kidney Diseases. “Predict Diabetes.” Kaggle, 9 Nov. 2022, www.kaggle.com/datasets/whenamancodes/predict-diabities?resource=download.\nMore Details on data: “Pima Indians Diabetes Database - Dataset by Data-Society.” Data.World, 13 Dec. 2016, www.data.world/data-society/pima-indians-diabetes-database.\nROC in health data: Nahm, Francis Sahngun. “Receiver Operating Characteristic Curve: Overview and Practical Use for Clinicians.” Korean Journal of Anesthesiology, U.S. National Library of Medicine, Feb. 2022, www.ncbi.nlm.nih.gov/pmc/articles/PMC8831439/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Arnab Das",
    "section": "",
    "text": "numbers guy 📊 | math & stats nut 📈 | Python 🐍| R 📊 | Optimization ⚙️ |\na passion for numbers and data-driven insights. python and r for mathematical modeling, statistical analysis, and optimization tasks. build solutions and create impact. leverage ml to uncover patterns and trends. communicate complex findings in a digestible manner. strive for excellence and make a difference.\n\n\n“The only way to do great work is to love what you do.”"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Pulsar Star Classification",
    "section": "",
    "text": "Show the code\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(kknn)\nlibrary(cowplot)\nlibrary(rsample)\nlibrary(themis)\nlibrary(parsnip)\nlibrary(tune)\nlibrary(workflows)\nlibrary(yardstick)"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Diabetes Prediction Modeling",
    "section": "",
    "text": "Diabetes is a prevalent chronic metabolic disorder posing significant health and economic burdens globally, particularly with the recent rise in type 2 diabetes cases. Predictive modeling offers a valuable approach for identifying individuals at risk and intervening early. The dataset we analyze in this project exclusively consists of female patients aged 21 years or above, all of whom are of Pima Indian heritage. These demographic constraints ensure a focused examination of diabetes within this specific population subset. Diagnostic measurements crucial for diabetes prediction, including glucose levels, blood pressure, insulin levels, and BMI were collected through medical examinations and tests conducted by healthcare professionals. Our aim is to develop an effective tool for diabetes risk assessment to gain insights into the factors contributing to its onset, ultimately improving health outcomes and quality of life for individuals vulnerable to diabetes.\n\n\nThe dataset originates from the National Institute of Diabetes and Digestive and Kidney Diseases and is utilized to predict the probability of diabetes diagnosis in female subjects aged 21 and above. There are a total of 768 observations and 9 variables in the dataset. The target variable is Outcome which indicates the presence of diabetes. The 8 explanatory variables are: Pregnancies, Glucose, BloodPressure, Skin Thickness, Insulin, BMI, DiabetesPredigreeFunction and Age. Below are the detailed description of each explanatory varibles:\n\nPregnancies: Integer variable indicating the number of pregnancies the individual has experienced.\nGlucose: Numeric variable representing plasma glucose concentration at 2 hours in an oral glucose tolerance test, measured in mg/dL.\nBloodPressure: Numeric variable denoting the diastolic blood pressure, measured in mmHg.\nSkin Thickness: Numeric variable indicating the thickness of the triceps skin fold, measured in mm.\nInsulin: Numeric variable representing insulin levels in the bloodstream two hours after a specific event (such as the administration of glucose), measured in micro-units per milliliter of serum.\nBMI: Numeric variable representing Body Mass Index (BMI), a measure of body fat based on height and weight, measured in kg/m^2.\nDiabetesPedigreeFunction: Numeric variable representing a function which scores the likelihood of diabetes based on family history.\nAge: Integer variable indicating the age of the individual.\nOutcome: Categorical (binary) variable, where 0 represents absence of diabetes and 1 represents presence of diabetes. This variable is the target variable for prediction.\n\n\n\n\nThe primary objective of this project is to develop a predictive model capable for predicting the probability of a subject having diabetes based on their diagnostic measurements. By variable and model selection, we aim to build a “best” model for prediction among all candidate models. Through this exploration, we seek to gain insights into the underlying factors contributing to diabetes onset and create a valuable tool for diabetes risk assessment. Further analysis, such as correlation analysis, could contribute to ensuring the reliability and robustness of the observed relationships.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(GGally)\nlibrary(glmnet)\nlibrary(caret)\nlibrary(MASS)\nlibrary(pROC)\nlibrary(cowplot)\n\n\n\n\n\n\n\nShow the code\ndiabetes &lt;- read.csv(\"diabetes.csv\")\nhead(diabetes)\n\n\n  Pregnancies Glucose BloodPressure SkinThickness Insulin  BMI\n1           6     148            72            35       0 33.6\n2           1      85            66            29       0 26.6\n3           8     183            64             0       0 23.3\n4           1      89            66            23      94 28.1\n5           0     137            40            35     168 43.1\n6           5     116            74             0       0 25.6\n  DiabetesPedigreeFunction Age Outcome\n1                    0.627  50       1\n2                    0.351  31       0\n3                    0.672  32       1\n4                    0.167  21       0\n5                    2.288  33       1\n6                    0.201  30       0\n\n\nShow the code\nnrow(diabetes)\n\n\n[1] 768\n\n\n\n\n\nMissing values can introduce bias in parameter estimates and reduce their precision. Upon observing that several attributes in our dataset contain missing values, we opted to clean the data by removing these rows.\n\n\nShow the code\ndiabetes_clean &lt;- diabetes[!(diabetes$Glucose == 0 | diabetes$BloodPressure == 0 | diabetes$SkinThickness == 0 | diabetes$Insulin == 0 | diabetes$BMI == 0 | diabetes$DiabetesPedigreeFunction == 0 | diabetes$Age == 0), ]\nhead(diabetes_clean)\n\n\n   Pregnancies Glucose BloodPressure SkinThickness Insulin  BMI\n4            1      89            66            23      94 28.1\n5            0     137            40            35     168 43.1\n7            3      78            50            32      88 31.0\n9            2     197            70            45     543 30.5\n14           1     189            60            23     846 30.1\n15           5     166            72            19     175 25.8\n   DiabetesPedigreeFunction Age Outcome\n4                     0.167  21       0\n5                     2.288  33       1\n7                     0.248  26       1\n9                     0.158  53       1\n14                    0.398  59       1\n15                    0.587  51       1\n\n\nShow the code\nnrow(diabetes_clean)\n\n\n[1] 392\n\n\nShow the code\noutcome_counts &lt;- table(diabetes_clean$Outcome)\nprint(outcome_counts)\n\n\n\n  0   1 \n262 130 \n\n\n\n\n\nThe number of rows in our dataset after removing the 0 values is 392. Although the dataset size has decreased, the remaining data still provides sufficient information to explore relationships, trends, and patterns. By excluding rows with unreliable physiological measurements, we ensure the integrity and accuracy of the dataset, allowing for more reliable insights and interpretations from subsequent analyses.\nFrom some basic exploratory data analysis we see the dataset contains around one-third positive (1) outcomes, and two-thirds negative (0) outomes, they are generally balanced enough. However, it’s important to remain vigilant for potential issues related to class imbalance and to employ appropriate techniques if imbalance becomes problematic during analysis."
  },
  {
    "objectID": "posts/welcome/index.html#introduction",
    "href": "posts/welcome/index.html#introduction",
    "title": "Pulsar Star Classification",
    "section": "Introduction",
    "text": "Introduction\nNeutron stars are high-density celestial bodies caused by a large mass star collapsing on itself due to its own gravity (Goldberger, 2019). Pulsars are a classification of neutron stars which “pulse” or emit electromagnetic radiation periodically because of its high rotational velocity and strong magnetic fields. Telescopes on Earth can measure the continuous pulses of radio frequencies emitted by pulsars (Rodriguez, 2019). Identifying pulsar signals among interference and noise can be difficult, so machine learning can be used to predict target pulsars based on their recorded metrics.\nBecause the pulsar produces periodic pulsation signals they are often too weak to detect (Liu, 2017). Preprocessing this time series by folding the data with respect to the rotational period of the pulsar, yields a unique and distinguishable pulsar fingerprint referred to as an integrated pulse profile. The profile shape has an increased signal quality and form, and through long term observations are considered generally stable, as opposed to the highly variable single pulses (Liu, 2017).\nHowever, before signals reach earth, they must pass through interstellar medium (ISM). This medium consists of everything that could possibly exist between a pulsar and the Earth (Liu, 2017). There are several major effects on pulsar radio signals, namely dispersion, scintillation, and scattering (Grootjans 2016), but for the purpose of this report only dispersion will be explored. The pulsar signal can be considered a plane wave whose frequency depends on interactions between ionized components of the ISM (Grootjans, 2016).The frequency dependence of these waves causes higher frequency waves to arrive earlier and lower frequency waves to arrive later, by an amount defined by a dispersion measure (DM) (Liu, 2017). The DM of any frequency is also proportional to the distance of the pulsar. All these factors result in the broadening and reducing the peak values for the frequency distribution curve of a pulsar. When a sufficient amount of dispersion is present in the signal, a signal-to-noise ratio (SNR) preprocessing (taking into consideration antenna aperture and other properties relevant to noise) must be applied to further analyze the pulsar curve by fitting the data (Grootjans, 2016). By applying this filter, a DM-SNR curve can be produced which has removed dispersion distortions in pulse shapes.\nIn the “Predicted Pulsar Star” Kaggle dataset, an integrated profile and a DM-SNR curve is considered. These two preprocessed measurements have tabulated results for the following statistics: mean, standard deviation, excess kurtosis, and skewness of these measurements. Excess kurtosis describes the tails on a normal distribution to measure the probability of events occurring outside of the normal range, unlike skewness which refers to the symmetry of a normal distribution. In our dataset, our target_class has been identified by binary classification, where a value of 1 represents a pulsar, and the value of 0 represents a non-pulsar.\nIn this project we will explore how a star’s integrated and DM-SNR profiles affect the accuracy of K-Nearest Neighbors classification models based on these two profiles. The question we aim to answer is: can DM-SNR curve and integrated profile measurements predict pulsar stars? If so, which preprocessing technique yields the most accurate model? We hypothesize that the model built using DM-SNR curve measurements will yield the most accurate model, because it takes into account the noise of space, which is a common source of error when star signals are recorded."
  },
  {
    "objectID": "posts/welcome/index.html#methods-and-results",
    "href": "posts/welcome/index.html#methods-and-results",
    "title": "Pulsar Star Classification",
    "section": "Methods and Results",
    "text": "Methods and Results\nWe’ll start by downloading the data set and reading it into a dataframe.\n\n\nShow the code\n# Load training data from Kaggle\npulsar_data &lt;- read_csv(\"pulsar_data_train.csv\")\n\n\n\n\nShow the code\n# Update column names to be more 'code-friendly'\ncolnames(pulsar_data) &lt;- c(\"mean_integrated\", \"stdev_integrated\", \"kurtosis_integrated\", \"skew_integrated\", \"mean_DMSNR\", \"stdev_DMSNR\", \"kurtosis_DMSNR\", \"skew_DMSNR\", \"class\")\n# Covert target class to factor\npulsar_data &lt;- pulsar_data %&gt;% mutate(class=as_factor(class))\n\n\n\n\nShow the code\n# Load testing file from Kaggle\npulsar_test_unclass &lt;- read_csv(\"pulsar_data_test.csv\")\n\n\n\n\nShow the code\n#Update column names to be more 'code friendly'\ncolnames(pulsar_test_unclass) &lt;- c(\"mean_integrated\", \"stdev_integrated\", \"kurtosis_integrated\", \"skew_integrated\", \"mean_DMSNR\", \"stdev_DMSNR\", \"kurtosis_DMSNR\", \"skew_DMSNR\", \"class\")\nsummary(pulsar_test_unclass)\n\n\n mean_integrated  stdev_integrated kurtosis_integrated skew_integrated  \n Min.   :  6.18   Min.   :24.79    Min.   :-1.8760     Min.   :-1.7647  \n 1st Qu.:101.04   1st Qu.:42.41    1st Qu.: 0.0306     1st Qu.:-0.1896  \n Median :114.76   Median :47.03    Median : 0.2273     Median : 0.1865  \n Mean   :111.17   Mean   :46.62    Mean   : 0.4837     Mean   : 1.7513  \n 3rd Qu.:127.02   3rd Qu.:51.13    3rd Qu.: 0.4751     3rd Qu.: 0.9188  \n Max.   :192.62   Max.   :98.78    Max.   : 7.6084     Max.   :65.3860  \n                                   NA's   :767                          \n   mean_DMSNR        stdev_DMSNR     kurtosis_DMSNR     skew_DMSNR      \n Min.   :  0.2132   Min.   :  7.37   Min.   :-2.722   Min.   :  -1.965  \n 1st Qu.:  1.9565   1st Qu.: 14.56   1st Qu.: 5.700   1st Qu.:  33.817  \n Median :  2.8307   Median : 18.55   Median : 8.384   Median :  81.392  \n Mean   : 12.4736   Mean   : 26.43   Mean   : 8.234   Mean   : 102.869  \n 3rd Qu.:  5.5903   3rd Qu.: 28.68   3rd Qu.:10.632   3rd Qu.: 136.893  \n Max.   :223.3921   Max.   :109.71   Max.   :34.540   Max.   :1191.001  \n                    NA's   :524                       NA's   :244       \n  class        \n Mode:logical  \n NA's:5370     \n               \n               \n               \n               \n               \n\n\nWe can see that the test dataset doesn’t have any target classes. So, this dataset is not useful to us in training/testing our model, and we will need to split the training dataset.\nWe will split the pulsar data set into training and testing sets. The testing set will not be touched until the model is ready for evaluation. We are splitting the data into 75% training and 25% testing because the dataset is sufficiently large (~9,000 observations) that leaving 25% of the data for testing is enough to accurately assess the performance of our model.\n\n\nShow the code\n# Split into training and testing set\nset.seed(111)\n# Determine how many rows have missing values.\nnum_rows_missing_values &lt;- sum(apply(pulsar_data, 1, anyNA))\n# Number of rows with missing values: 3183\n# For our predictions down the road, we need to remove rows with NA values from the test set\npulsar_omit_na &lt;- na.omit(pulsar_data) #this is so our KNN model will run properly later\npulsar_split &lt;- initial_split(pulsar_omit_na, prop = 0.75, strata = class)\npulsar_train &lt;- training(pulsar_split)\npulsar_test &lt;- testing(pulsar_split)\n\n\n\nExploratory Analysis\n\n\nShow the code\nhead(pulsar_train)\n\n\n# A tibble: 6 × 9\n  mean_integrated stdev_integrated kurtosis_integrated skew_integrated\n            &lt;dbl&gt;            &lt;dbl&gt;               &lt;dbl&gt;           &lt;dbl&gt;\n1           121.              48.4              0.375          -0.0132\n2            77.0             36.2              0.713           3.39  \n3           131.              53.2              0.133          -0.297 \n4           109.              55.9              0.565           0.0562\n5            95.0             40.2              0.348           1.15  \n6           130.              46.4             -0.0466         -0.0345\n# ℹ 5 more variables: mean_DMSNR &lt;dbl&gt;, stdev_DMSNR &lt;dbl&gt;,\n#   kurtosis_DMSNR &lt;dbl&gt;, skew_DMSNR &lt;dbl&gt;, class &lt;fct&gt;\n\n\nTable 1: Raw pulsar data (training set)\nThere are eight predictive variables and one target variable. The predictors are composed of two groups of readings: the integrated profile and the DMSNR profile. Each group has the same four measurements: mean, standard deviation, kurtosis, and skew. We will use descriptive statistics and visualizations to explore the differences between the two profiles in order to determine which predictors will be most useful for our model.\n\n\nShow the code\n# Calculate the number and percentage of observations that are pulsar stars.\nnum_obs &lt;- nrow(pulsar_train)\nclass_dist &lt;- pulsar_train %&gt;% group_by(class) %&gt;% summarize(n=n(), percentage=n()/num_obs*100)\nclass_dist\n\n\n# A tibble: 2 × 3\n  class     n percentage\n  &lt;fct&gt; &lt;int&gt;      &lt;dbl&gt;\n1 0      6301      90.6 \n2 1       653       9.39\n\n\nShow the code\n#class 0 (non-pulsar stars): 90.8% of the data\n#class 1 (pulsar stars): 9.2% of the data\n\n\nTable 2: Frequencies of target classes\n\n\nShow the code\n# We will also calculate the mean of the predictors in the dataset.\n#mean of the predictors\npredictor_means &lt;- pulsar_train %&gt;% select(-class) %&gt;% map_df(~mean(., na.rm=TRUE))\npredictor_means\n\n\n# A tibble: 1 × 8\n  mean_integrated stdev_integrated kurtosis_integrated skew_integrated\n            &lt;dbl&gt;            &lt;dbl&gt;               &lt;dbl&gt;           &lt;dbl&gt;\n1            111.             46.5               0.482            1.80\n# ℹ 4 more variables: mean_DMSNR &lt;dbl&gt;, stdev_DMSNR &lt;dbl&gt;,\n#   kurtosis_DMSNR &lt;dbl&gt;, skew_DMSNR &lt;dbl&gt;\n\n\nShow the code\n#now, let's omit rows with NA values.\npulsar_train &lt;- na.omit(pulsar_train)\n\n\nTable 3: Mean of predictor variables\nThe predictors’ scale varies significantly, as evidenced by the calculated means. Since the k-nn classification algorithm is particularly sensitive to predictors with larger values, we need to scale and center the data before building the model. This ensures that predictors with larger values (e.g. mean of the integrated profile) don’t have a disproportionate effect on the classification model.\n\n\nData Visualization\nVisualizing the data can give us an idea of the distribution of predictors, and may give us an indication of which predictors will be most useful in the creation of our model.\n\n\nShow the code\ndata_long &lt;- pulsar_train %&gt;%\npivot_longer(cols = -c(class, mean_integrated, stdev_integrated, kurtosis_integrated, skew_integrated), names_to = \"Variable\", values_to = \"Value\")\ndata_long$Variable &lt;- str_replace(data_long$Variable, \"_DMSNR\", \"\")\n\noptions(repr.plot.width = 15, repr.plot.height = 10)\nintdensity_plot &lt;- ggplot(data_long, aes(x = Value, fill = Variable)) +\n  geom_density(alpha = 0.7) +\n  facet_wrap(~ Variable, scales = \"free\", nrow = 2, ncol = 2) +\n  theme_minimal() +\n  ggtitle(\"DMSNR\") +\n  theme(text = element_text(size = 15),\n       axis.title.x = element_text(hjust = 1)) +\n  guides(fill = \"none\") +\n  scale_fill_manual(values = c(\"#e64186\", \"#672e8c\", \"#2d3569\", \"#c68dd8\"))\n                                                        \ndata_long1 &lt;- pulsar_train %&gt;%\n  pivot_longer(cols = -c(class, mean_DMSNR, stdev_DMSNR, kurtosis_DMSNR, skew_DMSNR), names_to = \"Variable\", values_to = \"Value\")\ndata_long1$Variable &lt;- str_replace(data_long1$Variable, \"_integrated\", \"\")\n\nDMSNRdensity_plot &lt;- ggplot(data_long1, aes(x = Value, fill = Variable)) +\n  geom_density(alpha = 0.7) +\n  facet_wrap(~ Variable, scales = \"free\", nrow = 2, ncol = 2) +\n  theme_minimal() +\n  ggtitle(\"Integrated\") +\n  theme(text = element_text(size = 15)) +\n  ylab(\"\") +\n  xlab(\"\") +\n  scale_fill_manual(values = c(\"#e64186\", \"#672e8c\", \"#2d3569\", \"#c68dd8\"), labels = c(\"Kurtosis\", \"Mean\", \"Skewness\", \"Standard Deviation\"))\n\ncombine_plot &lt;- plot_grid(intdensity_plot, DMSNRdensity_plot, nrow = 1, ncol = 2)\nplot_grid(ggdraw() + draw_label(\"Density plots\", fontface='bold', size = 25), combine_plot, ncol=1, rel_heights=c(0.1, 1))\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# To visualize the distribution of pulsar vs non-pulsar stars, we can use a bar chart.\nclass_plot &lt;- subset(class_dist, !is.na(class)) %&gt;% ggplot(aes(x=class, y=n, fill=class)) +\n  geom_bar(stat=\"identity\") +\n  ylab(\"Count\") +\n  xlab(\"Class\")+\n  labs(title=\"Frequencies of target classes\") +\n  theme_classic()+\n  theme(text=element_text(size=20)) +\n  scale_fill_brewer(palette = \"Purples\", labels = c(\"0\" = \"non-pulsar\", \"1\" = \"pulsar\"))\nclass_plot\n\n\n\n\n\n\n\n\n\nThere are significantly more observations in the non-pulsar (0) than pulsar (1) class. Imbalanced data can result in the model inappropriately preferring the more common class. In order to account for this imbalance, we will need to upsample the pulsar class to produce an accurate model (see methods for more details on how this is done).\nTo visualize the relationship between predictors and the target class, we can create violin plots of each predictor variable stratified by target class. Unlike boxplots, violin plots allow us to visualize both the distribution and density of the underlying data. This additional detail about the data will help with our intuition when it comes to selecting predictors for our model.\nUsing those plots, we can see which predictors vary most between the two target classes, and between integrated and DM-SNR.\nThe data needs to be tidied before the violin plots can be created. Specifically, we will be moving the values of each of the predictors from their own columns into key-value pairs using the gather function.\n\n\nShow the code\ntidy_stars &lt;- pulsar_train %&gt;%\ngather(key=profile, value=value, mean_integrated, stdev_integrated, kurtosis_integrated, skew_integrated,\nmean_DMSNR, stdev_DMSNR, kurtosis_DMSNR, skew_DMSNR)\nhead(tidy_stars)\n\n\n# A tibble: 6 × 3\n  class profile         value\n  &lt;fct&gt; &lt;chr&gt;           &lt;dbl&gt;\n1 0     mean_integrated 121. \n2 0     mean_integrated  77.0\n3 0     mean_integrated 131. \n4 0     mean_integrated 109. \n5 0     mean_integrated  95.0\n6 0     mean_integrated 130. \n\n\nTable 4: Tidied pulsar data\n\n\nShow the code\n  options(repr.plot.width= 15, repr.plot.height = 10)\n  # make violin plots comparing DM-SNR Curve and Integrated Profile variables, to see how they differ\n  # between our target classes\n  # tidy the data to generate a profile column\n\n# mean violin\nmean_violin &lt;-  tidy_stars %&gt;%\n  filter(profile==\"mean_integrated\"|profile==\"mean_DMSNR\")%&gt;%\n  ggplot(aes(x=profile, y=value), trim=TRUE)+\n  geom_violin(aes(fill=class), draw_quantiles=c(0.5), colour = \"red\")+\n  xlab(\"Target class\") +\n  ylab(\"Mean Value\")+\n  labs(fill=\"Class\", title=\"Mean\")+\n  theme_minimal()+\n  theme(text=element_text(size=20)) +\n  scale_fill_brewer(palette = \"Reds\", labels = c(\"0\" = \"non-pulsar\", \"1\" = \"pulsar\"))\n\n# sd violin\n  sd_violin &lt;-  tidy_stars %&gt;%\n  filter(profile==\"stdev_integrated\"|profile==\"stdev_DMSNR\")%&gt;%\n  ggplot(aes(x=profile, y=value), trim=TRUE)+\n  geom_violin(aes(fill=class), draw_quantiles=c(0.5), colour = \"purple\")+\n  xlab(\"Profile\") +\n  ylab(\"SD Value\")+\n  labs(fill=\"Class\", title=\"Standard Deviation\")+\n  theme_minimal()+\n  theme(text=element_text(size=20)) +\n  scale_fill_brewer(palette = \"PuRd\", labels = c(\"0\" = \"non-pulsar\", \"1\" = \"pulsar\"))\n\n# kurtosis violin\nkurtosis_violin &lt;-  tidy_stars %&gt;%\n  filter(profile==\"kurtosis_integrated\"|profile==\"kurtosis_DMSNR\")%&gt;%\n    ggplot(aes(x=profile, y=value))+\n  geom_violin(aes(fill=class), draw_quantiles=c(0.5), colour = \"blue\")+\n  xlab(\"Profile\") +\n  ylab(\"Kurtosis Value\")+\n  labs(fill=\"Class\", title=\"Kurtosis\")+\n  theme_minimal()+\n  theme(text=element_text(size=20)) +\n  scale_fill_brewer(palette = \"Blues\", labels = c(\"0\" = \"non-pulsar\", \"1\" = \"pulsar\"))\n\n# skew_violin\nskew_violin &lt;-  tidy_stars %&gt;%\n  filter(profile==\"skew_integrated\"|profile==\"skew_DMSNR\")%&gt;%\n  ggplot(aes(x=profile, y=value), draw_quantiles=c(0.5))+\n  geom_violin(aes(fill=class), colour = \"darkgreen\")+\n  xlab(\"Profile\") +\n  ylab(\"Skew Value\")+\n  labs(fill=\"Class\", title=\"Skew\")+\n  theme_minimal()+\n  theme(text=element_text(size=20)) +\n  scale_fill_brewer(palette = \"Greens\", labels = c(\"0\" = \"non-pulsar\", \"1\" = \"pulsar\"))\n\ncombined_plot &lt;- plot_grid(mean_violin, sd_violin, kurtosis_violin, skew_violin, nrow = 2, ncol = 2)\nplot_grid(ggdraw() + draw_label(\"Class Across Key Factors\", fontface='bold', size = 25), combined_plot, ncol=1, rel_heights=c(0.1, 1))\n\n\n\n\n\n\n\n\n\nFigures 2-5 are violin plots of the four types of measurements that were collected (mean, standard deviation, kurtosis, and skew), stratified by target class and profile (integrated vs DMSNR). These plots allow us to determine which predictors will be most useful for building our classification model.\nFirstly, it demonstrates how the predictors vary between the two target classes. A predictor whose distribution varies significantly between the target classes (for example, in Figure 3), is likely to be more useful for our classification model than a predictor with very similar distributions.\nSecondly, these plots demonstrate the difference between the integrated and DMSNR profiles. We can see which profile varies more between the two target classes, thus allowing us to predict which profile will be more useful for building classification models.\nWe can see that while each predictor is extremely different depending on whether it is from the integrated or DM-SNR profile, all groups show a difference in the distribution and mean of the target classes within the same variable. So, each would be a useful addition to their respective models. As well, from a broader perspective, we want to use all of the scientific data available to us, since it was painstakingly collected and cleaned, and omission of a particular variable without reasonable grounds could cause us to lose accuracy in our models.\n\n\nK-Nearest Neighbors Analysis\nAfter our preliminary analysis, our next step will be to use our integrated and DM-SNR variables to predict pulsar stars using the K-Nearest Neighbors classification algorithm.\nThis will help us determine whether one of these two recording techniques is better than the other at distinguishing pulsar stars from the noise of space.\nOur first step is to set up our KNN model, using the integrated variables. We will do this in several steps. 1. Make a recipe. The recipe will preprocess our data by centering and scaling our predictors (since the ranges of our predictors differ), and upsample our target class (since it is ~90% target class 0). 2. Make a KNN spec, to tell the computer to run KNN (and with what parameters). This will include the neighbors=tune() function so we can find the value of K that gives us the highest accuracy. 3. Make our cross validation sets, so we can test our tuning model and find accuracies of different K’s. 4. Make a tuning workflow, to tune the KNN model and find the value of K giving the highest accuracy.\n\n\nShow the code\n#1. First, we make our recipe, which will preprocess our data.\nrecipe_integrated &lt;- recipe(class ~ mean_integrated + stdev_integrated + kurtosis_integrated +\nskew_integrated, data = pulsar_train) %&gt;%\n    step_scale(all_predictors()) %&gt;%\n    step_center(all_predictors())%&gt;%\n    step_upsample(class, over_ratio=1, skip=TRUE)\n\nrecipe_integrated\n\n\n\n\nShow the code\n#2. Next, we will create our model, so the computer knows how to run the KNN\nknn_tune &lt;- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) %&gt;%\n            set_engine(\"kknn\") %&gt;%\n            set_mode(\"classification\")\n\n\n\n\nShow the code\n#3. next, we make our cross-validation set to test iterations of K\nvfold &lt;- vfold_cv(pulsar_train, v = 5, strata = class)\n\n#4. now, we make a workflow to find the best value of K, to give our training model the best accuracy.\nknn_fit &lt;- workflow() %&gt;%\n        add_recipe(recipe_integrated) %&gt;%\n        add_model(knn_tune) %&gt;%\n        tune_grid(resamples = vfold, grid = 20) %&gt;%\n        collect_metrics()\n        #note: grid=20 to test 20 values of K, rather than the usual 10\n        #This is so we can confidently find the value of K with the highest accuracy by being more rigorous with our testing\n\n\nNow that our model has been tuned, we can pull the accuracy of our tuning model for each iteration of K, and plot it to show how accuracy changes depending on K.\n\n\nShow the code\n#pull accuracies\naccuracies &lt;- knn_fit %&gt;%\n       filter(.metric == \"accuracy\")\n#plot accuracy against K\ncross_val_plot &lt;- ggplot(accuracies, aes(x = neighbors , y = mean)) +\n       geom_point() +\n       geom_line() +\n       labs(x = \"Neighbours\", y = \"Accuracy Estimate\", title = \"Integrated profile model accuracy\")  +\n       theme(text = element_text(size = 20))\n\ncross_val_plot\n\n\n\n\n\n\n\n\n\nAccording to the plot above, the value of K that gives the highest accuracy is K=2. We will now make another model that uses this value of K to predict the class of a testing set. This will require us to make a new KNN spec and workflow- but we can reuse our old recipe, since it isn’t affected by choosing K.\n\n\nShow the code\n#pull the value of K giving the highest accuracy (2)\nkn_integrated &lt;- knn_fit %&gt;%\n       filter(.metric == \"accuracy\") %&gt;%\n       arrange(desc(mean)) %&gt;%\n       head(1) %&gt;%\n       pull(neighbors)\nkn_integrated\n\n\n[1] 1\n\n\n\n\nShow the code\n#make another KNN spec, this time with neighbors= our chosen value of K\nint_spec &lt;- nearest_neighbor(weight_func = \"rectangular\", neighbors = kn_integrated) %&gt;%\n              set_engine(\"kknn\") %&gt;%\n              set_mode(\"classification\")\n#make the workflow to build our final model\nint_fit &lt;- workflow() %&gt;%\n             add_recipe(recipe_integrated) %&gt;%\n             add_model(int_spec) %&gt;%\n             fit(data = pulsar_train)\n\n\nNow that we’ve build our final model, we can test it using data it hasn’t seen before, i.e. our testing set. This will tell us its overall accuracy in predicting the class of an unknown star, and how good it is at predicting pulsar stars specifically. Once we have predicted the testing set, we can gather its metrics to see its accuracy, and plot it as a confusion matrix to see how many times it predicted each class correctly, and incorrectly.\nTable 5: Integrated profile model metrics\n\n\nShow the code\n#predict the testing set using our integrated model\nint_pred &lt;- predict(int_fit, pulsar_test) %&gt;% bind_cols(pulsar_test)\n#get the accuracy of our model at predicting the test class\nint_metrics &lt;- int_pred %&gt;% metrics(truth = class, estimate=.pred_class)\n\n#make a confusion matrix\nint_mat &lt;- int_pred %&gt;%\n      conf_mat(truth = class, estimate = .pred_class)\nint_mat\n\n\n          Truth\nPrediction    0    1\n         0 2071   36\n         1   51  161\n\n\nShow the code\ntidy_int_mat &lt;- tidy(int_mat) #this is for later\n\n\nTable 6: Confusion matrix for the integrated profile model\nWe are finished with our integrated model, and can begin making our DM-SNR model. This will follow the same steps as above. Firstly, we will set up a recipe to tune the value of K to that which gives the highest accuracy of our training model. To do this, we will: 1. Make a new recipe. 2. Reuse our old KNN spec (since it doesn’t depend on our variables) 3. Reuse our old cross validation sets (since they don’t depend on our variables) 4. Make a tuning workflow, to find the best value of K.\n\n\nShow the code\n#make the recipe\nrecipe_DMSNR &lt;- recipe(class ~ mean_DMSNR + stdev_DMSNR + kurtosis_DMSNR +\nskew_DMSNR, data = pulsar_train) %&gt;%\n    step_scale(all_predictors()) %&gt;%\n    step_center(all_predictors())%&gt;%\n    step_upsample(class, over_ratio=1, skip=TRUE)\n\n#reuse KNN spec and cross validation sets\n#make our tuning workflow\nknn_fit &lt;- workflow() %&gt;%\n        add_recipe(recipe_DMSNR) %&gt;%\n        add_model(knn_tune) %&gt;%\n        tune_grid(resamples = vfold, grid = 20) %&gt;% #note: grid=20 to test 20 values of K, rather than the usual 10\n        collect_metrics()\n\n\nWith our DM-SNR tuning workflow, we can pull the accuracies of each K in our tuning model, and plot them to see how accuracy depends on K.\n\n\nShow the code\n#pull accuracies\naccuracies &lt;- knn_fit %&gt;%\n       filter(.metric == \"accuracy\")\n#make the plot\ncross_val_plot &lt;- ggplot(accuracies, aes(x = neighbors , y = mean)) +\n       geom_point() +\n       geom_line() +\n       labs(x = \"Neighbours\", y = \"Accuracy Estimate\", title = \"DM-SNR profile model accuracy\")  +\n       theme(text = element_text(size = 20))\n\ncross_val_plot\n\n\n\n\n\n\n\n\n\nThe plot tells that the value of K giving the highest accuracy of our training model is K=1. We will pull that value, and use it to build a new model to predict our testing set (and other unknown stars). We can reuse our old recipe and build a new KNN spec for our best value of K (value of K giving the highest accuracy), and a new workflow for our final DM-SNR model.\n\n\nShow the code\nkn_DMSNR &lt;- knn_fit %&gt;%\n       filter(.metric == \"accuracy\") %&gt;%\n       arrange(desc(mean)) %&gt;%\n       head(1) %&gt;%\n       pull(neighbors)\n\n#make new KNN spec with best value of K\nDMSNR_int_spec &lt;- nearest_neighbor(weight_func = \"rectangular\", neighbors = kn_DMSNR) %&gt;%\n              set_engine(\"kknn\") %&gt;%\n              set_mode(\"classification\")\n#make final workflow\nDMSNR_int_fit &lt;- workflow() %&gt;%\n             add_recipe(recipe_DMSNR) %&gt;%\n             add_model(DMSNR_int_spec) %&gt;%\n             fit(data = pulsar_train)\n\n\nNow that our final DM-SNR model is built, we will test it on data it hasn’t seen before (the testing set) to find its accuracy. We can pull the overall accuracy of the model using the metrics() function, and use a confusion matrix to show how many times our model correctly and incorrectly identified the class of the stars by comparing their true and predicted class.\nTable 7: DM-SNR model metrics\n\n\nShow the code\n#Predict the testing data\nDMSNR_int_pred &lt;- predict(DMSNR_int_fit, pulsar_test) %&gt;%\n                    bind_cols(pulsar_test)\n#get the accuracy of our model at predicting the classes of the test data\nDMSNR_int_metrics &lt;- DMSNR_int_pred %&gt;%\nmetrics(truth = class, estimate=.pred_class)\n\n#make a confusion matrix\nDMSNR_mat &lt;- DMSNR_int_pred %&gt;%\n      conf_mat(truth = class, estimate = .pred_class)\nDMSNR_mat\n\n\n          Truth\nPrediction    0    1\n         0 2038   88\n         1   84  109\n\n\nShow the code\ntidy_DMSNR_mat &lt;- tidy(DMSNR_mat)#this is for later\n\n\nTable 8: DM-SNR model confusion matrix\nFinally, to display our data we will make a grouped bar chart, showing both the overall accuracy and the accuracy of predicting pulsar stars (the true positive rate). It is important for us to look at overall accuracy and pulsar star classification accuracy because while we do want a model that has a high overall accuracy, our goal is to find the model that is the best at predicting pular stars (not just non-pulsars), so we need to compare both metrics of our models. We will also calculate a false positive rate, as the goal of our project is to correctly identify pulsar stars - so while a false negative (classifying a pulsar as a non-pulsar star) is not ideal, it is not as detrimental as false positives (classifying a non-pulsar as a pulsar) as this affects our ability to gain accurate knowledge on pulsar stars using our model.\n\n\nShow the code\n#gather the values from our accuracy estimates\n#gather overall accuracy estimates from our two models\nDMSNR_accuracy &lt;- as.numeric(DMSNR_int_metrics[1, 3])*100 #92.3%\nintegrated_accuracy &lt;- as.numeric(int_metrics[1, 3])*100 #96.8%\n\n#gather accuracy estimates of pulsar stars only from our confusian matrices\n#this is correctly identified pulsars/ total number of true pulsars *100\npulsar_integrated &lt;- as.numeric(tidy_int_mat[4,2])/\n(as.numeric(tidy_int_mat[3,2])+as.numeric(tidy_int_mat[4,2]))*100\npulsar_integrated #77.5%\n\n\n[1] 81.72589\n\n\nShow the code\npulsar_DMSNR &lt;- as.numeric(tidy_DMSNR_mat[4,2])/\n(as.numeric(tidy_DMSNR_mat[3,2])+as.numeric(tidy_DMSNR_mat[4,2]))*100\npulsar_DMSNR #58.0%\n\n\n[1] 55.32995\n\n\nShow the code\n#gather accuracy estimates of pulsar stars from confusion matrix\n#calculate false positive rate\n#this is false positive rate (0's classified as 1)/total negatives (all 0's) *100\nintegrated_falsepos &lt;- as.numeric(tidy_int_mat[2, 2])/\n(as.numeric(tidy_int_mat[1, 2])+as.numeric(tidy_int_mat[2, 2]))*100\nintegrated_falsepos #1.2%\n\n\n[1] 2.403393\n\n\nShow the code\nDMSNR_falsepos &lt;- as.numeric(tidy_DMSNR_mat[2,2])/\n((as.numeric(tidy_DMSNR_mat[1,2])+as.numeric(tidy_DMSNR_mat[2,2])))*100\nDMSNR_falsepos #4.2%\n\n\n[1] 3.95853\n\n\nNow we can plot our accuracy estimates as a grouped bar chart.\n\n\nShow the code\n#make a new dataframe for our plot\naccuracies &lt;- data.frame(\"percent_accuracy\"=c(DMSNR_accuracy, integrated_accuracy, pulsar_integrated, pulsar_DMSNR))\nnames &lt;- data.frame(\"type\"=c(\"DMSNR\", \"Integrated\", \"Integrated\", \"DMSNR\"))\nsubgroup &lt;- data.frame(\"subgroup\"=c(\"Overall Accuracy\", \"Overall Accuracy\", \"Pulsar Identification Accuracy\", \"Pulsar Identification Accuracy\"))\naccuracy_table &lt;- bind_cols(names, accuracies, subgroup)\n\n#make the plot\naccuracy_plot &lt;- ggplot(accuracy_table, aes(x=type, y=percent_accuracy, fill=subgroup))+\ngeom_bar(aes(fill=subgroup),stat=\"identity\", position=\"dodge\")+\n  labs(x=\"Type of Star Recording\", y=\"Percent Accuracy of KNN Model\", fill=\"Accuracy Type\", title=\"Overall and true positive accuracy\")+\n  theme_classic()+\ntheme(text = element_text(size=20)) +\nscale_fill_brewer(palette = \"BuGn\")\n\n\naccuracy_plot\n\n\n\n\n\n\n\n\n\nFrom this barchart, we observe that the overall accuracy and accuracy of pulsar star identification differs between our two models. In both, the integrated model shows higher accuracy."
  },
  {
    "objectID": "posts/welcome/index.html#discussion",
    "href": "posts/welcome/index.html#discussion",
    "title": "Pulsar Star Classification",
    "section": "Discussion",
    "text": "Discussion\nWe created two models using the provided metrics for the integrated and DM-SNR profiles. The first model using integrated profile predictors yielded an accuracy of 96% for all predicted classes. The second model using the DM-SNR curve predictors had a lower accuracy at 91% for all predicted classes. In addition to better overall accuracy, the integrated curve model was also more accurate when predicting true positives (i.e. correctly identifying pulsar stars). Out of 219 pulsar stars in our data set, the integrated curve model correctly identified 178 (78%), compared with the DM-SNR curve model which only identified 127 (58%). Clearly, the integrated model is better than the DM-SNR curve model at predicting pulsar stars from the noise of space. However, considering how rare pulsar stars are an 81% accuracy of our best model may not be good enough if it is to be used in scientific research. To elucidate this, we calculated a false positive rate for each model, as falsely identifying a non-pulsar as a pulsar star affects the quality of future research far more than missing a pulsar star due to a false negative. Our integrated model had a false positive rate of 1.2%, while our DM-SNR model had a false positive rate of 4.2%. So again we see our integrated model is better at distinguishing pulsar stars, would give better prediction accuracy for our key metrics, and is a better choice for use in classifying new stars.\nBy predicting the target class for each observation in the testing data set using the integrated model, we accurately differentiated pulsars from non-pulsars for each observation in the testing data set and could predict the class of recently recorded pulsar observations.\nContrary to our hypothesis that the DM-SNR curve model would be more accurate, the integrated profile model proved to be better at predicting pulsars. The reasoning for our hypothesis was that the DM-SNR curve would be a better preprocessing method which removed distortions in pulse shapes due to dispersion, in comparison to the integrated pulse profile which preprocessed the time series by folding the data with respect to the rotational period. Analyzing the integrated vs DM-SNR model’s accuracies tells us how well these recording methods can identify a pulsar star and the best variables for filtering pulsar stars from the noise of space. We used the integrated model to predict the target class for the observations in the testing dataset, differentiating pulsar stars from non-pulsars. Prediction of pulsars will be beneficial for scientists and astronomers for celestial research. Using pulsar stars, scientists can measure cosmic distances, time and search for planets beyond Earth’s solar system. Moreover, it is possible to measure how the presence of massive bodies curves space-time. By observing pulsars, the researchers have shown repeatedly that close double neutron star systems send out strong gravitational waves (Cofield 2016). Helping scientists efficiently and accurately identify pulsar stars will ultimately help advance research areas that depend on pulsar stars.\nNow that we have demonstrated a reasonably effective model using the provided data set, the next step would be to test our model out in the real world. This will give us a better idea of how useful our model might actually be to scientists interested in identifying pulsar stars, as the competition data set we used for our model may not be entirely representative of the data typically produced by radio telescopes. We may find that additional cleaning or wrangling is needed in order to deploy our model on real-world data.\nAt the end of the data analysis, several questions may arise: - Are either of our models accurate enough for scientific research? - Is another classification model better suited to this data (e.g. Random Forest)? - Which predictors need more scientific advancement until they are as useful as the others? - Our best values of K were quite low. Do we need to omit variables that are adding extra variation into our models? - How accurate are the predicted labels from the model? - Can we further improve the model or method of classification?"
  },
  {
    "objectID": "posts/welcome/index.html#works-cited",
    "href": "posts/welcome/index.html#works-cited",
    "title": "Pulsar Star Classification",
    "section": "Works cited",
    "text": "Works cited\nCofield,C.(2016, April 22). What Are Pulsars? . https://www.space.com/32661-pulsars.html\nGrootjans et al. (2016, August). Detection of Dispersed Pulsars in a Time Series by Using a Matched Filtering Approach. https://essay.utwente.nl/71435/1/GROOTJANS_MA_EWI.pdf\nGoldberger, A. (2019, January 04). Classifying pulsar stars using AI techniques. https://medium.com/duke-ai-society-blog/classifying-pulsar-stars-using-ai-techniques-d2be70c0f691\nMax Planck Institute, PULSE: The Impact of European Pulsar Science on Modern Physics, https://phys.org/news/2005-12-pulse-impact-european-pulsar-science.html#:~:text=By%20observing%20pulsars%2C%20the%20researchers,confirm%20Einstein’s%20General%20Relativity%20Theory.\n(n.d.). Ggplot2 violin plot : Quick start guide - R software and data visualization. STHDA. http://www.sthda.com/english/wiki/ggplot2-violin-plot-quick-start-guide-r-software-and-data-visualization\n(n.d.). Violin Plot. Ggplot 2. https://ggplot2.tidyverse.org/reference/geom_violin.html\nRodriguez, F. (2019, October 07). Pulsar Stars Detection. https://datauab.github.io/pulsar_stars/\nLiu K., (2017, June 26).Introduction to Pulsar, Pulsar Timing, and measuring of Pulse Time-of-Arrivals. http://ipta.phys.wvu.edu/files/student-week-2017/IPTA2017_KuoLiu_pulsartiming.pdf"
  },
  {
    "objectID": "posts/pulsarstars/index.html",
    "href": "posts/pulsarstars/index.html",
    "title": "Pulsar Star Classification",
    "section": "",
    "text": "Show the code\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(kknn)\nlibrary(cowplot)\nlibrary(rsample)\nlibrary(themis)\nlibrary(parsnip)\nlibrary(tune)\nlibrary(workflows)\nlibrary(yardstick)\nlibrary(knitr)"
  },
  {
    "objectID": "posts/pulsarstars/index.html#introduction",
    "href": "posts/pulsarstars/index.html#introduction",
    "title": "Pulsar Star Classification",
    "section": "Introduction",
    "text": "Introduction\nNeutron stars are high-density celestial bodies caused by a large mass star collapsing on itself due to its own gravity (Goldberger, 2019). Pulsars are a classification of neutron stars which “pulse” or emit electromagnetic radiation periodically because of its high rotational velocity and strong magnetic fields. Telescopes on Earth can measure the continuous pulses of radio frequencies emitted by pulsars (Rodriguez, 2019). Identifying pulsar signals among interference and noise can be difficult, so machine learning can be used to predict target pulsars based on their recorded metrics.\nBecause the pulsar produces periodic pulsation signals they are often too weak to detect (Liu, 2017). Preprocessing this time series by folding the data with respect to the rotational period of the pulsar, yields a unique and distinguishable pulsar fingerprint referred to as an integrated pulse profile. The profile shape has an increased signal quality and form, and through long term observations are considered generally stable, as opposed to the highly variable single pulses (Liu, 2017).\nHowever, before signals reach earth, they must pass through interstellar medium (ISM). This medium consists of everything that could possibly exist between a pulsar and the Earth (Liu, 2017). There are several major effects on pulsar radio signals, namely dispersion, scintillation, and scattering (Grootjans 2016), but for the purpose of this report only dispersion will be explored. The pulsar signal can be considered a plane wave whose frequency depends on interactions between ionized components of the ISM (Grootjans, 2016).The frequency dependence of these waves causes higher frequency waves to arrive earlier and lower frequency waves to arrive later, by an amount defined by a dispersion measure (DM) (Liu, 2017). The DM of any frequency is also proportional to the distance of the pulsar. All these factors result in the broadening and reducing the peak values for the frequency distribution curve of a pulsar. When a sufficient amount of dispersion is present in the signal, a signal-to-noise ratio (SNR) preprocessing (taking into consideration antenna aperture and other properties relevant to noise) must be applied to further analyze the pulsar curve by fitting the data (Grootjans, 2016). By applying this filter, a DM-SNR curve can be produced which has removed dispersion distortions in pulse shapes.\nIn the “Predicted Pulsar Star” Kaggle dataset, an integrated profile and a DM-SNR curve is considered. These two preprocessed measurements have tabulated results for the following statistics: mean, standard deviation, excess kurtosis, and skewness of these measurements. Excess kurtosis describes the tails on a normal distribution to measure the probability of events occurring outside of the normal range, unlike skewness which refers to the symmetry of a normal distribution. In our dataset, our target_class has been identified by binary classification, where a value of 1 represents a pulsar, and the value of 0 represents a non-pulsar.\nIn this project we will explore how a star’s integrated and DM-SNR profiles affect the accuracy of K-Nearest Neighbors classification models based on these two profiles. The question we aim to answer is: can DM-SNR curve and integrated profile measurements predict pulsar stars? If so, which preprocessing technique yields the most accurate model? We hypothesize that the model built using DM-SNR curve measurements will yield the most accurate model, because it takes into account the noise of space, which is a common source of error when star signals are recorded."
  },
  {
    "objectID": "posts/pulsarstars/index.html#methods-and-results",
    "href": "posts/pulsarstars/index.html#methods-and-results",
    "title": "Pulsar Star Classification",
    "section": "Methods and Results",
    "text": "Methods and Results\nWe’ll start by downloading the data set and reading it into a dataframe.\n\n\nShow the code\n# Load training data from Kaggle\npulsar_data &lt;- read_csv(\"pulsar_data_train.csv\")\n\n# Update column names to be more 'code-friendly'\ncolnames(pulsar_data) &lt;- c(\"mean_integrated\", \"stdev_integrated\", \"kurtosis_integrated\", \"skew_integrated\", \"mean_DMSNR\", \"stdev_DMSNR\", \"kurtosis_DMSNR\", \"skew_DMSNR\", \"class\")\n# Covert target class to factor\npulsar_data &lt;- pulsar_data %&gt;% mutate(class=as_factor(class))\n\n# Load testing file from Kaggle\npulsar_test_unclass &lt;- read_csv(\"pulsar_data_test.csv\")\n\n#Update column names to be more 'code friendly'\ncolnames(pulsar_test_unclass) &lt;- c(\"mean_integrated\", \"stdev_integrated\", \"kurtosis_integrated\", \"skew_integrated\", \"mean_DMSNR\", \"stdev_DMSNR\", \"kurtosis_DMSNR\", \"skew_DMSNR\", \"class\")\nsummary(pulsar_test_unclass)\n\n\n mean_integrated  stdev_integrated kurtosis_integrated skew_integrated  \n Min.   :  6.18   Min.   :24.79    Min.   :-1.8760     Min.   :-1.7647  \n 1st Qu.:101.04   1st Qu.:42.41    1st Qu.: 0.0306     1st Qu.:-0.1896  \n Median :114.76   Median :47.03    Median : 0.2273     Median : 0.1865  \n Mean   :111.17   Mean   :46.62    Mean   : 0.4837     Mean   : 1.7513  \n 3rd Qu.:127.02   3rd Qu.:51.13    3rd Qu.: 0.4751     3rd Qu.: 0.9188  \n Max.   :192.62   Max.   :98.78    Max.   : 7.6084     Max.   :65.3860  \n                                   NA's   :767                          \n   mean_DMSNR        stdev_DMSNR     kurtosis_DMSNR     skew_DMSNR      \n Min.   :  0.2132   Min.   :  7.37   Min.   :-2.722   Min.   :  -1.965  \n 1st Qu.:  1.9565   1st Qu.: 14.56   1st Qu.: 5.700   1st Qu.:  33.817  \n Median :  2.8307   Median : 18.55   Median : 8.384   Median :  81.392  \n Mean   : 12.4736   Mean   : 26.43   Mean   : 8.234   Mean   : 102.869  \n 3rd Qu.:  5.5903   3rd Qu.: 28.68   3rd Qu.:10.632   3rd Qu.: 136.893  \n Max.   :223.3921   Max.   :109.71   Max.   :34.540   Max.   :1191.001  \n                    NA's   :524                       NA's   :244       \n  class        \n Mode:logical  \n NA's:5370     \n               \n               \n               \n               \n               \n\n\nWe can see that the test dataset doesn’t have any target classes. So, this dataset is not useful to us in training/testing our model, and we will need to split the training dataset.\nWe will split the pulsar data set into training and testing sets. The testing set will not be touched until the model is ready for evaluation. We are splitting the data into 75% training and 25% testing because the dataset is sufficiently large (~9,000 observations) that leaving 25% of the data for testing is enough to accurately assess the performance of our model.\n\n\nShow the code\n# Split into training and testing set\nset.seed(111)\n# Determine how many rows have missing values.\nnum_rows_missing_values &lt;- sum(apply(pulsar_data, 1, anyNA))\n# Number of rows with missing values: 3183\n# For our predictions down the road, we need to remove rows with NA values from the test set\npulsar_omit_na &lt;- na.omit(pulsar_data) #this is so our KNN model will run properly later\npulsar_split &lt;- initial_split(pulsar_omit_na, prop = 0.75, strata = class)\npulsar_train &lt;- training(pulsar_split)\npulsar_test &lt;- testing(pulsar_split)\n\n\n\nExploratory Analysis\n\n\nShow the code\nkable(head(pulsar_train))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean_integrated\nstdev_integrated\nkurtosis_integrated\nskew_integrated\nmean_DMSNR\nstdev_DMSNR\nkurtosis_DMSNR\nskew_DMSNR\nclass\n\n\n\n\n121.15625\n48.37297\n0.3754847\n-0.0131655\n3.168896\n18.39937\n7.449874\n65.15930\n0\n\n\n76.96875\n36.17556\n0.7128979\n3.3887186\n2.399666\n17.57100\n9.414652\n102.72297\n0\n\n\n130.58594\n53.22953\n0.1334083\n-0.2972416\n2.743311\n22.36255\n8.508364\n74.03132\n0\n\n\n109.40625\n55.91252\n0.5651059\n0.0562467\n2.797659\n19.49653\n9.443282\n97.37458\n0\n\n\n95.00781\n40.21981\n0.3475781\n1.1531644\n2.770067\n18.21774\n7.851205\n70.80194\n0\n\n\n130.08594\n46.40262\n-0.0465848\n-0.0344684\n5.050167\n27.99748\n6.059849\n38.15479\n0\n\n\n\n\n\nThere are eight predictive variables and one target variable. The predictors are composed of two groups of readings: the integrated profile and the DMSNR profile. Each group has the same four measurements: mean, standard deviation, kurtosis, and skew. We will use descriptive statistics and visualizations to explore the differences between the two profiles in order to determine which predictors will be most useful for our model.\n\n\nShow the code\n# Calculate the number and percentage of observations that are pulsar stars.\nnum_obs &lt;- nrow(pulsar_train)\nclass_dist &lt;- pulsar_train %&gt;% group_by(class) %&gt;% summarize(n=n(), percentage=n()/num_obs*100)\nclass_dist\n\n\n# A tibble: 2 × 3\n  class     n percentage\n  &lt;fct&gt; &lt;int&gt;      &lt;dbl&gt;\n1 0      6301      90.6 \n2 1       653       9.39\n\n\nShow the code\n#class 0 (non-pulsar stars): 90.8% of the data\n#class 1 (pulsar stars): 9.2% of the data\n\n\n\n\nShow the code\n# We will also calculate the mean of the predictors in the dataset.\n#mean of the predictors\npredictor_means &lt;- pulsar_train %&gt;% select(-class) %&gt;% map_df(~mean(., na.rm=TRUE))\npredictor_means\n\n\n# A tibble: 1 × 8\n  mean_integrated stdev_integrated kurtosis_integrated skew_integrated\n            &lt;dbl&gt;            &lt;dbl&gt;               &lt;dbl&gt;           &lt;dbl&gt;\n1            111.             46.5               0.482            1.80\n# ℹ 4 more variables: mean_DMSNR &lt;dbl&gt;, stdev_DMSNR &lt;dbl&gt;,\n#   kurtosis_DMSNR &lt;dbl&gt;, skew_DMSNR &lt;dbl&gt;\n\n\nShow the code\n#now, let's omit rows with NA values.\npulsar_train &lt;- na.omit(pulsar_train)\n\n\nThe predictors’ scale varies significantly, as evidenced by the calculated means. Since the k-nn classification algorithm is particularly sensitive to predictors with larger values, we need to scale and center the data before building the model. This ensures that predictors with larger values (e.g. mean of the integrated profile) don’t have a disproportionate effect on the classification model.\n\n\nData Visualization\nVisualizing the data can give us an idea of the distribution of predictors, and may give us an indication of which predictors will be most useful in the creation of our model.\n\n\nShow the code\ndata_long &lt;- pulsar_train %&gt;%\npivot_longer(cols = -c(class, mean_integrated, stdev_integrated, kurtosis_integrated, skew_integrated), names_to = \"Variable\", values_to = \"Value\")\ndata_long$Variable &lt;- str_replace(data_long$Variable, \"_DMSNR\", \"\")\n\noptions(repr.plot.width = 15, repr.plot.height = 10)\nintdensity_plot &lt;- ggplot(data_long, aes(x = Value, fill = Variable)) +\n  geom_density(alpha = 0.7) +\n  facet_wrap(~ Variable, scales = \"free\", nrow = 2, ncol = 2) +\n  theme_minimal() +\n  ggtitle(\"DMSNR\") +\n  theme(text = element_text(size = 15),\n       axis.title.x = element_text(hjust = 1)) +\n  guides(fill = \"none\") +\n  scale_fill_manual(values = c(\"#e64186\", \"#672e8c\", \"#2d3569\", \"#c68dd8\"))\n                                                        \ndata_long1 &lt;- pulsar_train %&gt;%\n  pivot_longer(cols = -c(class, mean_DMSNR, stdev_DMSNR, kurtosis_DMSNR, skew_DMSNR), names_to = \"Variable\", values_to = \"Value\")\ndata_long1$Variable &lt;- str_replace(data_long1$Variable, \"_integrated\", \"\")\n\nDMSNRdensity_plot &lt;- ggplot(data_long1, aes(x = Value, fill = Variable)) +\n  geom_density(alpha = 0.7) +\n  facet_wrap(~ Variable, scales = \"free\", nrow = 2, ncol = 2) +\n  theme_minimal() +\n  ggtitle(\"Integrated\") +\n  theme(text = element_text(size = 15)) +\n  ylab(\"\") +\n  xlab(\"\") +\n  scale_fill_manual(values = c(\"#e64186\", \"#672e8c\", \"#2d3569\", \"#c68dd8\"), labels = c(\"Kurtosis\", \"Mean\", \"Skewness\", \"Standard Deviation\"))\n\ncombine_plot &lt;- plot_grid(intdensity_plot, DMSNRdensity_plot, nrow = 1, ncol = 2)\nplot_grid(ggdraw() + draw_label(\"Density plots\", fontface='bold', size = 25), combine_plot, ncol=1, rel_heights=c(0.1, 1))\n\n\n\n\n\n\n\n\n\nThese distributions provide insights into the characteristics of the data for each variable, including the presence of outliers, the symmetry of the data, and the concentration of values. The skewness and kurtosis specifically provide information about the shape of the distribution, which can be useful in understanding the underlying patterns in the data.\n\n\nShow the code\n# To visualize the distribution of pulsar vs non-pulsar stars, we can use a bar chart.\nclass_plot &lt;- subset(class_dist, !is.na(class)) %&gt;% ggplot(aes(x=class, y=n, fill=class)) +\n  geom_bar(stat=\"identity\") +\n  ylab(\"Count\") +\n  xlab(\"Class\")+\n  labs(title=\"Frequencies of target classes\") +\n  theme_classic()+\n  theme(text=element_text(size=20)) +\n  scale_fill_brewer(palette = \"Purples\", labels = c(\"0\" = \"non-pulsar\", \"1\" = \"pulsar\"))\nclass_plot\n\n\n\n\n\n\n\n\n\nThere are significantly more observations in the non-pulsar (0) than pulsar (1) class. Imbalanced data can result in the model inappropriately preferring the more common class. In order to account for this imbalance, we will need to upsample the pulsar class to produce an accurate model (see methods for more details on how this is done).\nTo visualize the relationship between predictors and the target class, we can create violin plots of each predictor variable stratified by target class. Unlike boxplots, violin plots allow us to visualize both the distribution and density of the underlying data. This additional detail about the data will help with our intuition when it comes to selecting predictors for our model.\nUsing those plots, we can see which predictors vary most between the two target classes, and between integrated and DM-SNR.\nThe data needs to be tidied before the violin plots can be created. Specifically, we will be moving the values of each of the predictors from their own columns into key-value pairs using the gather function.\n\n\nShow the code\ntidy_stars &lt;- pulsar_train %&gt;%\ngather(key=profile, value=value, mean_integrated, stdev_integrated, kurtosis_integrated, skew_integrated,\nmean_DMSNR, stdev_DMSNR, kurtosis_DMSNR, skew_DMSNR)\nkable(head(tidy_stars))\n\n\n\n\n\nclass\nprofile\nvalue\n\n\n\n\n0\nmean_integrated\n121.15625\n\n\n0\nmean_integrated\n76.96875\n\n\n0\nmean_integrated\n130.58594\n\n\n0\nmean_integrated\n109.40625\n\n\n0\nmean_integrated\n95.00781\n\n\n0\nmean_integrated\n130.08594\n\n\n\n\n\n\n\nShow the code\n  # make violin plots comparing DM-SNR Curve and Integrated Profile variables, to see how they differ\n  # between our target classes\n  # tidy the data to generate a profile column\n\n# mean violin\nmean_violin &lt;-  tidy_stars %&gt;%\n  filter(profile==\"mean_integrated\"|profile==\"mean_DMSNR\")%&gt;%\n  ggplot(aes(x=profile, y=value), trim=TRUE)+\n  geom_violin(aes(fill=class), draw_quantiles=c(0.5), colour = \"red\")+\n  xlab(\"Target class\") +\n  ylab(\"Mean Value\")+\n  labs(fill=\"Class\", title=\"Mean\")+\n  theme_minimal()+\n  theme(text=element_text(size=20)) +\n  scale_fill_brewer(palette = \"Reds\", labels = c(\"0\" = \"non-pulsar\", \"1\" = \"pulsar\"))\n\n# sd violin\n  sd_violin &lt;-  tidy_stars %&gt;%\n  filter(profile==\"stdev_integrated\"|profile==\"stdev_DMSNR\")%&gt;%\n  ggplot(aes(x=profile, y=value), trim=TRUE)+\n  geom_violin(aes(fill=class), draw_quantiles=c(0.5), colour = \"purple\")+\n  xlab(\"Profile\") +\n  ylab(\"SD Value\")+\n  labs(fill=\"Class\", title=\"Standard Deviation\")+\n  theme_minimal()+\n  theme(text=element_text(size=20)) +\n  scale_fill_brewer(palette = \"PuRd\", labels = c(\"0\" = \"non-pulsar\", \"1\" = \"pulsar\"))\n\n# kurtosis violin\nkurtosis_violin &lt;-  tidy_stars %&gt;%\n  filter(profile==\"kurtosis_integrated\"|profile==\"kurtosis_DMSNR\")%&gt;%\n    ggplot(aes(x=profile, y=value))+\n  geom_violin(aes(fill=class), draw_quantiles=c(0.5), colour = \"blue\")+\n  xlab(\"Profile\") +\n  ylab(\"Kurtosis Value\")+\n  labs(fill=\"Class\", title=\"Kurtosis\")+\n  theme_minimal()+\n  theme(text=element_text(size=20)) +\n  scale_fill_brewer(palette = \"Blues\", labels = c(\"0\" = \"non-pulsar\", \"1\" = \"pulsar\"))\n\n# skew_violin\nskew_violin &lt;-  tidy_stars %&gt;%\n  filter(profile==\"skew_integrated\"|profile==\"skew_DMSNR\")%&gt;%\n  ggplot(aes(x=profile, y=value), draw_quantiles=c(0.5))+\n  geom_violin(aes(fill=class), colour = \"darkgreen\")+\n  xlab(\"Profile\") +\n  ylab(\"Skew Value\")+\n  labs(fill=\"Class\", title=\"Skew\")+\n  theme_minimal()+\n  theme(text=element_text(size=20)) +\n  scale_fill_brewer(palette = \"Greens\", labels = c(\"0\" = \"non-pulsar\", \"1\" = \"pulsar\"))\n\ncombined_plot &lt;- plot_grid(mean_violin, sd_violin, kurtosis_violin, skew_violin, nrow = 2, ncol = 2)\nplot_grid(ggdraw() + draw_label(\"Class Across Key Factors\", fontface='bold', size = 25), combined_plot, ncol=1, rel_heights=c(0.1, 1))\n\n\n\n\n\n\n\n\n\nFigures 2-5 are violin plots of the four types of measurements that were collected (mean, standard deviation, kurtosis, and skew), stratified by target class and profile (integrated vs DMSNR). These plots allow us to determine which predictors will be most useful for building our classification model.\nFirstly, it demonstrates how the predictors vary between the two target classes. A predictor whose distribution varies significantly between the target classes (for example, in Figure 3), is likely to be more useful for our classification model than a predictor with very similar distributions.\nSecondly, these plots demonstrate the difference between the integrated and DMSNR profiles. We can see which profile varies more between the two target classes, thus allowing us to predict which profile will be more useful for building classification models.\nWe can see that while each predictor is extremely different depending on whether it is from the integrated or DM-SNR profile, all groups show a difference in the distribution and mean of the target classes within the same variable. So, each would be a useful addition to their respective models. As well, from a broader perspective, we want to use all of the scientific data available to us, since it was painstakingly collected and cleaned, and omission of a particular variable without reasonable grounds could cause us to lose accuracy in our models.\n\n\nK-Nearest Neighbors Analysis\nAfter our preliminary analysis, our next step will be to use our integrated and DM-SNR variables to predict pulsar stars using the K-Nearest Neighbors classification algorithm.\nThis will help us determine whether one of these two recording techniques is better than the other at distinguishing pulsar stars from the noise of space.\nOur first step is to set up our KNN model, using the integrated variables. We will do this in several steps. 1. Make a recipe. The recipe will preprocess our data by centering and scaling our predictors (since the ranges of our predictors differ), and upsample our target class (since it is ~90% target class 0). 2. Make a KNN spec, to tell the computer to run KNN (and with what parameters). This will include the neighbors=tune() function so we can find the value of K that gives us the highest accuracy. 3. Make our cross validation sets, so we can test our tuning model and find accuracies of different K’s. 4. Make a tuning workflow, to tune the KNN model and find the value of K giving the highest accuracy.\n\n\nShow the code\n#1. First, we make our recipe, which will preprocess our data.\nrecipe_integrated &lt;- recipe(class ~ mean_integrated + stdev_integrated + kurtosis_integrated +\nskew_integrated, data = pulsar_train) %&gt;%\n    step_scale(all_predictors()) %&gt;%\n    step_center(all_predictors())%&gt;%\n    step_upsample(class, over_ratio=1, skip=TRUE)\n\nrecipe_integrated\n\n\n\n\nShow the code\n#2. Next, we will create our model, so the computer knows how to run the KNN\nknn_tune &lt;- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) %&gt;%\n            set_engine(\"kknn\") %&gt;%\n            set_mode(\"classification\")\n\n\n\n\nShow the code\n#3. next, we make our cross-validation set to test iterations of K\nvfold &lt;- vfold_cv(pulsar_train, v = 5, strata = class)\n\n#4. now, we make a workflow to find the best value of K, to give our training model the best accuracy.\nknn_fit &lt;- workflow() %&gt;%\n        add_recipe(recipe_integrated) %&gt;%\n        add_model(knn_tune) %&gt;%\n        tune_grid(resamples = vfold, grid = 20) %&gt;%\n        collect_metrics()\n        #note: grid=20 to test 20 values of K, rather than the usual 10\n        #This is so we can confidently find the value of K with the highest accuracy by being more rigorous with our testing\n\n\nNow that our model has been tuned, we can pull the accuracy of our tuning model for each iteration of K, and plot it to show how accuracy changes depending on K.\n\n\nShow the code\n#pull accuracies\naccuracies &lt;- knn_fit %&gt;%\n       filter(.metric == \"accuracy\")\n#plot accuracy against K\ncross_val_plot &lt;- ggplot(accuracies, aes(x = neighbors , y = mean)) +\n       geom_point() +\n       geom_line() +\n       labs(x = \"Neighbours\", y = \"Accuracy Estimate\", title = \"Integrated profile model accuracy\")  +\n       theme(text = element_text(size = 20))\n\ncross_val_plot\n\n\n\n\n\n\n\n\n\nAccording to the plot above, the value of K that gives the highest accuracy is K=2. We will now make another model that uses this value of K to predict the class of a testing set. This will require us to make a new KNN spec and workflow- but we can reuse our old recipe, since it isn’t affected by choosing K.\n\n\nShow the code\n#pull the value of K giving the highest accuracy (2)\nkn_integrated &lt;- knn_fit %&gt;%\n       filter(.metric == \"accuracy\") %&gt;%\n       arrange(desc(mean)) %&gt;%\n       head(1) %&gt;%\n       pull(neighbors)\nkn_integrated\n\n\n[1] 1\n\n\n\n\nShow the code\n#make another KNN spec, this time with neighbors= our chosen value of K\nint_spec &lt;- nearest_neighbor(weight_func = \"rectangular\", neighbors = kn_integrated) %&gt;%\n              set_engine(\"kknn\") %&gt;%\n              set_mode(\"classification\")\n#make the workflow to build our final model\nint_fit &lt;- workflow() %&gt;%\n             add_recipe(recipe_integrated) %&gt;%\n             add_model(int_spec) %&gt;%\n             fit(data = pulsar_train)\n\n\nNow that we’ve build our final model, we can test it using data it hasn’t seen before, i.e. our testing set. This will tell us its overall accuracy in predicting the class of an unknown star, and how good it is at predicting pulsar stars specifically. Once we have predicted the testing set, we can gather its metrics to see its accuracy, and plot it as a confusion matrix to see how many times it predicted each class correctly, and incorrectly.\n\n\nShow the code\n#predict the testing set using our integrated model\nint_pred &lt;- predict(int_fit, pulsar_test) %&gt;% bind_cols(pulsar_test)\n#get the accuracy of our model at predicting the test class\nint_metrics &lt;- int_pred %&gt;% metrics(truth = class, estimate=.pred_class)\n\n# Generate confusion matrix\nint_mat &lt;- int_pred %&gt;%\n  conf_mat(truth = class, estimate = .pred_class)\n\n# Convert confusion matrix to data frame\nint_mat_df &lt;- as.data.frame(int_mat$table)\n\n# Plot the confusion matrix using ggplot2\nggplot(int_mat_df, aes(x = Truth, y = Prediction, fill = factor(Freq))) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = Freq), vjust = 1) +\n  labs(title = \"Confusion Matrix\",\n       x = \"Predicted Class\",\n       y = \"True Class\",\n       fill = \"Frequency\") +\n  theme_minimal() +\n  guides(fill = \"none\") +\n  scale_fill_brewer(palette = \"Blues\")\n\n\n\n\n\n\n\n\n\nShow the code\ntidy_int_mat &lt;- tidy(int_mat) #this is for later\n\n\nWe are finished with our integrated model, and can begin making our DM-SNR model. This will follow the same steps as above. Firstly, we will set up a recipe to tune the value of K to that which gives the highest accuracy of our training model. To do this, we will: 1. Make a new recipe. 2. Reuse our old KNN spec (since it doesn’t depend on our variables) 3. Reuse our old cross validation sets (since they don’t depend on our variables) 4. Make a tuning workflow, to find the best value of K.\n\n\nShow the code\n#make the recipe\nrecipe_DMSNR &lt;- recipe(class ~ mean_DMSNR + stdev_DMSNR + kurtosis_DMSNR +\nskew_DMSNR, data = pulsar_train) %&gt;%\n    step_scale(all_predictors()) %&gt;%\n    step_center(all_predictors())%&gt;%\n    step_upsample(class, over_ratio=1, skip=TRUE)\n\n#reuse KNN spec and cross validation sets\n#make our tuning workflow\nknn_fit &lt;- workflow() %&gt;%\n        add_recipe(recipe_DMSNR) %&gt;%\n        add_model(knn_tune) %&gt;%\n        tune_grid(resamples = vfold, grid = 20) %&gt;% #note: grid=20 to test 20 values of K, rather than the usual 10\n        collect_metrics()\n\n\nWith our DM-SNR tuning workflow, we can pull the accuracies of each K in our tuning model, and plot them to see how accuracy depends on K.\n\n\nShow the code\n#pull accuracies\naccuracies &lt;- knn_fit %&gt;%\n       filter(.metric == \"accuracy\")\n#make the plot\ncross_val_plot &lt;- ggplot(accuracies, aes(x = neighbors , y = mean)) +\n       geom_point() +\n       geom_line() +\n       labs(x = \"Neighbours\", y = \"Accuracy Estimate\", title = \"DM-SNR profile model accuracy\")  +\n       theme(text = element_text(size = 20))\n\ncross_val_plot\n\n\n\n\n\n\n\n\n\nThe plot tells that the value of K giving the highest accuracy of our training model is K=1. We will pull that value, and use it to build a new model to predict our testing set (and other unknown stars). We can reuse our old recipe and build a new KNN spec for our best value of K (value of K giving the highest accuracy), and a new workflow for our final DM-SNR model.\n\n\nShow the code\nkn_DMSNR &lt;- knn_fit %&gt;%\n       filter(.metric == \"accuracy\") %&gt;%\n       arrange(desc(mean)) %&gt;%\n       head(1) %&gt;%\n       pull(neighbors)\n\n#make new KNN spec with best value of K\nDMSNR_int_spec &lt;- nearest_neighbor(weight_func = \"rectangular\", neighbors = kn_DMSNR) %&gt;%\n              set_engine(\"kknn\") %&gt;%\n              set_mode(\"classification\")\n#make final workflow\nDMSNR_int_fit &lt;- workflow() %&gt;%\n             add_recipe(recipe_DMSNR) %&gt;%\n             add_model(DMSNR_int_spec) %&gt;%\n             fit(data = pulsar_train)\n\n\nNow that our final DM-SNR model is built, we will test it on data it hasn’t seen before (the testing set) to find its accuracy. We can pull the overall accuracy of the model using the metrics() function, and use a confusion matrix to show how many times our model correctly and incorrectly identified the class of the stars by comparing their true and predicted class.\n\n\nShow the code\n#Predict the testing data\nDMSNR_int_pred &lt;- predict(DMSNR_int_fit, pulsar_test) %&gt;%\n                    bind_cols(pulsar_test)\n#get the accuracy of our model at predicting the classes of the test data\nDMSNR_int_metrics &lt;- DMSNR_int_pred %&gt;%\nmetrics(truth = class, estimate=.pred_class)\n\n#make a confusion matrix\nDMSNR_mat &lt;- DMSNR_int_pred %&gt;%\n      conf_mat(truth = class, estimate = .pred_class)\n\n# Convert confusion matrix to data frame\nDMSNR_mat_df &lt;- as.data.frame(DMSNR_mat$table)\n\n# Plot the confusion matrix using ggplot2\nggplot(DMSNR_mat_df, aes(x = Truth, y = Prediction, fill = factor(Freq))) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = Freq), vjust = 1) +\n  labs(title = \"Confusion Matrix\",\n       x = \"Predicted Class\",\n       y = \"True Class\",\n       fill = \"Frequency\") +\n  theme_minimal() +\n  guides(fill = \"none\") +\n  scale_fill_brewer(palette = \"Purples\")\n\n\n\n\n\n\n\n\n\nShow the code\ntidy_DMSNR_mat &lt;- tidy(DMSNR_mat)#this is for later\n\n\nFinally, to display our data we will make a grouped bar chart, showing both the overall accuracy and the accuracy of predicting pulsar stars (the true positive rate). It is important for us to look at overall accuracy and pulsar star classification accuracy because while we do want a model that has a high overall accuracy, our goal is to find the model that is the best at predicting pular stars (not just non-pulsars), so we need to compare both metrics of our models. We will also calculate a false positive rate, as the goal of our project is to correctly identify pulsar stars - so while a false negative (classifying a pulsar as a non-pulsar star) is not ideal, it is not as detrimental as false positives (classifying a non-pulsar as a pulsar) as this affects our ability to gain accurate knowledge on pulsar stars using our model.\n\n\nShow the code\n#gather the values from our accuracy estimates\n#gather overall accuracy estimates from our two models\nDMSNR_accuracy &lt;- as.numeric(DMSNR_int_metrics[1, 3])*100 #92.3%\nintegrated_accuracy &lt;- as.numeric(int_metrics[1, 3])*100 #96.8%\n\n#gather accuracy estimates of pulsar stars only from our confusian matrices\n#this is correctly identified pulsars/ total number of true pulsars *100\npulsar_integrated &lt;- as.numeric(tidy_int_mat[4,2])/\n(as.numeric(tidy_int_mat[3,2])+as.numeric(tidy_int_mat[4,2]))*100\npulsar_integrated #77.5%\n\n\n[1] 81.72589\n\n\nShow the code\npulsar_DMSNR &lt;- as.numeric(tidy_DMSNR_mat[4,2])/\n(as.numeric(tidy_DMSNR_mat[3,2])+as.numeric(tidy_DMSNR_mat[4,2]))*100\npulsar_DMSNR #58.0%\n\n\n[1] 55.32995\n\n\nShow the code\n#gather accuracy estimates of pulsar stars from confusion matrix\n#calculate false positive rate\n#this is false positive rate (0's classified as 1)/total negatives (all 0's) *100\nintegrated_falsepos &lt;- as.numeric(tidy_int_mat[2, 2])/\n(as.numeric(tidy_int_mat[1, 2])+as.numeric(tidy_int_mat[2, 2]))*100\nintegrated_falsepos #1.2%\n\n\n[1] 2.403393\n\n\nShow the code\nDMSNR_falsepos &lt;- as.numeric(tidy_DMSNR_mat[2,2])/\n((as.numeric(tidy_DMSNR_mat[1,2])+as.numeric(tidy_DMSNR_mat[2,2])))*100\nDMSNR_falsepos #4.2%\n\n\n[1] 3.95853\n\n\nNow we can plot our accuracy estimates as a grouped bar chart.\n\n\nShow the code\n#make a new dataframe for our plot\naccuracies &lt;- data.frame(\"percent_accuracy\"=c(DMSNR_accuracy, integrated_accuracy, pulsar_integrated, pulsar_DMSNR))\nnames &lt;- data.frame(\"type\"=c(\"DMSNR\", \"Integrated\", \"Integrated\", \"DMSNR\"))\nsubgroup &lt;- data.frame(\"subgroup\"=c(\"Overall Accuracy\", \"Overall Accuracy\", \"Pulsar Identification Accuracy\", \"Pulsar Identification Accuracy\"))\naccuracy_table &lt;- bind_cols(names, accuracies, subgroup)\n\n#make the plot\naccuracy_plot &lt;- ggplot(accuracy_table, aes(x=type, y=percent_accuracy, fill=subgroup))+\ngeom_bar(aes(fill=subgroup),stat=\"identity\", position=\"dodge\")+\n  labs(x=\"Type of Star Recording\", y=\"Percent Accuracy of KNN Model\", fill=\"Accuracy Type\", title=\"Overall and true positive accuracy\")+\n  theme_classic()+\ntheme(text = element_text(size=20)) +\nscale_fill_brewer(palette = \"BuGn\")\n\n\naccuracy_plot\n\n\n\n\n\n\n\n\n\nFrom this barchart, we observe that the overall accuracy and accuracy of pulsar star identification differs between our two models. In both, the integrated model shows higher accuracy."
  },
  {
    "objectID": "posts/pulsarstars/index.html#discussion",
    "href": "posts/pulsarstars/index.html#discussion",
    "title": "Pulsar Star Classification",
    "section": "Discussion",
    "text": "Discussion\nWe created two models using the provided metrics for the integrated and DM-SNR profiles. The first model using integrated profile predictors yielded an accuracy of 96% for all predicted classes. The second model using the DM-SNR curve predictors had a lower accuracy at 91% for all predicted classes. In addition to better overall accuracy, the integrated curve model was also more accurate when predicting true positives (i.e. correctly identifying pulsar stars). Out of 219 pulsar stars in our data set, the integrated curve model correctly identified 178 (78%), compared with the DM-SNR curve model which only identified 127 (58%). Clearly, the integrated model is better than the DM-SNR curve model at predicting pulsar stars from the noise of space. However, considering how rare pulsar stars are an 81% accuracy of our best model may not be good enough if it is to be used in scientific research. To elucidate this, we calculated a false positive rate for each model, as falsely identifying a non-pulsar as a pulsar star affects the quality of future research far more than missing a pulsar star due to a false negative. Our integrated model had a false positive rate of 1.2%, while our DM-SNR model had a false positive rate of 4.2%. So again we see our integrated model is better at distinguishing pulsar stars, would give better prediction accuracy for our key metrics, and is a better choice for use in classifying new stars.\nBy predicting the target class for each observation in the testing data set using the integrated model, we accurately differentiated pulsars from non-pulsars for each observation in the testing data set and could predict the class of recently recorded pulsar observations.\nContrary to our hypothesis that the DM-SNR curve model would be more accurate, the integrated profile model proved to be better at predicting pulsars. The reasoning for our hypothesis was that the DM-SNR curve would be a better preprocessing method which removed distortions in pulse shapes due to dispersion, in comparison to the integrated pulse profile which preprocessed the time series by folding the data with respect to the rotational period. Analyzing the integrated vs DM-SNR model’s accuracies tells us how well these recording methods can identify a pulsar star and the best variables for filtering pulsar stars from the noise of space. We used the integrated model to predict the target class for the observations in the testing dataset, differentiating pulsar stars from non-pulsars. Prediction of pulsars will be beneficial for scientists and astronomers for celestial research. Using pulsar stars, scientists can measure cosmic distances, time and search for planets beyond Earth’s solar system. Moreover, it is possible to measure how the presence of massive bodies curves space-time. By observing pulsars, the researchers have shown repeatedly that close double neutron star systems send out strong gravitational waves (Cofield 2016). Helping scientists efficiently and accurately identify pulsar stars will ultimately help advance research areas that depend on pulsar stars.\nNow that we have demonstrated a reasonably effective model using the provided data set, the next step would be to test our model out in the real world. This will give us a better idea of how useful our model might actually be to scientists interested in identifying pulsar stars, as the competition data set we used for our model may not be entirely representative of the data typically produced by radio telescopes. We may find that additional cleaning or wrangling is needed in order to deploy our model on real-world data.\nAt the end of the data analysis, several questions may arise: - Are either of our models accurate enough for scientific research? - Is another classification model better suited to this data (e.g. Random Forest)? - Which predictors need more scientific advancement until they are as useful as the others? - Our best values of K were quite low. Do we need to omit variables that are adding extra variation into our models? - How accurate are the predicted labels from the model? - Can we further improve the model or method of classification?"
  },
  {
    "objectID": "posts/pulsarstars/index.html#works-cited",
    "href": "posts/pulsarstars/index.html#works-cited",
    "title": "Pulsar Star Classification",
    "section": "Works cited",
    "text": "Works cited\nCofield,C.(2016, April 22). What Are Pulsars? . https://www.space.com/32661-pulsars.html\nGrootjans et al. (2016, August). Detection of Dispersed Pulsars in a Time Series by Using a Matched Filtering Approach. https://essay.utwente.nl/71435/1/GROOTJANS_MA_EWI.pdf\nGoldberger, A. (2019, January 04). Classifying pulsar stars using AI techniques. https://medium.com/duke-ai-society-blog/classifying-pulsar-stars-using-ai-techniques-d2be70c0f691\nMax Planck Institute, PULSE: The Impact of European Pulsar Science on Modern Physics, https://phys.org/news/2005-12-pulse-impact-european-pulsar-science.html\n(n.d.). Ggplot2 violin plot : Quick start guide - R software and data visualization. STHDA. http://www.sthda.com/english/wiki/ggplot2-violin-plot-quick-start-guide-r-software-and-data-visualization\n(n.d.). Violin Plot. Ggplot 2. https://ggplot2.tidyverse.org/reference/geom_violin.html\nRodriguez, F. (2019, October 07). Pulsar Stars Detection. https://datauab.github.io/pulsar_stars/\nLiu K., (2017, June 26).Introduction to Pulsar, Pulsar Timing, and measuring of Pulse Time-of-Arrivals. http://ipta.phys.wvu.edu/files/student-week-2017/IPTA2017_KuoLiu_pulsartiming.pdf"
  },
  {
    "objectID": "posts/diabetes/index.html",
    "href": "posts/diabetes/index.html",
    "title": "Diabetes Prediction Modeling",
    "section": "",
    "text": "Diabetes is a prevalent chronic metabolic disorder posing significant health and economic burdens globally, particularly with the recent rise in type 2 diabetes cases. Predictive modeling offers a valuable approach for identifying individuals at risk and intervening early. The dataset we analyze in this project exclusively consists of female patients aged 21 years or above, all of whom are of Pima Indian heritage. These demographic constraints ensure a focused examination of diabetes within this specific population subset. Diagnostic measurements crucial for diabetes prediction, including glucose levels, blood pressure, insulin levels, and BMI were collected through medical examinations and tests conducted by healthcare professionals. Our aim is to develop an effective tool for diabetes risk assessment to gain insights into the factors contributing to its onset, ultimately improving health outcomes and quality of life for individuals vulnerable to diabetes.\n\n\nThe dataset originates from the National Institute of Diabetes and Digestive and Kidney Diseases and is utilized to predict the probability of diabetes diagnosis in female subjects aged 21 and above. There are a total of 768 observations and 9 variables in the dataset. The target variable is Outcome which indicates the presence of diabetes. The 8 explanatory variables are: Pregnancies, Glucose, BloodPressure, Skin Thickness, Insulin, BMI, DiabetesPredigreeFunction and Age. Below are the detailed description of each explanatory varibles:\n\nPregnancies: Integer variable indicating the number of pregnancies the individual has experienced.\nGlucose: Numeric variable representing plasma glucose concentration at 2 hours in an oral glucose tolerance test, measured in mg/dL.\nBloodPressure: Numeric variable denoting the diastolic blood pressure, measured in mmHg.\nSkin Thickness: Numeric variable indicating the thickness of the triceps skin fold, measured in mm.\nInsulin: Numeric variable representing insulin levels in the bloodstream two hours after a specific event (such as the administration of glucose), measured in micro-units per milliliter of serum.\nBMI: Numeric variable representing Body Mass Index (BMI), a measure of body fat based on height and weight, measured in kg/m^2.\nDiabetesPedigreeFunction: Numeric variable representing a function which scores the likelihood of diabetes based on family history.\nAge: Integer variable indicating the age of the individual.\nOutcome: Categorical (binary) variable, where 0 represents absence of diabetes and 1 represents presence of diabetes. This variable is the target variable for prediction.\n\n\n\n\nThe primary objective of this project is to develop a predictive model capable for predicting the probability of a subject having diabetes based on their diagnostic measurements. By variable and model selection, we aim to build a “best” model for prediction among all candidate models. Through this exploration, we seek to gain insights into the underlying factors contributing to diabetes onset and create a valuable tool for diabetes risk assessment. Further analysis, such as correlation analysis, could contribute to ensuring the reliability and robustness of the observed relationships.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(GGally)\nlibrary(glmnet)\nlibrary(caret)\nlibrary(MASS)\nlibrary(pROC)\nlibrary(cowplot)\nlibrary(knitr)\n\n\n\n\n\n\n\nShow the code\ndiabetes &lt;- read.csv(\"diabetes.csv\")\nkable(head(diabetes))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n5\n116\n74\n0\n0\n25.6\n0.201\n30\n0\n\n\n\n\n\nShow the code\nnrow(diabetes)\n\n\n[1] 768\n\n\n\n\n\nMissing values can introduce bias in parameter estimates and reduce their precision. Upon observing that several attributes in our dataset contain missing values, we opted to clean the data by removing these rows.\n\n\nShow the code\ndiabetes_clean &lt;- diabetes[!(diabetes$Glucose == 0 | diabetes$BloodPressure == 0 | diabetes$SkinThickness == 0 | diabetes$Insulin == 0 | diabetes$BMI == 0 | diabetes$DiabetesPedigreeFunction == 0 | diabetes$Age == 0), ]\nkable(head(diabetes_clean))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n4\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n5\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n7\n3\n78\n50\n32\n88\n31.0\n0.248\n26\n1\n\n\n9\n2\n197\n70\n45\n543\n30.5\n0.158\n53\n1\n\n\n14\n1\n189\n60\n23\n846\n30.1\n0.398\n59\n1\n\n\n15\n5\n166\n72\n19\n175\n25.8\n0.587\n51\n1\n\n\n\n\n\nShow the code\nnrow(diabetes_clean)\n\n\n[1] 392\n\n\nShow the code\noutcome_counts &lt;- table(diabetes_clean$Outcome)\nprint(outcome_counts)\n\n\n\n  0   1 \n262 130 \n\n\n\n\n\nThe number of rows in our dataset after removing the 0 values is 392. Although the dataset size has decreased, the remaining data still provides sufficient information to explore relationships, trends, and patterns. By excluding rows with unreliable physiological measurements, we ensure the integrity and accuracy of the dataset, allowing for more reliable insights and interpretations from subsequent analyses.\nFrom some basic exploratory data analysis we see the dataset contains around one-third positive (1) outcomes, and two-thirds negative (0) outomes, they are generally balanced enough. However, it’s important to remain vigilant for potential issues related to class imbalance and to employ appropriate techniques if imbalance becomes problematic during analysis."
  },
  {
    "objectID": "posts/diabetes/index.html#introduction",
    "href": "posts/diabetes/index.html#introduction",
    "title": "Diabetes Prediction Modeling",
    "section": "",
    "text": "Diabetes is a prevalent chronic metabolic disorder posing significant health and economic burdens globally, particularly with the recent rise in type 2 diabetes cases. Predictive modeling offers a valuable approach for identifying individuals at risk and intervening early. The dataset we analyze in this project exclusively consists of female patients aged 21 years or above, all of whom are of Pima Indian heritage. These demographic constraints ensure a focused examination of diabetes within this specific population subset. Diagnostic measurements crucial for diabetes prediction, including glucose levels, blood pressure, insulin levels, and BMI were collected through medical examinations and tests conducted by healthcare professionals. Our aim is to develop an effective tool for diabetes risk assessment to gain insights into the factors contributing to its onset, ultimately improving health outcomes and quality of life for individuals vulnerable to diabetes.\n\n\nThe dataset originates from the National Institute of Diabetes and Digestive and Kidney Diseases and is utilized to predict the probability of diabetes diagnosis in female subjects aged 21 and above. There are a total of 768 observations and 9 variables in the dataset. The target variable is Outcome which indicates the presence of diabetes. The 8 explanatory variables are: Pregnancies, Glucose, BloodPressure, Skin Thickness, Insulin, BMI, DiabetesPredigreeFunction and Age. Below are the detailed description of each explanatory varibles:\n\nPregnancies: Integer variable indicating the number of pregnancies the individual has experienced.\nGlucose: Numeric variable representing plasma glucose concentration at 2 hours in an oral glucose tolerance test, measured in mg/dL.\nBloodPressure: Numeric variable denoting the diastolic blood pressure, measured in mmHg.\nSkin Thickness: Numeric variable indicating the thickness of the triceps skin fold, measured in mm.\nInsulin: Numeric variable representing insulin levels in the bloodstream two hours after a specific event (such as the administration of glucose), measured in micro-units per milliliter of serum.\nBMI: Numeric variable representing Body Mass Index (BMI), a measure of body fat based on height and weight, measured in kg/m^2.\nDiabetesPedigreeFunction: Numeric variable representing a function which scores the likelihood of diabetes based on family history.\nAge: Integer variable indicating the age of the individual.\nOutcome: Categorical (binary) variable, where 0 represents absence of diabetes and 1 represents presence of diabetes. This variable is the target variable for prediction.\n\n\n\n\nThe primary objective of this project is to develop a predictive model capable for predicting the probability of a subject having diabetes based on their diagnostic measurements. By variable and model selection, we aim to build a “best” model for prediction among all candidate models. Through this exploration, we seek to gain insights into the underlying factors contributing to diabetes onset and create a valuable tool for diabetes risk assessment. Further analysis, such as correlation analysis, could contribute to ensuring the reliability and robustness of the observed relationships.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(GGally)\nlibrary(glmnet)\nlibrary(caret)\nlibrary(MASS)\nlibrary(pROC)\nlibrary(cowplot)\nlibrary(knitr)\n\n\n\n\n\n\n\nShow the code\ndiabetes &lt;- read.csv(\"diabetes.csv\")\nkable(head(diabetes))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n5\n116\n74\n0\n0\n25.6\n0.201\n30\n0\n\n\n\n\n\nShow the code\nnrow(diabetes)\n\n\n[1] 768\n\n\n\n\n\nMissing values can introduce bias in parameter estimates and reduce their precision. Upon observing that several attributes in our dataset contain missing values, we opted to clean the data by removing these rows.\n\n\nShow the code\ndiabetes_clean &lt;- diabetes[!(diabetes$Glucose == 0 | diabetes$BloodPressure == 0 | diabetes$SkinThickness == 0 | diabetes$Insulin == 0 | diabetes$BMI == 0 | diabetes$DiabetesPedigreeFunction == 0 | diabetes$Age == 0), ]\nkable(head(diabetes_clean))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n4\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n5\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n7\n3\n78\n50\n32\n88\n31.0\n0.248\n26\n1\n\n\n9\n2\n197\n70\n45\n543\n30.5\n0.158\n53\n1\n\n\n14\n1\n189\n60\n23\n846\n30.1\n0.398\n59\n1\n\n\n15\n5\n166\n72\n19\n175\n25.8\n0.587\n51\n1\n\n\n\n\n\nShow the code\nnrow(diabetes_clean)\n\n\n[1] 392\n\n\nShow the code\noutcome_counts &lt;- table(diabetes_clean$Outcome)\nprint(outcome_counts)\n\n\n\n  0   1 \n262 130 \n\n\n\n\n\nThe number of rows in our dataset after removing the 0 values is 392. Although the dataset size has decreased, the remaining data still provides sufficient information to explore relationships, trends, and patterns. By excluding rows with unreliable physiological measurements, we ensure the integrity and accuracy of the dataset, allowing for more reliable insights and interpretations from subsequent analyses.\nFrom some basic exploratory data analysis we see the dataset contains around one-third positive (1) outcomes, and two-thirds negative (0) outomes, they are generally balanced enough. However, it’s important to remain vigilant for potential issues related to class imbalance and to employ appropriate techniques if imbalance becomes problematic during analysis."
  },
  {
    "objectID": "posts/diabetes/index.html#methods-and-results",
    "href": "posts/diabetes/index.html#methods-and-results",
    "title": "Diabetes Prediction Modeling",
    "section": "Methods and Results:",
    "text": "Methods and Results:\n\nExploratory Data Analysis (EDA):\nBefore delving into specifics, it’s essential to examine the overall distribution of outcomes across variables. This exploration provides insight into how outcomes vary in response to changes in each variable.\n\nExplore the multicollinearity:\nAccording to regression assumptions, multicollinearity among explanatory variables should be avoided. If a multicollinearity problem exists in the dataset, the standard errors of estimated coefficients will be inflated, and coefficient estimates will be unstable, making it difficult to determine variable significance. Additionally, the interpretation of coefficients will be misleading. We can explore the correlation matrix for better insights.\n\n\nShow the code\n# getting correlation values between variables\ncorr_matrix &lt;- diabetes_clean %&gt;%\n  dplyr::select(- Outcome) %&gt;%\n  cor() %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"var1\") %&gt;%\n  pivot_longer(-var1, names_to = \"var2\", values_to = \"corr\")\n\n# plotting a correlation matrix\noptions(repr.plot.width = 15, repr.plot.height = 15)\ncorr_matrix %&gt;%\n  ggplot(aes(var1, var2)) +\n  geom_tile(aes(fill = corr), color = \"white\") +\n  scale_fill_distiller(\"Correlation Coefficient \\n\",\n    palette =  \"Spectral\",\n    direction = 1, limits = c(-1,1)\n  ) +\n    theme(\n        axis.text.x = element_text(\n          angle = 45, vjust = 1,\n          size = 18, hjust = 1\n        ),\n        axis.text.y = element_text(\n          vjust = 1,\n          size = 18, hjust = 1\n        ),\n        title = element_text(size = 20, face = \"bold\"),\n        legend.title = element_text(size = 18, face = \"bold\"),\n        legend.text = element_text(size = 20),\n        legend.key.size = unit(2, \"cm\"),\n        text = element_text(size = 20),\n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 25)) +\n      coord_fixed() +\n      geom_text(aes(var1, var2, label = round(corr, 2)), color = \"black\", size = 6) +\n    labs(title = \"Correlation Matrix\")\n\n\n\n\n\n\n\n\n\n\nUnderstanding variables which show correlation:\nIn the dataset analysis, several pairs of variables show significant correlations. Notably, Glucose and Insulin correlate at 0.58, indicating a regulatory response to blood sugar levels. Age and Pregnancies exhibit a correlation of 0.68, reflecting reproductive aging. BMI and SkinThickness correlate at 0.66, suggesting a link between body fat and skin thickness. BloodPressure and BMI show a correlation of 0.30, indicating a connection between hypertension and obesity. Lastly, Glucose and Age correlate at 0.34, potentially indicating age-related changes in glucose metabolism and diabetes risk. The above all shows the potential issue of multicollinearity in the dataset.\nConversely, the correlation between other variables appears to be within acceptable ranges, suggesting that they are not significantly affected by multicollinearity. Therefore, we need to addresse multicollinearity issue by some techniques such as variable selection or regularization methods,improving the robustness of the regression model.\n\n\n\nDistribution of predictors:\nThe density plots for the variables in this dataset illustrate the distribution of each variable’s values. This visualization helps in understanding the spread, central tendency, and shape of the data for variables such as Pregnancies, Glucose, BloodPressure, Skin Thickness, Insulin, BMI, DiabetesPedigreeFunction, and Age. These plots offer insights into the prevalence and distribution of key factors associated with diabetes diagnosis in the female subjects aged 21 and above.\n\n\nShow the code\ndata_long &lt;- diabetes_clean %&gt;%\n  pivot_longer(cols = -Outcome, names_to = \"Variable\", values_to = \"Value\")\n\noptions(repr.plot.width = 15, repr.plot.height = 10)\ndensity_plot &lt;- ggplot(data_long, aes(x = Value, fill = Variable)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~ Variable, scales = \"free\", nrow = 2, ncol = 4) +\n  theme_minimal() +\n  ggtitle(\"Density Plots of Factors\") +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\", size = 25),\n        text = element_text(size = 15)) +\n  guides(fill = \"none\")\n\ndensity_plot\n\n\n\n\n\n\n\n\n\n\nUnderstanding the distributions:\nAge skews right, indicating a younger population. Blood Pressure and BMI are normally distributed, representing the population. DiabetesPedigreeFunction and Insulin skew right, with low values prevalent; Glucose is normally distributed; Pregnancies skew right, suggesting fewer are common; Skin Thickness is nearly normal, peaking at lower values. These patterns aid in understanding population demographics and physiological factors influencing diabetes prediction.\n\n\n\nObserving the relationship between each predictor variable and the outcome:\nWe aim to gain insights into the relationship between each explanatory variable and the response variable before conducting regression analysis. Given the binary nature of the response variable, utilizing boxplots to visualize the relationship between each explanatory variable and the response variable offers a convenient approach.\n\n\nShow the code\nfunction_plot &lt;- ggplot(data = diabetes_clean, aes(x = factor(Outcome), y =DiabetesPedigreeFunction, fill = factor(Outcome))) +\n  geom_boxplot(colour = \"red\") +\n  labs(title = \"DiabetesPedigreeFunction vs. Outcome\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Diabetes Pedigree Function\") + theme(\n    text = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.title = element_text(size = 15)\n  ) +\n  scale_fill_brewer(palette = \"Reds\", labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\"))+\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  theme(text = element_text(size = 15))\n\nSkin_plot &lt;- ggplot(data = diabetes_clean, aes(x = as.factor(Outcome), y =SkinThickness, fill = factor(Outcome))) +\n  geom_boxplot(colour = \"purple\") +\n  labs(title = \"Skin Thickness vs. Outcome\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Skin Thickness\") + theme(\n    text = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.title = element_text(size = 15)\n) +\n  scale_fill_brewer(palette = \"PuRd\", labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\"))+\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  theme(text = element_text(size = 15))\n\nglucose_plot &lt;- ggplot(data = diabetes_clean, aes(x = as.factor(Outcome), y =Glucose, fill = factor(Outcome))) +\n  geom_boxplot(colour = \"blue\") +\n  labs(title = \"Glucose vs. Outcome\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Glucose Level\") + theme(\n    text = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.title = element_text(size = 15)\n  ) +\n  scale_fill_brewer(palette = \"Blues\", labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\"))+\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  theme(text = element_text(size = 15))\n\nbloodPressure_plot &lt;- ggplot(data = diabetes_clean, aes(x = as.factor(Outcome), y =BloodPressure, fill = factor(Outcome))) +\n  geom_boxplot(colour = \"red\") +\n  labs(title = \"Blood Pressure vs. Outcome\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Blood Pressure\") + theme(\n    text = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.title = element_text(size = 15)\n  ) +\n  scale_fill_brewer(palette = \"RdPu\", labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\"))+\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  theme(text = element_text(size = 15))\n\nInsulin_plot &lt;- ggplot(data = diabetes_clean, aes(x = as.factor(Outcome), y =Insulin, fill = factor(Outcome))) +\n  geom_boxplot(colour = \"darkgreen\") +\n  labs(title = \"Insulin vs. Outcome\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Insulin\") + theme(\n    text = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.title = element_text(size = 15)\n  )  +\n  scale_fill_brewer(palette = \"BuGn\", labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\"))+\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  theme(text = element_text(size = 15))\n\nBMI_plot &lt;- ggplot(data = diabetes_clean, aes(x = as.factor(Outcome), y =BMI, fill = factor(Outcome))) +\n  geom_boxplot(colour = \"darkgreen\") +\n  labs(title = \"BMI vs. Outcome\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Body Mass Index\")  + theme(\n    text = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.title = element_text(size = 15)\n  )  +\n  scale_fill_brewer(palette = \"Greens\", labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  theme(text = element_text(size = 15))\n\nAge_plot &lt;- ggplot(data = diabetes_clean, aes(x = as.factor(Outcome), y =Age, fill = factor(Outcome))) +\n  geom_boxplot(colour = \"red\") +\n  labs(title = \"Age vs. Outcome\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Age\")  + theme(\n    text = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.title = element_text(size = 15)\n  )  +\n  scale_fill_brewer(palette = \"OrRd\", labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  theme(text = element_text(size = 15))\n\nPregnancies_plot &lt;- ggplot(data = diabetes_clean, aes(x = as.factor(Outcome), y =Pregnancies, fill = factor(Outcome))) +\n  geom_boxplot(colour = \"DarkBlue\") +\n  labs(title = \"Pregnancies vs. Outcome\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Number of Pregnancies\") + theme(\n    text = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.title = element_text(size = 15)\n  )  +\n  scale_fill_brewer(palette = \"BuPu\", labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  theme(text = element_text(size = 15))\n\ncombined_plot &lt;- plot_grid(function_plot, Skin_plot, glucose_plot, bloodPressure_plot, Insulin_plot, BMI_plot, Age_plot, Pregnancies_plot, nrow = 2, ncol = 4)\n\noptions(repr.plot.width = 20, repr.plot.height = 15)\nplot_grid(ggdraw() + draw_label(\"Diabetes Outcome Across Key Factors\", fontface='bold', size = 25), combined_plot, ncol=1, rel_heights=c(0.1, 1))\n\n\n\n\n\n\n\n\n\n\nConclusion from boxplots:\n\nFrom the eight boxplots above, a notable disparity emerges in the mean glucose levels between individuals with and without diabetes. Specifically, the mean glucose level appears markedly higher among those with diabetes compared to those without, suggesting a positive association between glucose level and diabetes. Given this observation, further investigation into the relationship between glucose level and diabetes outcome is warranted.\nMoreover, upon inspecting the boxplot depicting diabetes status against age, a similar pattern emerges. Individuals diagnosed with diabetes have a higher mean age compared to those without. Consequently, it can be inferred that both age and glucose level are potentially significant explanatory variables associated with diabetes outcome.\nAdditionally, it’s noteworthy that the mean values of other variables exhibit slight variations based on whether individuals have diabetes or not. Specifically, when an individual has diabetes, the mean values of all eight predictor variables are higher compared to when the person doesn’t have diabetes, suggesting a potentially positive relationship between each X and Y to some extent.\n\n\n\n\n\nMethods (plan):\n\nModel selection methods:\n\nWe will begin with a full model incorporating all eight variables, then use backward selection based on AIC and BIC to refine our model selection. This process yields two models: an AIC-selected model and a BIC-selected model. Backward selection eliminates a non-significant predictor from the model in each interaction, resulting in an interpretable final model. AIC and BIC serve as suitable selection criteria due to the binary nature of the response variable in our dataset. Unlike adjusted \\(R^2\\) or residual mean square, AIC and BIC focus on maximizing the likelihood of the data while penalizing model complexity. For comparision, AIC emphasizes maximizing the likelihood, BIC adds a higher penalty for decreasing model complexity, favoring a more straightforward and simpler model.\nGiven the objective of setting up a model for prediction, avoiding model overfitting and reducing the variance of estimated cofficients are important concerns we need to consider. To address this concern, LASSO regression gives us a great advantage in terms of effectively shrinking some coefficients to zero, thereby increasing the model’s generalizability to out-of-sample data. Therefore, we will also incorporate a LASSO regression model into our analysis to serve as another candidate model.\nWe will compare the predictive performance of four candidate models: the full model, AIC-selected model, BIC-selected model, and LASSO model. After splitting the data into training and testing subsets, we will build up each model using the training dataset and evaluate their performance based on AUC values. The model with the highest AUC value was selected as the final model. We will then assess the generalization ability of the final model by fitting it to the testing dataset and computing the AUC value. Using a probability threshold of 0.5, we classify individuals as “1” or “0” accordingly. Additionally, we will compute confusion matrices and evaluated metrics such as Accuracy and Precision to determine the best model for predicting diabetes status. This comprehensive approach allows us to identify the most effective model for our predictive task.\n\n\n\nImplementation of a proposed model:\n\nSpliting the data into training set and testing set:\n\n\n\nShow the code\nset.seed(123)\ntraining.samples &lt;- diabetes_clean$Outcome %&gt;%\n  createDataPartition(p = 0.7, list = FALSE)\ndiabetes_train  &lt;- diabetes_clean[training.samples, ]\ndiabetes_test &lt;- diabetes_clean[-training.samples, ]\nnrow(diabetes_train)\n\n\n[1] 275\n\n\nShow the code\nnrow(diabetes_test)\n\n\n[1] 117\n\n\n\nFit the full logistic regression model using training dataset:\n\n\n\nShow the code\nfull_model &lt;- glm(formula = Outcome ~ ., family = binomial, data = diabetes_train)\nsummary(full_model)\n\n\n\nCall:\nglm(formula = Outcome ~ ., family = binomial, data = diabetes_train)\n\nCoefficients:\n                           Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              -1.050e+01  1.482e+00  -7.087 1.37e-12 ***\nPregnancies              -1.740e-02  6.880e-02  -0.253   0.8004    \nGlucose                   4.394e-02  7.526e-03   5.838 5.27e-09 ***\nBloodPressure            -7.669e-03  1.558e-02  -0.492   0.6225    \nSkinThickness             3.943e-03  2.082e-02   0.189   0.8498    \nInsulin                  -6.144e-04  1.651e-03  -0.372   0.7097    \nBMI                       8.170e-02  3.462e-02   2.360   0.0183 *  \nDiabetesPedigreeFunction  1.121e+00  5.238e-01   2.139   0.0324 *  \nAge                       4.648e-02  2.312e-02   2.010   0.0444 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 354.52  on 274  degrees of freedom\nResidual deviance: 238.11  on 266  degrees of freedom\nAIC: 256.11\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nBackward selection based on AIC to get a AIC-selected model:\n\n\n\nShow the code\nAIC_selection &lt;- stepAIC(full_model, method = \"backward\")\n\n\nStart:  AIC=256.11\nOutcome ~ Pregnancies + Glucose + BloodPressure + SkinThickness + \n    Insulin + BMI + DiabetesPedigreeFunction + Age\n\n                           Df Deviance    AIC\n- SkinThickness             1   238.15 254.15\n- Pregnancies               1   238.18 254.18\n- Insulin                   1   238.25 254.25\n- BloodPressure             1   238.35 254.35\n&lt;none&gt;                          238.11 256.11\n- Age                       1   242.40 258.40\n- DiabetesPedigreeFunction  1   243.01 259.01\n- BMI                       1   244.01 260.01\n- Glucose                   1   282.11 298.11\n\nStep:  AIC=254.15\nOutcome ~ Pregnancies + Glucose + BloodPressure + Insulin + BMI + \n    DiabetesPedigreeFunction + Age\n\n                           Df Deviance    AIC\n- Pregnancies               1   238.21 252.21\n- Insulin                   1   238.29 252.29\n- BloodPressure             1   238.40 252.40\n&lt;none&gt;                          238.15 254.15\n- Age                       1   242.62 256.62\n- DiabetesPedigreeFunction  1   243.09 257.10\n- BMI                       1   247.97 261.97\n- Glucose                   1   282.15 296.15\n\nStep:  AIC=252.21\nOutcome ~ Glucose + BloodPressure + Insulin + BMI + DiabetesPedigreeFunction + \n    Age\n\n                           Df Deviance    AIC\n- Insulin                   1   238.34 250.34\n- BloodPressure             1   238.46 250.46\n&lt;none&gt;                          238.21 252.21\n- DiabetesPedigreeFunction  1   243.20 255.20\n- Age                       1   244.89 256.89\n- BMI                       1   248.19 260.19\n- Glucose                   1   282.19 294.19\n\nStep:  AIC=250.34\nOutcome ~ Glucose + BloodPressure + BMI + DiabetesPedigreeFunction + \n    Age\n\n                           Df Deviance    AIC\n- BloodPressure             1   238.56 248.56\n&lt;none&gt;                          238.34 250.34\n- DiabetesPedigreeFunction  1   243.28 253.28\n- Age                       1   244.93 254.93\n- BMI                       1   248.29 258.29\n- Glucose                   1   293.71 303.71\n\nStep:  AIC=248.56\nOutcome ~ Glucose + BMI + DiabetesPedigreeFunction + Age\n\n                           Df Deviance    AIC\n&lt;none&gt;                          238.56 248.56\n- DiabetesPedigreeFunction  1   243.82 251.82\n- Age                       1   245.01 253.01\n- BMI                       1   249.26 257.26\n- Glucose                   1   293.72 301.72\n\n\nShow the code\nAIC_selection\n\n\n\nCall:  glm(formula = Outcome ~ Glucose + BMI + DiabetesPedigreeFunction + \n    Age, family = binomial, data = diabetes_train)\n\nCoefficients:\n             (Intercept)                   Glucose                       BMI  \n               -10.62017                   0.04244                   0.07836  \nDiabetesPedigreeFunction                       Age  \n                 1.14649                   0.04054  \n\nDegrees of Freedom: 274 Total (i.e. Null);  270 Residual\nNull Deviance:      354.5 \nResidual Deviance: 238.6    AIC: 248.6\n\n\n\n\nShow the code\nAIC_model &lt;- glm(formula = Outcome ~ DiabetesPedigreeFunction + Age + BMI + Glucose, family = binomial, data = diabetes_train)\nsummary(AIC_model)\n\n\n\nCall:\nglm(formula = Outcome ~ DiabetesPedigreeFunction + Age + BMI + \n    Glucose, family = binomial, data = diabetes_train)\n\nCoefficients:\n                           Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              -10.620170   1.335864  -7.950 1.86e-15 ***\nDiabetesPedigreeFunction   1.146492   0.520234   2.204  0.02754 *  \nAge                        0.040539   0.016146   2.511  0.01205 *  \nBMI                        0.078362   0.025096   3.122  0.00179 ** \nGlucose                    0.042436   0.006673   6.360 2.02e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 354.52  on 274  degrees of freedom\nResidual deviance: 238.56  on 270  degrees of freedom\nAIC: 248.56\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nBackward selection based on BIC to get a BIC-selected model:\n\n\n\nShow the code\nBIC_selection &lt;- step(full_model, direction = \"backward\", k = log(nrow(diabetes_clean)), trace = FALSE)\nBIC_selection\n\n\n\nCall:  glm(formula = Outcome ~ Glucose + BMI + Age, family = binomial, \n    data = diabetes_train)\n\nCoefficients:\n(Intercept)      Glucose          BMI          Age  \n  -10.19977      0.04212      0.08450      0.04100  \n\nDegrees of Freedom: 274 Total (i.e. Null);  271 Residual\nNull Deviance:      354.5 \nResidual Deviance: 243.8    AIC: 251.8\n\n\n\n\nShow the code\nBIC_model &lt;- glm(formula = Outcome ~ Age + BMI + Glucose, family = binomial, data = diabetes_train)\nsummary(BIC_model)\n\n\n\nCall:\nglm(formula = Outcome ~ Age + BMI + Glucose, family = binomial, \n    data = diabetes_train)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -10.199770   1.287171  -7.924 2.30e-15 ***\nAge           0.040997   0.015832   2.589 0.009612 ** \nBMI           0.084503   0.024857   3.400 0.000675 ***\nGlucose       0.042124   0.006539   6.442 1.18e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 354.52  on 274  degrees of freedom\nResidual deviance: 243.82  on 271  degrees of freedom\nAIC: 251.82\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nCompute three confusion matrics for full model, AIC-selected model and BIC-selected model seperately:\n\n\n\nShow the code\ndiabetes_pred_class_full_model &lt;-\n  round(predict(full_model, type = \"response\"), 0)\ndiabetes_confusion_matrix &lt;-\n    confusionMatrix(\n    data = as.factor(diabetes_pred_class_full_model),\n    reference = as.factor(diabetes_train$Outcome),\n    positive = '1'\n)\n\ndiabetes_confusion_matrix\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 159  35\n         1  21  60\n                                          \n               Accuracy : 0.7964          \n                 95% CI : (0.7439, 0.8424)\n    No Information Rate : 0.6545          \n    P-Value [Acc &gt; NIR] : 1.8e-07         \n                                          \n                  Kappa : 0.5335          \n                                          \n Mcnemar's Test P-Value : 0.08235         \n                                          \n            Sensitivity : 0.6316          \n            Specificity : 0.8833          \n         Pos Pred Value : 0.7407          \n         Neg Pred Value : 0.8196          \n             Prevalence : 0.3455          \n         Detection Rate : 0.2182          \n   Detection Prevalence : 0.2945          \n      Balanced Accuracy : 0.7575          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\n\n\nShow the code\ndiabetes_pred_class_AIC_model &lt;-\n  round(predict(AIC_model, type = \"response\"), 0)\n\ndiabetes_confusion_matrix_AIC_model &lt;-\n    confusionMatrix(\n    data = as.factor(diabetes_pred_class_AIC_model),\n    reference = as.factor(diabetes_train$Outcome),\n    positive = '1'\n)\n\ndiabetes_confusion_matrix_AIC_model\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 160  35\n         1  20  60\n                                          \n               Accuracy : 0.8             \n                 95% CI : (0.7478, 0.8456)\n    No Information Rate : 0.6545          \n    P-Value [Acc &gt; NIR] : 8.513e-08       \n                                          \n                  Kappa : 0.5406          \n                                          \n Mcnemar's Test P-Value : 0.05906         \n                                          \n            Sensitivity : 0.6316          \n            Specificity : 0.8889          \n         Pos Pred Value : 0.7500          \n         Neg Pred Value : 0.8205          \n             Prevalence : 0.3455          \n         Detection Rate : 0.2182          \n   Detection Prevalence : 0.2909          \n      Balanced Accuracy : 0.7602          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\n\n\nShow the code\ndiabetes_pred_class_BIC_model &lt;-\n  round(predict(BIC_model, type = \"response\"), 0)\n\ndiabetes_confusion_matrix_BIC_model &lt;-\n    confusionMatrix(\n    data = as.factor(diabetes_pred_class_BIC_model),\n    reference = as.factor(diabetes_train$Outcome),\n    positive = '1'\n)\n\ndiabetes_confusion_matrix_BIC_model\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 160  36\n         1  20  59\n                                          \n               Accuracy : 0.7964          \n                 95% CI : (0.7439, 0.8424)\n    No Information Rate : 0.6545          \n    P-Value [Acc &gt; NIR] : 1.8e-07         \n                                          \n                  Kappa : 0.5311          \n                                          \n Mcnemar's Test P-Value : 0.04502         \n                                          \n            Sensitivity : 0.6211          \n            Specificity : 0.8889          \n         Pos Pred Value : 0.7468          \n         Neg Pred Value : 0.8163          \n             Prevalence : 0.3455          \n         Detection Rate : 0.2145          \n   Detection Prevalence : 0.2873          \n      Balanced Accuracy : 0.7550          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\n\nGet AUC values for these 3 candidate models:\n\n\n\nShow the code\nROC_full_log &lt;- roc(\n  response = diabetes_train$Outcome,\n  predictor = predict(full_model, type = \"response\")\n)\ncat(\"Full model AUC value:\", ROC_full_log$auc)\n\n\nFull model AUC value: 0.8774269\n\n\nShow the code\nROC_AIC_log &lt;- roc(\n  response = diabetes_train$Outcome,\n  predictor = predict(AIC_model, type = \"response\")\n)\ncat(\"AIC-selected model AUC value:\", ROC_AIC_log$auc)\n\n\nAIC-selected model AUC value: 0.8776023\n\n\nShow the code\nROC_BIC_log &lt;- roc(\n  response = diabetes_train$Outcome,\n  predictor = predict(BIC_model, type = \"response\")\n)\ncat(\"BIC-selected model AUC value:\", ROC_BIC_log$auc)\n\n\nBIC-selected model AUC value: 0.8598246\n\n\n\nUsing LASSO to get a logistic regression model and compare the prediction performance of LASSO model with those 3 models above.\n\n\n\nShow the code\nmodel_matrix_X_train &lt;-\n    as.matrix(diabetes_train[, -9])\n\nmatrix_Y_train &lt;-\n    as.matrix(diabetes_train[, 9], ncol = 1)\n\n\n\n\nShow the code\n#set.seed(271)\ndiabetes_cv_lambda_LASSO &lt;-\n  cv.glmnet(\n  x = model_matrix_X_train, y = matrix_Y_train,\n  alpha = 1,\n  family = 'binomial',\n  type.measure = 'auc',\n  nfolds = 5)\n\ndiabetes_cv_lambda_LASSO\n\n\n\nCall:  cv.glmnet(x = model_matrix_X_train, y = matrix_Y_train, type.measure = \"auc\",      nfolds = 5, alpha = 1, family = \"binomial\") \n\nMeasure: AUC \n\n     Lambda Index Measure      SE Nonzero\nmin 0.01601    31  0.8510 0.02857       5\n1se 0.07092    15  0.8241 0.03988       3\n\n\n\n\nShow the code\ndiabetes_lambda_1se_AUC_LASSO &lt;- round(diabetes_cv_lambda_LASSO$lambda.1se, 4)\n\ndiabetes_lambda_1se_AUC_LASSO\n\n\n[1] 0.0709\n\n\n\n\nShow the code\ndiabetes_LASSO_1se_AUC &lt;- glmnet(\n  x = model_matrix_X_train, y = matrix_Y_train,\n  alpha = 1,\n  family = 'binomial',\n  lambda = diabetes_lambda_1se_AUC_LASSO\n)\ncoef(diabetes_LASSO_1se_AUC)\n\n\n9 x 1 sparse Matrix of class \"dgCMatrix\"\n                                   s0\n(Intercept)              -5.220454794\nPregnancies               .          \nGlucose                   0.028193519\nBloodPressure             .          \nSkinThickness             .          \nInsulin                   .          \nBMI                       0.021872562\nDiabetesPedigreeFunction  .          \nAge                       0.008939217\n\n\n\n\nShow the code\nROC_lasso &lt;-\n    roc(\n        response = diabetes_train$Outcome,\n        predictor = predict(diabetes_LASSO_1se_AUC,\n                     newx = model_matrix_X_train)[,\"s0\"] )\nROC_lasso\n\n\n\nCall:\nroc.default(response = diabetes_train$Outcome, predictor = predict(diabetes_LASSO_1se_AUC,     newx = model_matrix_X_train)[, \"s0\"])\n\nData: predict(diabetes_LASSO_1se_AUC, newx = model_matrix_X_train)[, \"s0\"] in 180 controls (diabetes_train$Outcome 0) &lt; 95 cases (diabetes_train$Outcome 1).\nArea under the curve: 0.8496\n\n\nShow the code\nAUC_lasso &lt;- pROC::auc(ROC_lasso)\n\n\n\nCompare the AUC values for our 4 candidate models, full model, AIC-selected model, BIC-selected model and LASSO model.\n\n\n\nShow the code\nmodel_names &lt;- c(\"Full Model\", \"AIC-selected Model\", \"BIC-selected Model\", \"LASSO Model\")\nAUC_values &lt;- c(ROC_full_log$auc, ROC_AIC_log$auc, ROC_BIC_log$auc, as.double(AUC_lasso))\ncomparison_table &lt;- data.frame(Model = model_names, AUC = AUC_values)\n\nkable(comparison_table)\n\n\n\n\n\nModel\nAUC\n\n\n\n\nFull Model\n0.8774269\n\n\nAIC-selected Model\n0.8776023\n\n\nBIC-selected Model\n0.8598246\n\n\nLASSO Model\n0.8496491\n\n\n\n\n\n\n\nResults:\nAfter comparing the AUC values for the four candidate models, we observed that the AIC-selected model had the best prediction performance on the training dataset. Consequently, we determined to adopt the AIC-selected model as our final predictive model. Next, we will assess the out-of-sample performance of our final model by applying it to the testing dataset, thereby generating the ROC curve and computing the corresponding AUC value.\n\n\nShow the code\noptions(repr.plot.width = 15, repr.plot.height = 10)\n\nmodel_X_test &lt;- diabetes_test[, -which(names(diabetes_test) == \"Outcome\")]\n\npredicted_prob_AIC &lt;- predict(AIC_model, newdata = model_X_test, type = \"response\")\npredicted_fullmodel &lt;- predict(full_model, newdata = model_X_test, type = \"response\")\npredicted_prob_BIC &lt;- predict(BIC_model, newdata = model_X_test, type = \"response\")\npredicted_lasso &lt;- predict(diabetes_LASSO_1se_AUC, newx = as.matrix(model_X_test))[,\"s0\"]\n\nROC_AIC_model_in_testdata &lt;- roc(response = diabetes_test$Outcome, predictor = predicted_prob_AIC)\nplot(ROC_AIC_model_in_testdata,  print.auc = TRUE, print.auc.x = 0.5, print.auc.y = 0.5, col = \"steelblue\", lwd = 3, lty = 2, main = \"ROC Curve of Models using Test Dataset\", cex.main = 2)\n\nROC_full_model_in_testdata &lt;- roc(response = diabetes_test$Outcome, predictor = predicted_fullmodel)\nplot(ROC_full_model_in_testdata, print.auc = TRUE, print.auc.x = 0.5, print.auc.y = 0.45, col = \"black\", lwd = 3, lty = 2, add = TRUE)\n\nROC_BIC_model_in_testdata &lt;- roc(response = diabetes_test$Outcome, predictor = predicted_prob_BIC)\nplot(ROC_BIC_model_in_testdata, print.auc = TRUE, print.auc.x = 0.5, print.auc.y = 0.40, col = \"springgreen\", lwd = 3, lty = 2, add = TRUE)\n\nROC_lasso_in_testdata &lt;- roc(response = diabetes_test$Outcome, predictor = predicted_lasso)\nplot(ROC_lasso_in_testdata, print.auc = TRUE, print.auc.x = 0.5, print.auc.y = 0.35, col = \"red\", lwd = 3, lty = 2, add = TRUE)\n\n\nlegend(\"bottomright\", legend = model_names, col = c(\"black\", \"steelblue\", \"springgreen\", \"red\"), lty = 2, lwd = 3)\n\n\n\n\n\n\n\n\n\n\n\nResult summary and conclusion:\nBased on our analysis, the AIC-selected model has the best prediction performance among the four models we compared. Upon fitting the AIC-selected model to the testing dataset, we obtained an AUC value of 0.808, which underscores the robust predictive capability of our AIC-selected model when applied to out-of-sample data.\nTherefore, we have concluded that the AIC-selected model aligns most effectively with our project’s objective of establishing a predictive model for determining the probabilty of an individual having diabetes. Below is a summary of the selected model:\n\\[\n\\begin{align*}\n\\log\\left(\\frac{p_i}{1-p_i}\\right) &= -10.620170 +1.146492 \\cdot \\text{DiabetesPedigreeFunction} + 0.040539 \\cdot \\text{Age} \\\\\n&\\quad + 0.078362 \\cdot \\text{BMI} + 0.042436 \\cdot \\text{Glucose}\n\\end{align*}\n\\]\nwhere \\(p_i\\) is the probability of the \\(i{\\text{th}}\\) individual having diabetes.\nGiven an individual’s diabetes percentage, age, BMI, and glucose level, this model can be used to predictive the probability of diabetes as\n\\[p_{i} = \\frac{1}{1+e^{-(-10.620170 + 1.146492 \\times \\text{DiabetesPedigreeFunction} + 0.040539 \\times \\text{Age} + 0.078362 \\times \\text{BMI} + 0.042436 \\times \\text{Glucose})}}\\]"
  },
  {
    "objectID": "posts/diabetes/index.html#discussion",
    "href": "posts/diabetes/index.html#discussion",
    "title": "Diabetes Prediction Modeling",
    "section": "Discussion:",
    "text": "Discussion:\nThrough this project, we’ve developed a predictive model to estimate the probability of an individual having diabetes. In our final model, we’ve left with four key variables, making the model has the best prediction performance: DiabetesPedigreeFunction, Age, BMI, and Glucose. Notably, all coefficients associated with these variables are positive, indicating that higher values for these factors correlate with an increased prbability of diabetes. Following model comparison and selection processes, the AIC-selected model has been determined as the optimal choice, demonstrating best predictive performance both in-sample and out-of-sample prediction among our 4 candidate models with AUC value of approximately 0.8, indicating its robust predictive capabilities.\nThe outcome of our analysis was surprising. While AIC-based stepwise selection is commonly used to explore predictor-response relationships, we opted to assess the efficacy of a LASSO model, known for predictive power and overfitting avoidance. We expected the LASSO model to outperform the AIC-selected model in out-of-sample prediction accuracy. However, results showed the AIC-selected model not only offered good interpretability but also outperformed the LASSO model in terms of AUC values. This outcome is better than we expected as it signifies a balance between model interpretability and predictive powerness in our final model.\nWhile the AIC-selected model performed best on the testing dataset, its superiority on out-of-sample data is not guaranteed. Implementing k-fold cross-validation and calculating CV-AUC values for our four candidate models can enhance our methodology. This approach assesses models across various data partitions, reducing reliance on chance results from a single train-test split. CV-AUC values may not always align with initial model selection; for instance, the Lasso model might show the highest CV-AUC value, indicating superior prediction performance. Integrating k-fold cross-validation into our model evaluation enhances our methodology’s robustness, ensuring our final predictive model is well-suited for generalization to unseen data.\nA key area for future exploration is identifying additional predictors beyond those in our dataset that could influence diabetes risk. Factors like other medical histories and pharmaceutical supplements may provide valuable insights. Additionally, we should investigate how different parameters, such as lambda values, affect the LASSO model’s performance. In our analysis we use lambda.1se value, and explore the performance with the lambda.min value. In summary, exploring new predictors and optimizing regularization parameters can enhance our predictive models and improve our ability to predict and manage diabetes."
  },
  {
    "objectID": "posts/diabetes/index.html#references",
    "href": "posts/diabetes/index.html#references",
    "title": "Diabetes Prediction Modeling",
    "section": "References:",
    "text": "References:\n\nRelated Study 1: Joshi, Ram D, and Chandra K Dhakal. “Predicting Type 2 Diabetes Using Logistic Regression and Machine Learning Approaches.” International Journal of Environmental Research and Public Health, U.S. National Library of Medicine, 9 July 2021, www.ncbi.nlm.nih.gov/pmc/articles/PMC8306487/.\nRelated Study 2: Chang, Victor, et al. “Pima Indians Diabetes Mellitus Classification Based on Machine Learning (ML) Algorithms.” Neural Computing & Applications, U.S. National Library of Medicine, 24 Mar. 2022, www.ncbi.nlm.nih.gov/pmc/articles/PMC8943493/.\nData source: National Institute of Diabetes and Digestive and Kidney Diseases. “Predict Diabetes.” Kaggle, 9 Nov. 2022, www.kaggle.com/datasets/whenamancodes/predict-diabities?resource=download.\nMore Details on data: “Pima Indians Diabetes Database - Dataset by Data-Society.” Data.World, 13 Dec. 2016, www.data.world/data-society/pima-indians-diabetes-database.\nROC in health data: Nahm, Francis Sahngun. “Receiver Operating Characteristic Curve: Overview and Practical Use for Clinicians.” Korean Journal of Anesthesiology, U.S. National Library of Medicine, Feb. 2022, www.ncbi.nlm.nih.gov/pmc/articles/PMC8831439/."
  },
  {
    "objectID": "posts/flagmodel/index.html",
    "href": "posts/flagmodel/index.html",
    "title": "Modeling for flagging potential cancellations",
    "section": "",
    "text": "The behavior of customers and booking options have thoroughly altered by the online hotel reservation channels. A significant number of reservations are canceled always, mostly due to cancellations or no-shows. Cancellations can be caused by a variety of factors, such as scheduling conflicts, changes in plans, etc. This is observed more frequently due to the fact that hotels often make it easier for guests to cancel their reservations by offering free or low cost cancellation options. Although this benefits the guests heavily, it may risk the hotel losing on potential revenue.\nTherefore, the outcome of this project is to assist the hotel owners to better understand whether a customer will confirm the booking or cancel it by developing and implementing machine learning methodologies.\nSource : The dataset used in this project is from Kaggle. Link -&gt; Reservation Cancellation Prediction Dataset\n\n\n\n\nShow the code\nimport os\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import (\n    RandomizedSearchCV,\n    cross_validate,\n    train_test_split,\n)\nfrom sklearn.metrics import (\n    f1_score,\n)\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\npd.set_option(\"display.max_colwidth\", 0)\nfrom sklearn.feature_selection import RFECV\n\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)\n\nfrom IPython.display import display, HTML\nimport shap, eli5\n\n\n\n\n\nIn this exploratory data analysis (EDA) section, we load the training dataset from a CSV file and prepare it for analysis. We split the data into training and testing sets, with 70% allocated for training. Then, we provide a summary of the descriptive statistics for the training dataset to gain insights into its distribution and characteristics.\n\n\nShow the code\ndata = pd.read_csv(\"train__dataset.csv\")\nX = data.drop(\"booking_status\", axis=1)\ny = data[\"booking_status\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=111, train_size = 0.7) # setting a random state to make it reproducible\n\ndf_train = X_train.join(y_train)\ndf_train.describe()\n\n\n\n\n\n\n\n\n\n\nno_of_adults\nno_of_children\nno_of_weekend_nights\nno_of_week_nights\ntype_of_meal_plan\nrequired_car_parking_space\nroom_type_reserved\nlead_time\narrival_year\narrival_month\narrival_date\nmarket_segment_type\nrepeated_guest\nno_of_previous_cancellations\nno_of_previous_bookings_not_canceled\navg_price_per_room\nno_of_special_requests\nbooking_status\n\n\n\n\ncount\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n\n\nmean\n1.845845\n0.106892\n0.808980\n2.214100\n0.313037\n0.031902\n0.336510\n84.945884\n2017.821189\n7.441985\n15.628909\n0.806459\n0.026073\n0.022922\n0.159354\n103.513689\n0.616384\n0.324143\n\n\nstd\n0.517027\n0.410776\n0.871795\n1.418276\n0.623953\n0.175747\n0.774089\n85.788163\n0.383209\n3.081619\n8.779931\n0.645516\n0.159359\n0.379929\n1.785368\n35.090118\n0.790173\n0.468072\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n2017.000000\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n2.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n0.000000\n17.000000\n2018.000000\n5.000000\n8.000000\n0.000000\n0.000000\n0.000000\n0.000000\n80.750000\n0.000000\n0.000000\n\n\n50%\n2.000000\n0.000000\n1.000000\n2.000000\n0.000000\n0.000000\n0.000000\n57.000000\n2018.000000\n8.000000\n16.000000\n1.000000\n0.000000\n0.000000\n0.000000\n99.450000\n0.000000\n0.000000\n\n\n75%\n2.000000\n0.000000\n2.000000\n3.000000\n0.000000\n0.000000\n0.000000\n125.000000\n2018.000000\n10.000000\n23.000000\n1.000000\n0.000000\n0.000000\n0.000000\n120.000000\n1.000000\n1.000000\n\n\nmax\n4.000000\n9.000000\n6.000000\n17.000000\n3.000000\n1.000000\n6.000000\n443.000000\n2018.000000\n12.000000\n31.000000\n4.000000\n1.000000\n13.000000\n58.000000\n375.500000\n5.000000\n1.000000\n\n\n\n\n\n\n\n\nBased on the summary statistics provided, here are some initial observations about the data: * There are 12,695 observations in the train dataset. * The average number of adults per booking is 1.85 while the average number of children per booking is 0.11  * On average people visit between July and August, meaning the hotel is busier during summer holidays. * The average number of nights stayed over a weekend is 0.81 while the average number of nights stayed during the week is 2.21  * The average lead time for bookings is 84.95 days, with a standard deviation of 85.79.  * Only 2.6% of bookings are from repeated guests on average  * The average price per room is 103.51  * The majority of bookings (67.6%) are confirmed, while the remaining 32.4% are canceled."
  },
  {
    "objectID": "posts/flagmodel/index.html#background",
    "href": "posts/flagmodel/index.html#background",
    "title": "Modeling for flagging potential cancellations",
    "section": "",
    "text": "The behavior of customers and booking options have thoroughly altered by the online hotel reservation channels. A significant number of reservations are canceled always, mostly due to cancellations or no-shows. Cancellations can be caused by a variety of factors, such as scheduling conflicts, changes in plans, etc. This is observed more frequently due to the fact that hotels often make it easier for guests to cancel their reservations by offering free or low cost cancellation options. Although this benefits the guests heavily, it may risk the hotel losing on potential revenue.\nTherefore, the outcome of this project is to assist the hotel owners to better understand whether a customer will confirm the booking or cancel it by developing and implementing machine learning methodologies.\nSource : The dataset used in this project is from Kaggle. Link -&gt; Reservation Cancellation Prediction Dataset\n\n\n\n\nShow the code\nimport os\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import (\n    RandomizedSearchCV,\n    cross_validate,\n    train_test_split,\n)\nfrom sklearn.metrics import (\n    f1_score,\n)\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\npd.set_option(\"display.max_colwidth\", 0)\nfrom sklearn.feature_selection import RFECV\n\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)\n\nfrom IPython.display import display, HTML\nimport shap, eli5\n\n\n\n\n\nIn this exploratory data analysis (EDA) section, we load the training dataset from a CSV file and prepare it for analysis. We split the data into training and testing sets, with 70% allocated for training. Then, we provide a summary of the descriptive statistics for the training dataset to gain insights into its distribution and characteristics.\n\n\nShow the code\ndata = pd.read_csv(\"train__dataset.csv\")\nX = data.drop(\"booking_status\", axis=1)\ny = data[\"booking_status\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=111, train_size = 0.7) # setting a random state to make it reproducible\n\ndf_train = X_train.join(y_train)\ndf_train.describe()\n\n\n\n\n\n\n\n\n\n\nno_of_adults\nno_of_children\nno_of_weekend_nights\nno_of_week_nights\ntype_of_meal_plan\nrequired_car_parking_space\nroom_type_reserved\nlead_time\narrival_year\narrival_month\narrival_date\nmarket_segment_type\nrepeated_guest\nno_of_previous_cancellations\nno_of_previous_bookings_not_canceled\navg_price_per_room\nno_of_special_requests\nbooking_status\n\n\n\n\ncount\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n\n\nmean\n1.845845\n0.106892\n0.808980\n2.214100\n0.313037\n0.031902\n0.336510\n84.945884\n2017.821189\n7.441985\n15.628909\n0.806459\n0.026073\n0.022922\n0.159354\n103.513689\n0.616384\n0.324143\n\n\nstd\n0.517027\n0.410776\n0.871795\n1.418276\n0.623953\n0.175747\n0.774089\n85.788163\n0.383209\n3.081619\n8.779931\n0.645516\n0.159359\n0.379929\n1.785368\n35.090118\n0.790173\n0.468072\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n2017.000000\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n2.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n0.000000\n17.000000\n2018.000000\n5.000000\n8.000000\n0.000000\n0.000000\n0.000000\n0.000000\n80.750000\n0.000000\n0.000000\n\n\n50%\n2.000000\n0.000000\n1.000000\n2.000000\n0.000000\n0.000000\n0.000000\n57.000000\n2018.000000\n8.000000\n16.000000\n1.000000\n0.000000\n0.000000\n0.000000\n99.450000\n0.000000\n0.000000\n\n\n75%\n2.000000\n0.000000\n2.000000\n3.000000\n0.000000\n0.000000\n0.000000\n125.000000\n2018.000000\n10.000000\n23.000000\n1.000000\n0.000000\n0.000000\n0.000000\n120.000000\n1.000000\n1.000000\n\n\nmax\n4.000000\n9.000000\n6.000000\n17.000000\n3.000000\n1.000000\n6.000000\n443.000000\n2018.000000\n12.000000\n31.000000\n4.000000\n1.000000\n13.000000\n58.000000\n375.500000\n5.000000\n1.000000\n\n\n\n\n\n\n\n\nBased on the summary statistics provided, here are some initial observations about the data: * There are 12,695 observations in the train dataset. * The average number of adults per booking is 1.85 while the average number of children per booking is 0.11  * On average people visit between July and August, meaning the hotel is busier during summer holidays. * The average number of nights stayed over a weekend is 0.81 while the average number of nights stayed during the week is 2.21  * The average lead time for bookings is 84.95 days, with a standard deviation of 85.79.  * Only 2.6% of bookings are from repeated guests on average  * The average price per room is 103.51  * The majority of bookings (67.6%) are confirmed, while the remaining 32.4% are canceled."
  },
  {
    "objectID": "posts/flagmodel/index.html#data-visualization",
    "href": "posts/flagmodel/index.html#data-visualization",
    "title": "Modeling for flagging potential cancellations",
    "section": "Data visualization:",
    "text": "Data visualization:\nTo identify patterns or trends in the data relative to target class, we can create visualisations of some features for each target classes to see the differences between them.\n\nOutlier Analysis of Numerical Features:\n\n\nShow the code\n# Define the features\nsns.set_theme(style=\"darkgrid\")\nfeatures = ['lead_time', 'avg_price_per_room']\nfig, axes = plt.subplots(nrows = 1, ncols=len(features), figsize=(15, 6))\nfig.suptitle(\"Boxen Plots of Features\", fontsize=16, fontweight='bold')\n\n# Iterate over features\nfor i, feature in enumerate(features):\n    sns.boxenplot(data=df_train, y = feature, x = 'booking_status', palette=\"husl\", ax=axes[i])\n    axes[i].set_title(\"Boxen Plot of \" + feature + \" by booking status\")\n\n\n\n\n\n\n\n\n\nThe provided boxen plots offer insights into booking behavior based on lead time and average room price. For lead time, bookings not cancelled tend to have shorter lead times, possibly indicating last-minute reservations, while cancelled bookings show longer and more varied lead times, suggesting advanced bookings with diverse lead times. For average room price, non-cancelled bookings exhibit similar median prices as cancelled bookings. This insight challenges the initial observation and prompts the need for further analysis to better understand guest booking preferences and cancellation behaviors.\n\n\nDistribution Analysis of Numerical Features:\n\n\nShow the code\n# Define the features\nfeatures = ['lead_time', 'avg_price_per_room']\nfig, axes = plt.subplots(nrows = 1, ncols=len(features), figsize=(10, 5))\nfig.suptitle(\"Density Plots of Features\", fontsize=16, fontweight='bold')\n\n# Iterate over features\nfor i, feature in enumerate(features):\n    sns.kdeplot(data=df_train, x=feature, hue=\"booking_status\", bw_adjust=2, fill=True, palette=\"husl\", ax=axes[i],\n                legend=False)\n    axes[i].set_title(\"KDE Plot of \" + feature + \" by booking status\")\nfig.legend(title = \"Booking Status\", labels=[\"Cancelled\", \"Not Cancelled\"], bbox_to_anchor=(1.1, 0.5), fontsize = \"large\", title_fontsize='large')\n\n\n\n\n\n\n\n\n\nThe histograms reveal distinct patterns in booking behavior and cancellation likelihood based on lead time and average room price. For lead time, bookings are predominantly confirmed when the interval between booking and arrival is short, with cancellations becoming more prevalent as lead time increases beyond 100 days. Conversely, guests are more likely to confirm bookings when the lead time is under 100 days. Regarding average room price, confirmed bookings consistently outnumber cancellations across all price ranges. However, the highest confirmation rates occur when the average room price falls between 50 and 125. Beyond this range, cancellation probabilities increase, approaching parity with confirmation probabilities. In summary, shorter lead times and moderate room prices between 50 and 125 are associated with higher confirmation rates, while longer lead times are associated with increased cancellation probabilities. These insights can inform hotel management strategies to optimize booking processes and minimize cancellation rates.\n\n\nDistribution Analysis of Categorical Features:\n\n\nShow the code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Define the categorical features\ncat_features = ['no_of_adults', 'no_of_children', 'no_of_weekend_nights', 'no_of_week_nights', \n                'type_of_meal_plan', 'required_car_parking_space', 'room_type_reserved', \n                'market_segment_type', 'repeated_guest', 'no_of_previous_cancellations', \n                'no_of_previous_bookings_not_canceled', 'no_of_special_requests']\n\n# Create subplots with 3 rows and 4 columns\nfig, axes = plt.subplots(nrows=3, ncols=4, figsize=(15, 10))\nfig.suptitle(\"Count Plots of Categorical Features\", fontsize=20, fontweight='bold')\n\n# Iterate over features\nfor i, feature in enumerate(cat_features):\n    # Filter the DataFrame to include only the top N most frequent categories\n    top_categories = df_train[feature].value_counts().index[:10]\n    filtered_df = df_train[df_train[feature].isin(top_categories)]\n    \n    row = i // 4  # Calculate the row index\n    col = i % 4   # Calculate the column index\n    sns.countplot(data=filtered_df, x=feature, hue=\"booking_status\", palette=\"mako\", ax=axes[row, col])\n    axes[row, col].set_title(feature)\n    axes[row, col].legend().set_visible(False)\n\nfig.legend(title = \"Booking Status\", labels=[\"Not cancelled\", \"Cancelled\"], bbox_to_anchor=(1.1, 0.5), fontsize = \"large\", title_fontsize='large')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nBased on the bar plots:\n\nNumber of Adults and Children: Most bookings are made for one or two adults, and very few include children. This suggests that the hotel is primarily used by adults, possibly for business or couples on vacation.\nNumber of Weekend Nights and Week Nights: Guests typically stay from one to four nights during both weekends and weekdays. This could indicate that the hotel is popular for short stays.\nType of Meal Plan: A specific type of meal plan is predominantly chosen by guests. This could be due to the convenience or cost-effectiveness of this plan.\nRequired Car Parking Space: A large number of guests do not require car parking spaces, which might suggest that many guests use public transportation or other means of travel.\nRoom Type Reserved: Various room types are reserved with one being more common. This could indicate a preference for a particular room type, possibly due to cost, size, or amenities.\nMarket Segment Type: There’s a notable difference in market segment types between cancelled and not cancelled bookings. This could suggest that certain market segments are more likely to cancel their bookings.\nRepeated Guest: Most guests are not repeated ones, indicating that the hotel has a diverse guest population.\nNumber of Previous Cancellations and Bookings Not Cancelled: A majority have zero previous cancellations and many have no prior non-cancelled reservations. This could suggest that most guests are first-time visitors.\nNumber of Special Requests: Special requests during stays are relatively uncommon, which might indicate that most guests’ needs are met by the standard amenities provided by the hotel.\n\nThese observations provide valuable insights into the hotel’s guest demographics and their preferences, which could be useful for making strategic decisions to improve guest satisfaction and business performance. However, these are just observations based on the given plots, and further analysis would be needed to draw more concrete conclusions."
  },
  {
    "objectID": "posts/flagmodel/index.html#modeling",
    "href": "posts/flagmodel/index.html#modeling",
    "title": "Modeling for flagging potential cancellations",
    "section": "Modeling:",
    "text": "Modeling:\n\nPreprocessing and transformations:\n\n\nShow the code\nnumeric_features = [\"no_of_adults\", \"no_of_children\",\"no_of_weekend_nights\",\"no_of_week_nights\", \"lead_time\", \"no_of_previous_cancellations\",\"no_of_previous_bookings_not_canceled\",\"avg_price_per_room\",\"no_of_special_requests\", \"arrival_month\"] # apply scaling\ncategorical_features = [\"type_of_meal_plan\", \"room_type_reserved\", \"market_segment_type\"] # apply one-hot encoding\nbinary_features = [\"required_car_parking_space\", \"repeated_guest\"] # apply one-hot encoding with drop=\"if_binary\"\ndrop_features = [\"arrival_year\", \"arrival_date\"] # customers make bookings depending on mostly months  \ntarget = \"booking_status\"\n\nnumeric_transformer = StandardScaler()\ncategorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\nbinary_transformer = OneHotEncoder(drop=\"if_binary\", dtype=int)\n\npreprocessor = make_column_transformer(\n    (numeric_transformer, numeric_features),\n    (categorical_transformer, categorical_features),\n    (binary_transformer, binary_features),\n    (\"drop\", drop_features),\n)\n\n\nWe separate features into numerical and categorical types because they require different types of transformations. We apply one-hot encoding on the categorical features and standard scaling on the numerical features. In this project, we will approach different methods and employ different models. One of such models is a logistic regression model.\nBelow is a helper function to make calculating validation scores easier:\n\n\nShow the code\ndef mean_std_cross_val_scores(model, X_train, y_train, scoring, **kwargs):\n    \"\"\"\n    Returns mean and std of cross validation\n\n    Parameters\n    ----------\n    model :\n        scikit-learn model\n    X_train : numpy array or pandas DataFrame\n        X in the training data\n    y_train :\n        y in the training data\n\n    Returns\n    ----------\n        pandas Series with mean scores from cross_validation\n    \"\"\"\n    if scoring == \"f1\":\n        scores = cross_validate(model, X_train, y_train, scoring = \"f1\", **kwargs)\n        mean_scores = pd.DataFrame(scores).mean()\n        std_scores = pd.DataFrame(scores).std()\n        out_col = []\n\n        for i in range(len(mean_scores)):\n            out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n\n        return pd.Series(data=out_col, index=mean_scores.index)\n    else:\n        scores = cross_validate(model, X_train, y_train, **kwargs)\n\n        mean_scores = pd.DataFrame(scores).mean()\n        std_scores = pd.DataFrame(scores).std()\n        out_col = []\n\n    for i in range(len(mean_scores)):\n        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n\n    return pd.Series(data=out_col, index=mean_scores.index)\n\n\n\n\nLinear models:\n\n\nShow the code\nresults_dict = {}\nC_vals = 10.0 ** np.arange(-2, 2, 0.5)\n\nfor C in C_vals:\n    lr_pipe = make_pipeline(preprocessor, LogisticRegression(max_iter=1000, C=C))\n    results_dict[\"Logistic Regression with C=\" + str(C)] = mean_std_cross_val_scores(\n        lr_pipe, X_train, y_train, return_train_score=True, scoring =\"accuracy\"\n    )\n    \npd.DataFrame(results_dict)\n\n\n\n\n\n\n\n\n\n\nLogistic Regression with C=0.01\nLogistic Regression with C=0.03162277660168379\nLogistic Regression with C=0.1\nLogistic Regression with C=0.31622776601683794\nLogistic Regression with C=1.0\nLogistic Regression with C=3.1622776601683795\nLogistic Regression with C=10.0\nLogistic Regression with C=31.622776601683793\n\n\n\n\nfit_time\n0.033 (+/- 0.005)\n0.035 (+/- 0.002)\n0.048 (+/- 0.002)\n0.066 (+/- 0.003)\n0.095 (+/- 0.007)\n0.132 (+/- 0.007)\n0.164 (+/- 0.007)\n0.161 (+/- 0.023)\n\n\nscore_time\n0.007 (+/- 0.002)\n0.006 (+/- 0.001)\n0.006 (+/- 0.000)\n0.006 (+/- 0.000)\n0.006 (+/- 0.001)\n0.006 (+/- 0.000)\n0.005 (+/- 0.001)\n0.006 (+/- 0.000)\n\n\ntest_score\n0.798 (+/- 0.009)\n0.800 (+/- 0.007)\n0.802 (+/- 0.006)\n0.802 (+/- 0.006)\n0.803 (+/- 0.006)\n0.803 (+/- 0.006)\n0.803 (+/- 0.006)\n0.803 (+/- 0.006)\n\n\ntrain_score\n0.798 (+/- 0.003)\n0.800 (+/- 0.003)\n0.803 (+/- 0.003)\n0.803 (+/- 0.002)\n0.804 (+/- 0.002)\n0.804 (+/- 0.002)\n0.804 (+/- 0.002)\n0.804 (+/- 0.002)\n\n\n\n\n\n\n\n\nFrom the above observations, we can conclude that changing C values after a specific threshold does not affect the training scores. Thus, we should try optimizing a different hyperparameter or try using another model and optimize it to get a better model.\nTherefore, we try using another model with a different estimator. Since linear models seemed to be at its limit, we can improve our results by exploring the use of non-linear estimators and evaluating their effectiveness.\n\n\nNon-linear models:\n\n\nShow the code\nresults_dict = {}\nmodels = {\n    \"Decision Trees\": DecisionTreeClassifier(random_state=111),\n    \"kNN\": KNeighborsClassifier(),\n    \"Random Forest\": RandomForestClassifier(n_jobs=-1, n_estimators=100, random_state = 111)\n}\n\nfor x in models:\n    for y in [\"accuracy\", \"f1\"]:\n        pipe = make_pipeline(preprocessor, models[x])\n        results_dict[x + \" \" + str(y)] = mean_std_cross_val_scores(\n            pipe, X_train, y_train, return_train_score=True, scoring = y)\n        \npd.DataFrame(results_dict)\n\n\n\n\n\n\n\n\n\n\nDecision Trees accuracy\nDecision Trees f1\nkNN accuracy\nkNN f1\nRandom Forest accuracy\nRandom Forest f1\n\n\n\n\nfit_time\n0.046 (+/- 0.005)\n0.043 (+/- 0.001)\n0.015 (+/- 0.002)\n0.015 (+/- 0.000)\n1.133 (+/- 1.820)\n0.314 (+/- 0.008)\n\n\nscore_time\n0.006 (+/- 0.001)\n0.007 (+/- 0.000)\n0.163 (+/- 0.001)\n0.163 (+/- 0.001)\n0.035 (+/- 0.001)\n0.035 (+/- 0.002)\n\n\ntest_score\n0.845 (+/- 0.005)\n0.763 (+/- 0.007)\n0.832 (+/- 0.006)\n0.732 (+/- 0.008)\n0.883 (+/- 0.006)\n0.810 (+/- 0.010)\n\n\ntrain_score\n0.996 (+/- 0.000)\n0.994 (+/- 0.001)\n0.882 (+/- 0.002)\n0.812 (+/- 0.004)\n0.996 (+/- 0.000)\n0.994 (+/- 0.001)\n\n\n\n\n\n\n\n\nBased on the mean test scores, Random Forest seems to be performing the best, with an mean test score around 0.883. This is followed by Decision Trees with a score 0.845, and kNN with a score 0.832. Random Forest also have the best f1 score meaning that it can correctly identify both positive and negative examples with the highest accuracy among the rest of the models.\nGiven that the Random Forest model is the most effective among other models, we will focus on optimizing its hyperparameters to improve its performance.\n\n\nOptimization:\nWe will look to optimize the hyperparameters max_depth, n_estimators and class_weight. We will use random search, an automated technique to find the best set of hyperparameters.\n\n\nShow the code\nrf = RandomForestClassifier(n_jobs=-1, random_state=111)\nac_search_results = []\nbest_accuracy = 0\nbest_accuracy_params = []\n\nf1_search_results =[]\nbest_f1 = 0\nbest_f1_params = []\n\nparam_grid = {\"randomforestclassifier__max_depth\": [10, 20, 30, 40, 50],\n             \"randomforestclassifier__n_estimators\": [50, 100, 200, 250, 300],\n             \"randomforestclassifier__class_weight\": [None, {0:1, 1:3},'balanced']}\n\npipe = make_pipeline(preprocessor, rf)\nrandom_search = RandomizedSearchCV(\n    pipe, param_grid, cv=5, return_train_score=True, n_iter=15, random_state=111\n)\nrandom_search.fit(X_train, y_train)\nfor x in [\"accuracy\", \"f1\"]:\n    pipe = make_pipeline(preprocessor, rf)\n    random_search = RandomizedSearchCV(\n        pipe, param_grid, cv=5, return_train_score=True, n_iter=15, random_state=111, scoring = x\n    )\n    random_search.fit(X_train, y_train)\n    if x == \"accuracy\":\n        ac_search_results = pd.DataFrame(random_search.cv_results_)\n        best_accuracy = random_search.best_score_\n        best_accuracy_params = random_search.best_params_\n    else:\n        f1_search_results = pd.DataFrame(random_search.cv_results_)\n        best_f1 = random_search.best_score_\n        best_f1_params = random_search.best_params_\n\n\n\n\nShow the code\nprint(best_accuracy_params, best_accuracy)\nprint(best_f1_params, best_f1)\n\n\n{'randomforestclassifier__n_estimators': 250, 'randomforestclassifier__max_depth': 50, 'randomforestclassifier__class_weight': 'balanced'} 0.8844426939740055\n{'randomforestclassifier__n_estimators': 200, 'randomforestclassifier__max_depth': 20, 'randomforestclassifier__class_weight': {0: 1, 1: 3}} 0.8158630987451104\n\n\nAt first glance, the accuracy is more improved than the what we got intially. Also, the f1 increased slightly. The best f1 params and accuracy params differ by just the class weight. Looking at the f1_search_results and ac_search_ results, the difference between the accuracies and f1 scores between these two params is trivial for both optimum parameters. Therefore, we pick the best f1 params as the accuracy score (for best f1 params) is slightly lower than its best score. Also, we can afford to lose a very small accuracy score for a better f1 score. Moreover, using lower n_estimators will make fitting faster.\nTherefore, the final optimized model: RandomForestClassifier(n_estimators = 200, max_depth=20, class_weight = {0: 1, 1: 3})\n\n\nFeature importances and selection:\nAn important part of this project involved creating and transforming the features used in our model, given a rather large amount of raw data. First, we need to find which features are important to the model. Then, we will use feature selection to select specific features to make our model simpler and better.\n\nFeature importances:\nWe will use eli5 to view which features are important.\n\n\nShow the code\nX_train_pp = preprocessor.fit_transform(X_train)\n\ncolumn_names = numeric_features + list(\n    preprocessor.named_transformers_[\"onehotencoder-1\"].get_feature_names_out(categorical_features)) + list(\n    preprocessor.named_transformers_[\"onehotencoder-2\"].get_feature_names_out(binary_features))\n\npipe_rf = make_pipeline(preprocessor, RandomForestClassifier(n_estimators = 200, max_depth=20, n_jobs=-1, random_state=111, class_weight = {0: 1, 1: 3}))\npipe_rf.fit(X_train, y_train)\nexp_df = eli5.explain_weights_df(pipe_rf.named_steps[\"randomforestclassifier\"], feature_names=column_names)\n\nvscrollbar = {\n 'selector': '',\n 'props': ' height: 240px; overflow-y: scroll;display: inline-block;'\n}\n\nexp_df.style.set_table_styles([vscrollbar])\n\n\n\n\n\n\n\n\n \nfeature\nweight\nstd\n\n\n\n\n0\nlead_time\n0.330100\n0.022381\n\n\n1\navg_price_per_room\n0.163066\n0.018598\n\n\n2\nno_of_special_requests\n0.130147\n0.023908\n\n\n3\narrival_month\n0.103653\n0.011179\n\n\n4\nno_of_week_nights\n0.058911\n0.007677\n\n\n5\nno_of_weekend_nights\n0.043598\n0.007356\n\n\n6\nmarket_segment_type_1\n0.036235\n0.024143\n\n\n7\nno_of_adults\n0.024855\n0.006412\n\n\n8\nmarket_segment_type_0\n0.022669\n0.017395\n\n\n9\nrequired_car_parking_space_1\n0.012265\n0.003019\n\n\n10\ntype_of_meal_plan_0\n0.008535\n0.003026\n\n\n11\nno_of_children\n0.008337\n0.002368\n\n\n12\ntype_of_meal_plan_2\n0.008323\n0.006497\n\n\n13\nroom_type_reserved_0\n0.008220\n0.002739\n\n\n14\nmarket_segment_type_2\n0.007616\n0.007625\n\n\n15\nroom_type_reserved_1\n0.007319\n0.002676\n\n\n16\ntype_of_meal_plan_1\n0.007046\n0.003891\n\n\n17\nno_of_previous_bookings_not_canceled\n0.005011\n0.006992\n\n\n18\nrepeated_guest_1\n0.004493\n0.005855\n\n\n19\nroom_type_reserved_2\n0.002372\n0.001090\n\n\n20\nroom_type_reserved_3\n0.001942\n0.001076\n\n\n21\nroom_type_reserved_4\n0.001838\n0.001006\n\n\n22\nmarket_segment_type_4\n0.001729\n0.002832\n\n\n23\nno_of_previous_cancellations\n0.000645\n0.001057\n\n\n24\nroom_type_reserved_5\n0.000638\n0.000509\n\n\n25\nmarket_segment_type_3\n0.000417\n0.000554\n\n\n26\nroom_type_reserved_6\n0.000018\n0.000067\n\n\n27\ntype_of_meal_plan_3\n0.000001\n0.000006\n\n\n\n\n\n\nFrom the table: * lead_time, avg_price_per_room, no_of_special_requests and arrival month have the highest weight among all the features(&gt;0.1). * no_of_week_nights, no_of_weekend_nights , market_segment_type_1, and no_of_adults have moderate to low weight. * The rest of the features have very low to no weight. * lead_time is the most important feature, followed by avg_price_per_room and no_of_special_requests.\n\n\nShow the code\npp_df = pd.DataFrame(X_train_pp, columns=column_names)\nrf_explainer = shap.TreeExplainer(pipe_rf.named_steps[\"randomforestclassifier\"])\ntrain_rf_shap_values = rf_explainer.shap_values(pp_df.sample(n=1000, random_state = 111)) # choosing only 1000 samples c\n\n\n\n\nShow the code\nshap.summary_plot(train_rf_shap_values[1], pp_df.sample(n=1000, random_state = 111))\nshap.dependence_plot(\"lead_time\", train_rf_shap_values[1], pp_df.sample(n=1000, random_state = 111))\nshap.dependence_plot(\"avg_price_per_room\", train_rf_shap_values[1], pp_df.sample(n=1000, random_state = 111))\n\n\nNo data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights summary:\n\nFirst plot: The analysis reveals a strong correlation between lead_time values and the corresponding shap values. High lead_time values tend to have high shap values, while low lead_time values are associated with low shap values. This indicates that high lead_times are more likely to predict the outcome 1, whereas lower lead_times are more likely to predict 0. Similarly, for the no_of_special_requests feature, it is observed that high values tend to have low shap values, indicating that high no_of_special_requests are more likely to predict 0, while lower values are more likely to predict 1. Additionally, the analysis shows a similar pattern for the avg_price_per_room feature. High avg_price_per_room values have high shap values, indicating a higher likelihood of predicting 1, whereas low avg_price_per_room values have low shap values, suggesting a higher likelihood of predicting 0. Furthermore, the required_car_parking_space_1 feature has low shap values for high values, meaning that high required_car_parking_space_1 values are more likely to predict 1, while lower values are more likely to predict 0.\nSecond plot: The majority of data points in the graph correspond to market_segment_type_1. Additionally, there is a positive relationship between the lead_time values and their respective shap values, suggesting that as lead_time increases, so does its impact on the prediction. Non-market_segment_type_1 data points are mostly concentrated near the bottom of the graph, with a few outliers at the top.\nThird plot: The graph indicates a mixed proportion of high and low lead_time values among the data points. Moreover, there is a positive correlation between the avg_price_per_room values and their corresponding shap values, indicating that higher avg_price_per_room values have a greater impact on the prediction.\n\n\n\nFeature selection:\nWe will use RFECV to select the number of features to use in our model. It uses cross-validation to select number of features.\n\n\nShow the code\nmodel = RandomForestClassifier(n_estimators = 200, max_depth=20, class_weight = {0: 1, 1: 3})\nrfe_cv = RFECV(model, cv=10,n_jobs=-1)\nrfe_cv.fit(X_train_pp, y_train)\n\nprint('Original number of features:', X_train_pp.shape[1])\nprint('Number of selected features:', sum(rfe_cv.support_), '\\n\\n')\nprint(\"Selected features: \", np.array(column_names)[rfe_cv.support_])\n\n\nOriginal number of features: 28\nNumber of selected features: 18 \n\n\nSelected features:  ['no_of_adults' 'no_of_children' 'no_of_weekend_nights'\n 'no_of_week_nights' 'lead_time' 'avg_price_per_room'\n 'no_of_special_requests' 'arrival_month' 'type_of_meal_plan_0'\n 'type_of_meal_plan_1' 'type_of_meal_plan_2' 'room_type_reserved_0'\n 'room_type_reserved_1' 'market_segment_type_0' 'market_segment_type_1'\n 'market_segment_type_2' 'required_car_parking_space_1' 'repeated_guest_1']\n\n\nAs seen from the results, we have managed to reduce the number of features considerably (by half). Next, we will use this to check whether our model has improved or how much it improved.\n\n\nShow the code\nnew_train = rfe_cv.transform(X_train_pp)\n\nprint('accuracy scores:\\n \\n', mean_std_cross_val_scores(\n    model, new_train, y_train, return_train_score=True, scoring =\"accuracy\"\n),'\\n \\n','f1 scores:\\n \\n', mean_std_cross_val_scores(\n    model, new_train, y_train, return_train_score=True, scoring =\"f1\"\n))\n\n\naccuracy scores:\n \n fit_time       1.767 (+/- 0.027)\nscore_time     0.084 (+/- 0.001)\ntest_score     0.884 (+/- 0.006)\ntrain_score    0.986 (+/- 0.002)\ndtype: object \n \n f1 scores:\n \n fit_time       1.784 (+/- 0.066)\nscore_time     0.084 (+/- 0.001)\ntest_score     0.816 (+/- 0.005)\ntrain_score    0.979 (+/- 0.001)\ndtype: object\n\n\nThe accuracy scores and f1 scores increased very slightly but it is still a major improvement since our model is much simpler and considers lower parameters.\nWe have a proper model and have selected important features to train our model. Now, we will check how the model works on unseen datasets."
  },
  {
    "objectID": "posts/flagmodel/index.html#results",
    "href": "posts/flagmodel/index.html#results",
    "title": "Modeling for flagging potential cancellations",
    "section": "Results:",
    "text": "Results:\n\n\nShow the code\nmodel.fit(new_train,y_train)\nnew_test = preprocessor.transform(X_test)\nnew_test = rfe_cv.transform(new_test)\n\naccuracy_testscore = model.score(new_test, y_test)\nf1_testscore = f1_score(y_test, model.predict(new_test))\nprint(\"Accuracy: \" + str(accuracy_testscore))\nprint(\"F1 score: \" + str(f1_testscore))\n\n\nAccuracy: 0.8869900771775082\nF1 score: 0.827972027972028\n\n\nFrom the train and test scores above: * It has good test scores meaning it effectively classifies the data into their respective classes. * The test scores agree with the validation scores from before. * The test values are overfit. The predictions have high variance among them but low bias. * The difference in train/test value means the model might not generalize well to new unseen data.\n\nModel caveats:\nThroughout the project, the models created and methods used might not be best and the following results might not be perfect. Having said that, the following caveats can be inferred:\n\nGeneralizability: The significant difference between train accuracy/F1 score and test F1 accuracy/F1 score means that the model could be overfitting the training data. As a result, it might not generalize well to unseen data. This is mainly due to the fact we are using an ensemble model and other steps we took to optimize it such as RFECV, random search.\nSuboptimal search results: Using grid search instead of random search could have provided better performing parameters since it is an exhaustive search. The hyperparameters found might be suboptimal compared to what we could have found using grid search.\nTime series relationships: The relationship between the features and booking_status can change across the years/different dates. Since we did not consider time series analysis, we might have missed underlying patterns in the data, resulting in suboptimal model performance.\nComplexity: Since we used an ensemble model, the model is complex and requires high computational power. Therefore, it is harder to understand or interpret the model, and its training time is also higher than usual.\nNon-important features: RFECV might not have identified all the redundant features, which can increase model complexity and negatively impact the its performance.\n\nOverall, the model still works as an useful tool to assist the hotel owners in better understanding whether a customer will honor the reservation or cancel it. Although the model is far from perfect, it can always be improved or further optimized. Training the model with new data sets occasionally will also improve the models performance in making informed decisions about managing hotel reservations."
  },
  {
    "objectID": "hc.html",
    "href": "hc.html",
    "title": "Predictive model to flag potential cancellations",
    "section": "",
    "text": "The behavior of customers and booking options have thoroughly altered by the online hotel reservation channels. A significant number of reservations are canceled always, mostly due to cancellations or no-shows. Cancellations can be caused by a variety of factors, such as scheduling conflicts, changes in plans, etc. This is observed more frequently due to the fact that hotels often make it easier for guests to cancel their reservations by offering free or low cost cancellation options. Although this benefits the guests heavily, it may risk the hotel losing on potential revenue.\nTherefore, the outcome of this project is to assist the hotel owners to better understand whether a customer will confirm the booking or cancel it by developing and implementing machine learning methodologies.\nSource : The dataset used in this project is from Kaggle. Link -&gt; Reservation Cancellation Prediction Dataset\n\n\n\nimport os\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.cluster import DBSCAN, KMeans\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import (\n    GridSearchCV,\n    RandomizedSearchCV,\n    cross_validate,\n    train_test_split,\n)\n\nfrom sklearn.metrics import (\n    classification_report,\n    confusion_matrix,\n    f1_score,\n    recall_score,\n    precision_score,\n    make_scorer,\n    \n)\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\npd.set_option(\"display.max_colwidth\", 0)\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.feature_selection import SequentialFeatureSelector\n\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)\n\nimport shap, eli5\n\n\n\n\nIn this exploratory data analysis (EDA) section, we load the training dataset from a CSV file and prepare it for analysis. We split the data into training and testing sets, with 70% allocated for training. Then, we provide a summary of the descriptive statistics for the training dataset to gain insights into its distribution and characteristics.\n\ndata = pd.read_csv(\"train__dataset.csv\")\nX = data.drop(\"booking_status\", axis=1)\ny = data[\"booking_status\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=111, train_size = 0.7) # setting a random state to make it reproducible\n\ndf_train = X_train.join(y_train)\ndf_train.describe()\n\n\n\n\n\n\n\n\n\nno_of_adults\nno_of_children\nno_of_weekend_nights\nno_of_week_nights\ntype_of_meal_plan\nrequired_car_parking_space\nroom_type_reserved\nlead_time\narrival_year\narrival_month\narrival_date\nmarket_segment_type\nrepeated_guest\nno_of_previous_cancellations\nno_of_previous_bookings_not_canceled\navg_price_per_room\nno_of_special_requests\nbooking_status\n\n\n\n\ncount\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n\n\nmean\n1.845845\n0.106892\n0.808980\n2.214100\n0.313037\n0.031902\n0.336510\n84.945884\n2017.821189\n7.441985\n15.628909\n0.806459\n0.026073\n0.022922\n0.159354\n103.513689\n0.616384\n0.324143\n\n\nstd\n0.517027\n0.410776\n0.871795\n1.418276\n0.623953\n0.175747\n0.774089\n85.788163\n0.383209\n3.081619\n8.779931\n0.645516\n0.159359\n0.379929\n1.785368\n35.090118\n0.790173\n0.468072\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n2017.000000\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n2.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n0.000000\n17.000000\n2018.000000\n5.000000\n8.000000\n0.000000\n0.000000\n0.000000\n0.000000\n80.750000\n0.000000\n0.000000\n\n\n50%\n2.000000\n0.000000\n1.000000\n2.000000\n0.000000\n0.000000\n0.000000\n57.000000\n2018.000000\n8.000000\n16.000000\n1.000000\n0.000000\n0.000000\n0.000000\n99.450000\n0.000000\n0.000000\n\n\n75%\n2.000000\n0.000000\n2.000000\n3.000000\n0.000000\n0.000000\n0.000000\n125.000000\n2018.000000\n10.000000\n23.000000\n1.000000\n0.000000\n0.000000\n0.000000\n120.000000\n1.000000\n1.000000\n\n\nmax\n4.000000\n9.000000\n6.000000\n17.000000\n3.000000\n1.000000\n6.000000\n443.000000\n2018.000000\n12.000000\n31.000000\n4.000000\n1.000000\n13.000000\n58.000000\n375.500000\n5.000000\n1.000000\n\n\n\n\n\n\n\n\nBased on the summary statistics provided, here are some initial observations about the data: * There are 12,695 observations in the train dataset. * The average number of adults per booking is 1.85 while the average number of children per booking is 0.11  * On average people visit between July and August, meaning the hotel is busier during summer holidays. * The average number of nights stayed over a weekend is 0.81 while the average number of nights stayed during the week is 2.21  * The average lead time for bookings is 84.95 days, with a standard deviation of 85.79.  * Only 2.6% of bookings are from repeated guests on average  * The average price per room is 103.51  * The majority of bookings (67.6%) are confirmed, while the remaining 32.4% are canceled."
  },
  {
    "objectID": "hc.html#background",
    "href": "hc.html#background",
    "title": "Predictive model to flag potential cancellations",
    "section": "",
    "text": "The behavior of customers and booking options have thoroughly altered by the online hotel reservation channels. A significant number of reservations are canceled always, mostly due to cancellations or no-shows. Cancellations can be caused by a variety of factors, such as scheduling conflicts, changes in plans, etc. This is observed more frequently due to the fact that hotels often make it easier for guests to cancel their reservations by offering free or low cost cancellation options. Although this benefits the guests heavily, it may risk the hotel losing on potential revenue.\nTherefore, the outcome of this project is to assist the hotel owners to better understand whether a customer will confirm the booking or cancel it by developing and implementing machine learning methodologies.\nSource : The dataset used in this project is from Kaggle. Link -&gt; Reservation Cancellation Prediction Dataset\n\n\n\nimport os\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.cluster import DBSCAN, KMeans\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import (\n    GridSearchCV,\n    RandomizedSearchCV,\n    cross_validate,\n    train_test_split,\n)\n\nfrom sklearn.metrics import (\n    classification_report,\n    confusion_matrix,\n    f1_score,\n    recall_score,\n    precision_score,\n    make_scorer,\n    \n)\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\npd.set_option(\"display.max_colwidth\", 0)\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.feature_selection import SequentialFeatureSelector\n\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)\n\nimport shap, eli5\n\n\n\n\nIn this exploratory data analysis (EDA) section, we load the training dataset from a CSV file and prepare it for analysis. We split the data into training and testing sets, with 70% allocated for training. Then, we provide a summary of the descriptive statistics for the training dataset to gain insights into its distribution and characteristics.\n\ndata = pd.read_csv(\"train__dataset.csv\")\nX = data.drop(\"booking_status\", axis=1)\ny = data[\"booking_status\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=111, train_size = 0.7) # setting a random state to make it reproducible\n\ndf_train = X_train.join(y_train)\ndf_train.describe()\n\n\n\n\n\n\n\n\n\nno_of_adults\nno_of_children\nno_of_weekend_nights\nno_of_week_nights\ntype_of_meal_plan\nrequired_car_parking_space\nroom_type_reserved\nlead_time\narrival_year\narrival_month\narrival_date\nmarket_segment_type\nrepeated_guest\nno_of_previous_cancellations\nno_of_previous_bookings_not_canceled\navg_price_per_room\nno_of_special_requests\nbooking_status\n\n\n\n\ncount\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n12695.000000\n\n\nmean\n1.845845\n0.106892\n0.808980\n2.214100\n0.313037\n0.031902\n0.336510\n84.945884\n2017.821189\n7.441985\n15.628909\n0.806459\n0.026073\n0.022922\n0.159354\n103.513689\n0.616384\n0.324143\n\n\nstd\n0.517027\n0.410776\n0.871795\n1.418276\n0.623953\n0.175747\n0.774089\n85.788163\n0.383209\n3.081619\n8.779931\n0.645516\n0.159359\n0.379929\n1.785368\n35.090118\n0.790173\n0.468072\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n2017.000000\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n2.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n0.000000\n17.000000\n2018.000000\n5.000000\n8.000000\n0.000000\n0.000000\n0.000000\n0.000000\n80.750000\n0.000000\n0.000000\n\n\n50%\n2.000000\n0.000000\n1.000000\n2.000000\n0.000000\n0.000000\n0.000000\n57.000000\n2018.000000\n8.000000\n16.000000\n1.000000\n0.000000\n0.000000\n0.000000\n99.450000\n0.000000\n0.000000\n\n\n75%\n2.000000\n0.000000\n2.000000\n3.000000\n0.000000\n0.000000\n0.000000\n125.000000\n2018.000000\n10.000000\n23.000000\n1.000000\n0.000000\n0.000000\n0.000000\n120.000000\n1.000000\n1.000000\n\n\nmax\n4.000000\n9.000000\n6.000000\n17.000000\n3.000000\n1.000000\n6.000000\n443.000000\n2018.000000\n12.000000\n31.000000\n4.000000\n1.000000\n13.000000\n58.000000\n375.500000\n5.000000\n1.000000\n\n\n\n\n\n\n\n\nBased on the summary statistics provided, here are some initial observations about the data: * There are 12,695 observations in the train dataset. * The average number of adults per booking is 1.85 while the average number of children per booking is 0.11  * On average people visit between July and August, meaning the hotel is busier during summer holidays. * The average number of nights stayed over a weekend is 0.81 while the average number of nights stayed during the week is 2.21  * The average lead time for bookings is 84.95 days, with a standard deviation of 85.79.  * Only 2.6% of bookings are from repeated guests on average  * The average price per room is 103.51  * The majority of bookings (67.6%) are confirmed, while the remaining 32.4% are canceled."
  },
  {
    "objectID": "hc.html#data-visualization",
    "href": "hc.html#data-visualization",
    "title": "Predictive model to flag potential cancellations",
    "section": "Data visualization:",
    "text": "Data visualization:\nTo identify patterns or trends in the data relative to target class, we can create visualisations of some features for each target classes to see the differences between them.\n\nOutlier Analysis of Numerical Features:\n\n# Define the features\nfeatures = ['lead_time', 'avg_price_per_room']\nfig, axes = plt.subplots(nrows = 1, ncols=len(features), figsize=(15, 6))\nfig.suptitle(\"Boxen Plots of Features\", fontsize=16, fontweight='bold')\n\n# Iterate over features\nfor i, feature in enumerate(features):\n    sns.boxenplot(data=df_train, y = feature, x = 'booking_status', palette=\"husl\", ax=axes[i])\n    axes[i].set_title(\"Boxen Plot of \" + feature + \" by booking status\")\n\n\n\n\n\n\n\n\nThe provided boxen plots offer insights into booking behavior based on lead time and average room price. For lead time, bookings not cancelled tend to have shorter lead times, possibly indicating last-minute reservations, while cancelled bookings show longer and more varied lead times, suggesting advanced bookings with diverse lead times. For average room price, non-cancelled bookings exhibit similar median prices as cancelled bookings. This insight challenges the initial observation and prompts the need for further analysis to better understand guest booking preferences and cancellation behaviors.\n\n\nDistribution Analysis of Numerical Features:\n\n# Define the features\nfeatures = ['lead_time', 'avg_price_per_room']\nfig, axes = plt.subplots(nrows = 1, ncols=len(features), figsize=(10, 5))\nfig.suptitle(\"Density Plots of Features\", fontsize=16, fontweight='bold')\n\n# Iterate over features\nfor i, feature in enumerate(features):\n    sns.kdeplot(data=df_train, x=feature, hue=\"booking_status\", bw_adjust=2, fill=True, palette=\"husl\", ax=axes[i],\n                legend=False)\n    axes[i].set_title(\"KDE Plot of \" + feature + \" by booking status\")\nfig.legend(title = \"Booking Status\", labels=[\"Cancelled\", \"Not Cancelled\"], bbox_to_anchor=(1.1, 0.5), fontsize = \"large\", title_fontsize='large')\n\n\n\n\n\n\n\n\nThe histograms reveal distinct patterns in booking behavior and cancellation likelihood based on lead time and average room price. For lead time, bookings are predominantly confirmed when the interval between booking and arrival is short, with cancellations becoming more prevalent as lead time increases beyond 100 days. Conversely, guests are more likely to confirm bookings when the lead time is under 100 days. Regarding average room price, confirmed bookings consistently outnumber cancellations across all price ranges. However, the highest confirmation rates occur when the average room price falls between 50 and 125. Beyond this range, cancellation probabilities increase, approaching parity with confirmation probabilities. In summary, shorter lead times and moderate room prices between 50 and 125 are associated with higher confirmation rates, while longer lead times are associated with increased cancellation probabilities. These insights can inform hotel management strategies to optimize booking processes and minimize cancellation rates.\n\n\nDistribution Analysis of Categorical Features:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Define the categorical features\ncat_features = ['no_of_adults', 'no_of_children', 'no_of_weekend_nights', 'no_of_week_nights', \n                'type_of_meal_plan', 'required_car_parking_space', 'room_type_reserved', \n                'market_segment_type', 'repeated_guest', 'no_of_previous_cancellations', \n                'no_of_previous_bookings_not_canceled', 'no_of_special_requests']\n\n# Create subplots with 3 rows and 4 columns\nfig, axes = plt.subplots(nrows=3, ncols=4, figsize=(15, 10))\nfig.suptitle(\"Count Plots of Categorical Features\", fontsize=20, fontweight='bold')\n\n# Iterate over features\nfor i, feature in enumerate(cat_features):\n    # Filter the DataFrame to include only the top N most frequent categories\n    top_categories = df_train[feature].value_counts().index[:10]\n    filtered_df = df_train[df_train[feature].isin(top_categories)]\n    \n    row = i // 4  # Calculate the row index\n    col = i % 4   # Calculate the column index\n    sns.countplot(data=filtered_df, x=feature, hue=\"booking_status\", palette=\"mako\", ax=axes[row, col])\n    axes[row, col].set_title(feature)\n    axes[row, col].legend().set_visible(False)\n\nfig.legend(title = \"Booking Status\", labels=[\"Not cancelled\", \"Cancelled\"], bbox_to_anchor=(1.1, 0.5), fontsize = \"large\", title_fontsize='large')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nBased on the bar plots:\n\nNumber of Adults and Children: Most bookings are made for one or two adults, and very few include children. This suggests that the hotel is primarily used by adults, possibly for business or couples on vacation.\nNumber of Weekend Nights and Week Nights: Guests typically stay from one to four nights during both weekends and weekdays. This could indicate that the hotel is popular for short stays.\nType of Meal Plan: A specific type of meal plan is predominantly chosen by guests. This could be due to the convenience or cost-effectiveness of this plan.\nRequired Car Parking Space: A large number of guests do not require car parking spaces, which might suggest that many guests use public transportation or other means of travel.\nRoom Type Reserved: Various room types are reserved with one being more common. This could indicate a preference for a particular room type, possibly due to cost, size, or amenities.\nMarket Segment Type: There’s a notable difference in market segment types between cancelled and not cancelled bookings. This could suggest that certain market segments are more likely to cancel their bookings.\nRepeated Guest: Most guests are not repeated ones, indicating that the hotel has a diverse guest population.\nNumber of Previous Cancellations and Bookings Not Cancelled: A majority have zero previous cancellations and many have no prior non-cancelled reservations. This could suggest that most guests are first-time visitors.\nNumber of Special Requests: Special requests during stays are relatively uncommon, which might indicate that most guests’ needs are met by the standard amenities provided by the hotel.\n\nThese observations provide valuable insights into the hotel’s guest demographics and their preferences, which could be useful for making strategic decisions to improve guest satisfaction and business performance. However, these are just observations based on the given plots, and further analysis would be needed to draw more concrete conclusions."
  },
  {
    "objectID": "hc.html#modeling",
    "href": "hc.html#modeling",
    "title": "Predictive model to flag potential cancellations",
    "section": "Modeling:",
    "text": "Modeling:\n\nPreprocessing and transformations:\n\nnumeric_features = [\"no_of_adults\", \"no_of_children\",\"no_of_weekend_nights\",\"no_of_week_nights\", \"lead_time\", \"no_of_previous_cancellations\",\"no_of_previous_bookings_not_canceled\",\"avg_price_per_room\",\"no_of_special_requests\", \"arrival_month\"] # apply scaling\ncategorical_features = [\"type_of_meal_plan\", \"room_type_reserved\", \"market_segment_type\"] # apply one-hot encoding\nbinary_features = [\"required_car_parking_space\", \"repeated_guest\"] # apply one-hot encoding with drop=\"if_binary\"\ndrop_features = [\"arrival_year\", \"arrival_date\"] # customers make bookings depending on mostly months  \ntarget = \"booking_status\"\n\nnumeric_transformer = StandardScaler()\ncategorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\nbinary_transformer = OneHotEncoder(drop=\"if_binary\", dtype=int)\n\npreprocessor = make_column_transformer(\n    (numeric_transformer, numeric_features),\n    (categorical_transformer, categorical_features),\n    (binary_transformer, binary_features),\n    (\"drop\", drop_features),\n)\n\nWe separate features into numerical and categorical types because they require different types of transformations. We apply one-hot encoding on the categorical features and standard scaling on the numerical features. In this project, we will approach different methods and employ different models. One of such models is a logistic regression model.\nBelow is a helper function to make calculating validation scores easier:\n\ndef mean_std_cross_val_scores(model, X_train, y_train, scoring, **kwargs):\n    \"\"\"\n    Returns mean and std of cross validation\n\n    Parameters\n    ----------\n    model :\n        scikit-learn model\n    X_train : numpy array or pandas DataFrame\n        X in the training data\n    y_train :\n        y in the training data\n\n    Returns\n    ----------\n        pandas Series with mean scores from cross_validation\n    \"\"\"\n    if scoring == \"f1\":\n        scores = cross_validate(model, X_train, y_train, scoring = \"f1\", **kwargs)\n        mean_scores = pd.DataFrame(scores).mean()\n        std_scores = pd.DataFrame(scores).std()\n        out_col = []\n\n        for i in range(len(mean_scores)):\n            out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n\n        return pd.Series(data=out_col, index=mean_scores.index)\n    else:\n        scores = cross_validate(model, X_train, y_train, **kwargs)\n\n        mean_scores = pd.DataFrame(scores).mean()\n        std_scores = pd.DataFrame(scores).std()\n        out_col = []\n\n    for i in range(len(mean_scores)):\n        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n\n    return pd.Series(data=out_col, index=mean_scores.index)\n\n\n\nLinear models:\n\nresults_dict = {}\nC_vals = 10.0 ** np.arange(-2, 2, 0.5)\n\nfor C in C_vals:\n    lr_pipe = make_pipeline(preprocessor, LogisticRegression(max_iter=1000, C=C))\n    results_dict[\"Logistic Regression with C=\" + str(C)] = mean_std_cross_val_scores(\n        lr_pipe, X_train, y_train, return_train_score=True, scoring =\"accuracy\"\n    )\n    \npd.DataFrame(results_dict)\n\n\n\n\n\n\n\n\n\nLogistic Regression with C=0.01\nLogistic Regression with C=0.03162277660168379\nLogistic Regression with C=0.1\nLogistic Regression with C=0.31622776601683794\nLogistic Regression with C=1.0\nLogistic Regression with C=3.1622776601683795\nLogistic Regression with C=10.0\nLogistic Regression with C=31.622776601683793\n\n\n\n\nfit_time\n0.029 (+/- 0.004)\n0.032 (+/- 0.003)\n0.045 (+/- 0.002)\n0.061 (+/- 0.001)\n0.089 (+/- 0.005)\n0.119 (+/- 0.008)\n0.150 (+/- 0.010)\n0.147 (+/- 0.020)\n\n\nscore_time\n0.006 (+/- 0.001)\n0.005 (+/- 0.001)\n0.005 (+/- 0.000)\n0.006 (+/- 0.000)\n0.006 (+/- 0.001)\n0.006 (+/- 0.000)\n0.005 (+/- 0.001)\n0.006 (+/- 0.000)\n\n\ntest_score\n0.798 (+/- 0.009)\n0.800 (+/- 0.007)\n0.802 (+/- 0.006)\n0.802 (+/- 0.006)\n0.803 (+/- 0.006)\n0.803 (+/- 0.006)\n0.803 (+/- 0.006)\n0.803 (+/- 0.006)\n\n\ntrain_score\n0.798 (+/- 0.003)\n0.800 (+/- 0.003)\n0.803 (+/- 0.003)\n0.803 (+/- 0.002)\n0.804 (+/- 0.002)\n0.804 (+/- 0.002)\n0.804 (+/- 0.002)\n0.804 (+/- 0.002)\n\n\n\n\n\n\n\n\nFrom the above observations, we can conclude that changing C values after a specific threshold does not affect the training scores. Thus, we should try optimizing a different hyperparameter or try using another model and optimize it to get a better model.\nTherefore, we try using another model with a different estimator. Since linear models seemed to be at its limit, we can improve our results by exploring the use of non-linear estimators and evaluating their effectiveness.\n\n\nNon-linear models:\n\nresults_dict = {}\nmodels = {\n    \"Decision Trees\": DecisionTreeClassifier(random_state=111),\n    \"kNN\": KNeighborsClassifier(),\n    \"Random Forest\": RandomForestClassifier(n_jobs=-1, n_estimators=100, random_state = 111)\n}\n\nfor x in models:\n    for y in [\"accuracy\", \"f1\"]:\n        pipe = make_pipeline(preprocessor, models[x])\n        results_dict[x + \" \" + str(y)] = mean_std_cross_val_scores(\n            pipe, X_train, y_train, return_train_score=True, scoring = y)\n        \npd.DataFrame(results_dict)\n\n\n\n\n\n\n\n\n\nDecision Trees accuracy\nDecision Trees f1\nkNN accuracy\nkNN f1\nRandom Forest accuracy\nRandom Forest f1\n\n\n\n\nfit_time\n0.044 (+/- 0.001)\n0.042 (+/- 0.001)\n0.013 (+/- 0.001)\n0.012 (+/- 0.001)\n1.068 (+/- 1.683)\n0.306 (+/- 0.004)\n\n\nscore_time\n0.005 (+/- 0.001)\n0.007 (+/- 0.001)\n0.178 (+/- 0.029)\n0.163 (+/- 0.001)\n0.034 (+/- 0.004)\n0.033 (+/- 0.002)\n\n\ntest_score\n0.845 (+/- 0.005)\n0.763 (+/- 0.007)\n0.832 (+/- 0.006)\n0.732 (+/- 0.008)\n0.883 (+/- 0.006)\n0.810 (+/- 0.010)\n\n\ntrain_score\n0.996 (+/- 0.000)\n0.994 (+/- 0.001)\n0.882 (+/- 0.002)\n0.812 (+/- 0.004)\n0.996 (+/- 0.000)\n0.994 (+/- 0.001)\n\n\n\n\n\n\n\n\nBased on the mean test scores, Random Forest seems to be performing the best, with an mean test score around 0.883. This is followed by Decision Trees with a score 0.845, and kNN with a score 0.832. Random Forest also have the best f1 score meaning that it can correctly identify both positive and negative examples with the highest accuracy among the rest of the models.\nGiven that the Random Forest model is the most effective among other models, we will focus on optimizing its hyperparameters to improve its performance.\n\n\nOptimization:\nWe will look to optimize the hyperparameters max_depth, n_estimators and class_weight. We will use random search, an automated technique to find the best set of hyperparameters.\n\nrf = RandomForestClassifier(n_jobs=-1, random_state=111)\nac_search_results = []\nbest_accuracy = 0\nbest_accuracy_params = []\n\nf1_search_results =[]\nbest_f1 = 0\nbest_f1_params = []\n\nparam_grid = {\"randomforestclassifier__max_depth\": [10, 20, 30, 40, 50],\n             \"randomforestclassifier__n_estimators\": [50, 100, 200, 250, 300],\n             \"randomforestclassifier__class_weight\": [None, {0:1, 1:3},'balanced']}\n\npipe = make_pipeline(preprocessor, rf)\nrandom_search = RandomizedSearchCV(\n    pipe, param_grid, cv=5, return_train_score=True, n_iter=15, random_state=111\n)\nrandom_search.fit(X_train, y_train)\nfor x in [\"accuracy\", \"f1\"]:\n    pipe = make_pipeline(preprocessor, rf)\n    random_search = RandomizedSearchCV(\n        pipe, param_grid, cv=5, return_train_score=True, n_iter=15, random_state=111, scoring = x\n    )\n    random_search.fit(X_train, y_train)\n    if x == \"accuracy\":\n        ac_search_results = pd.DataFrame(random_search.cv_results_)\n        best_accuracy = random_search.best_score_\n        best_accuracy_params = random_search.best_params_\n    else:\n        f1_search_results = pd.DataFrame(random_search.cv_results_)\n        best_f1 = random_search.best_score_\n        best_f1_params = random_search.best_params_\n\n\nprint(best_accuracy_params, best_accuracy)\nprint(best_f1_params, best_f1)\n\n{'randomforestclassifier__n_estimators': 250, 'randomforestclassifier__max_depth': 50, 'randomforestclassifier__class_weight': 'balanced'} 0.8844426939740055\n{'randomforestclassifier__n_estimators': 200, 'randomforestclassifier__max_depth': 20, 'randomforestclassifier__class_weight': {0: 1, 1: 3}} 0.8158630987451104\n\n\nAt first glance, the accuracy is more improved than the what we got intially. Also, the f1 increased slightly. The best f1 params and accuracy params differ by just the class weight. Looking at the f1_search_results and ac_search_ results, the difference between the accuracies and f1 scores between these two params is trivial for both optimum parameters. Therefore, we pick the best f1 params as the accuracy score (for best f1 params) is slightly lower than its best score. Also, we can afford to lose a very small accuracy score for a better f1 score. Moreover, using lower n_estimators will make fitting faster.\nTherefore, the final optimized model: RandomForestClassifier(n_estimators = 200, max_depth=20, class_weight = {0: 1, 1: 3})\n\n\nFeature importances and selection:\nAn important part of this project involved creating and transforming the features used in our model, given a rather large amount of raw data. First, we need to find which features are important to the model. Then, we will use feature selection to select specific features to make our model simpler and better.\n\nFeature importances:\nWe will use eli5 to view which features are important.\n\nX_train_pp = preprocessor.fit_transform(X_train)\n\ncolumn_names = numeric_features + list(\n    preprocessor.named_transformers_[\"onehotencoder-1\"].get_feature_names_out(categorical_features)) + list(\n    preprocessor.named_transformers_[\"onehotencoder-2\"].get_feature_names_out(binary_features))\n\npipe_rf = make_pipeline(preprocessor, RandomForestClassifier(n_estimators = 200, max_depth=20, n_jobs=-1, random_state=111, class_weight = {0: 1, 1: 3}))\npipe_rf.fit(X_train, y_train)\neli5.explain_weights(pipe_rf.named_steps[\"randomforestclassifier\"], feature_names=column_names)\n\n\n\n    \n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n        \n\n\n\nWeight\nFeature\n\n\n\n\n0.3301 ± 0.0448\nlead_time\n\n\n0.1631 ± 0.0372\navg_price_per_room\n\n\n0.1301 ± 0.0478\nno_of_special_requests\n\n\n0.1037 ± 0.0224\narrival_month\n\n\n0.0589 ± 0.0154\nno_of_week_nights\n\n\n0.0436 ± 0.0147\nno_of_weekend_nights\n\n\n0.0362 ± 0.0483\nmarket_segment_type_1\n\n\n0.0249 ± 0.0128\nno_of_adults\n\n\n0.0227 ± 0.0348\nmarket_segment_type_0\n\n\n0.0123 ± 0.0060\nrequired_car_parking_space_1\n\n\n0.0085 ± 0.0061\ntype_of_meal_plan_0\n\n\n0.0083 ± 0.0047\nno_of_children\n\n\n0.0083 ± 0.0130\ntype_of_meal_plan_2\n\n\n0.0082 ± 0.0055\nroom_type_reserved_0\n\n\n0.0076 ± 0.0152\nmarket_segment_type_2\n\n\n0.0073 ± 0.0054\nroom_type_reserved_1\n\n\n0.0070 ± 0.0078\ntype_of_meal_plan_1\n\n\n0.0050 ± 0.0140\nno_of_previous_bookings_not_canceled\n\n\n0.0045 ± 0.0117\nrepeated_guest_1\n\n\n0.0024 ± 0.0022\nroom_type_reserved_2\n\n\n… 8 more …\n\n\n\n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n\n\n\n\nFrom the table: * lead_time, avg_price_per_room, no_of_special_requests and arrival month have the highest weight among all the features(&gt;0.1). * no_of_week_nights, no_of_weekend_nights , market_segment_type_1, and no_of_adults have moderate to low weight. * The rest of the features have very low to no weight. * lead_time is the most important feature, followed by avg_price_per_room and no_of_special_requests.\n\npp_df = pd.DataFrame(X_train_pp, columns=column_names)\nrf_explainer = shap.TreeExplainer(pipe_rf.named_steps[\"randomforestclassifier\"])\ntrain_rf_shap_values = rf_explainer.shap_values(pp_df.sample(n=1000, random_state = 111)) # choosing only 1000 samples c\n\n\nshap.summary_plot(train_rf_shap_values[1], pp_df.sample(n=1000, random_state = 111))\nshap.dependence_plot(\"lead_time\", train_rf_shap_values[1], pp_df.sample(n=1000, random_state = 111))\nshap.dependence_plot(\"avg_price_per_room\", train_rf_shap_values[1], pp_df.sample(n=1000, random_state = 111))\n\nNo data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights summary:\n\nFirst plot: The analysis reveals a strong correlation between lead_time values and the corresponding shap values. High lead_time values tend to have high shap values, while low lead_time values are associated with low shap values. This indicates that high lead_times are more likely to predict the outcome 1, whereas lower lead_times are more likely to predict 0. Similarly, for the no_of_special_requests feature, it is observed that high values tend to have low shap values, indicating that high no_of_special_requests are more likely to predict 0, while lower values are more likely to predict 1. Additionally, the analysis shows a similar pattern for the avg_price_per_room feature. High avg_price_per_room values have high shap values, indicating a higher likelihood of predicting 1, whereas low avg_price_per_room values have low shap values, suggesting a higher likelihood of predicting 0. Furthermore, the required_car_parking_space_1 feature has low shap values for high values, meaning that high required_car_parking_space_1 values are more likely to predict 1, while lower values are more likely to predict 0.\nSecond plot: The majority of data points in the graph correspond to market_segment_type_1. Additionally, there is a positive relationship between the lead_time values and their respective shap values, suggesting that as lead_time increases, so does its impact on the prediction. Non-market_segment_type_1 data points are mostly concentrated near the bottom of the graph, with a few outliers at the top.\nThird plot: The graph indicates a mixed proportion of high and low lead_time values among the data points. Moreover, there is a positive correlation between the avg_price_per_room values and their corresponding shap values, indicating that higher avg_price_per_room values have a greater impact on the prediction.\n\n\n\nFeature selection:\nWe will use RFECV to select the number of features to use in our model. It uses cross-validation to select number of features.\n\nmodel = RandomForestClassifier(n_estimators = 200, max_depth=20, class_weight = {0: 1, 1: 3})\nrfe_cv = RFECV(model, cv=10,n_jobs=-1)\nrfe_cv.fit(X_train_pp, y_train)\n\nprint('Original number of features:', X_train_pp.shape[1])\nprint('Number of selected features:', sum(rfe_cv.support_), '\\n\\n')\nprint(\"Selected features: \", np.array(column_names)[rfe_cv.support_])\n\nOriginal number of features: 28\nNumber of selected features: 18 \n\n\nSelected features:  ['no_of_adults' 'no_of_children' 'no_of_weekend_nights'\n 'no_of_week_nights' 'lead_time' 'avg_price_per_room'\n 'no_of_special_requests' 'arrival_month' 'type_of_meal_plan_0'\n 'type_of_meal_plan_1' 'type_of_meal_plan_2' 'room_type_reserved_0'\n 'room_type_reserved_1' 'market_segment_type_0' 'market_segment_type_1'\n 'market_segment_type_2' 'required_car_parking_space_1' 'repeated_guest_1']\n\n\nAs seen from the results, we have managed to reduce the number of features considerably (by half). Next, we will use this to check whether our model has improved or how much it improved.\n\nnew_train = rfe_cv.transform(X_train_pp)\n\nprint('accuracy scores:\\n \\n', mean_std_cross_val_scores(\n    model, new_train, y_train, return_train_score=True, scoring =\"accuracy\"\n),'\\n \\n','f1 scores:\\n \\n', mean_std_cross_val_scores(\n    model, new_train, y_train, return_train_score=True, scoring =\"f1\"\n))\n\naccuracy scores:\n \n fit_time       1.575 (+/- 0.047)\nscore_time     0.078 (+/- 0.001)\ntest_score     0.882 (+/- 0.004)\ntrain_score    0.986 (+/- 0.001)\ndtype: object \n \n f1 scores:\n \n fit_time       1.651 (+/- 0.083)\nscore_time     0.083 (+/- 0.005)\ntest_score     0.817 (+/- 0.009)\ntrain_score    0.978 (+/- 0.001)\ndtype: object\n\n\nThe accuracy scores and f1 scores increased very slightly but it is still a major improvement since our model is much simpler and considers lower parameters.\nWe have a proper model and have selected important features to train our model. Now, we will check how the model works on unseen datasets."
  },
  {
    "objectID": "hc.html#results",
    "href": "hc.html#results",
    "title": "Predictive model to flag potential cancellations",
    "section": "Results:",
    "text": "Results:\n\nmodel.fit(new_train,y_train)\nnew_test = preprocessor.transform(X_test)\nnew_test = rfe_cv.transform(new_test)\n\naccuracy_testscore = model.score(new_test, y_test)\nf1_testscore = f1_score(y_test, model.predict(new_test))\nprint(\"Accuracy: \" + str(accuracy_testscore))\nprint(\"F1 score: \" + str(f1_testscore))\n\nAccuracy: 0.8827636898199192\nF1 score: 0.8212885154061624\n\n\nFrom the train and test scores above: * It has good test scores meaning it effectively classifies the data into their respective classes. * The test scores agree with the validation scores from before. * The test values are overfit. The predictions have high variance among them but low bias. * The difference in train/test value means the model might not generalize well to new unseen data.\n\nModel caveats:\nThroughout the project, the models created and methods used might not be best and the following results might not be perfect. Having said that, the following caveats can be inferred:\n\nGeneralizability: The significant difference between train accuracy/F1 score and test F1 accuracy/F1 score means that the model could be overfitting the training data. As a result, it might not generalize well to unseen data. This is mainly due to the fact we are using an ensemble model and other steps we took to optimize it such as RFECV, random search.\nSuboptimal search results: Using grid search instead of random search could have provided better performing parameters since it is an exhaustive search. The hyperparameters found might be suboptimal compared to what we could have found using grid search.\nTime series relationships: The relationship between the features and booking_status can change across the years/different dates. Since we did not consider time series analysis, we might have missed underlying patterns in the data, resulting in suboptimal model performance.\nComplexity: Since we used an ensemble model, the model is complex and requires high computational power. Therefore, it is harder to understand or interpret the model, and its training time is also higher than usual.\nNon-important features: RFECV might not have identified all the redundant features, which can increase model complexity and negatively impact the its performance.\n\nOverall, the model still works as an useful tool to assist the hotel owners in better understanding whether a customer will honor the reservation or cancel it. Although the model is far from perfect, it can always be improved or further optimized. Training the model with new data sets occasionally will also improve the models performance in making informed decisions about managing hotel reservations."
  },
  {
    "objectID": "visuals.html",
    "href": "visuals.html",
    "title": "Visualizations",
    "section": "",
    "text": "Year-wise Percentage of Internet Users Globally\n\n\n\npython\n\n\nplotly\n\n\n\n\n\n\n\nArnab Das\n\n\nJan 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTop FIFA national teams over the years\n\n\n\npython\n\n\nMatplotlib\n\n\n\n\n\n\n\nArnab Das\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "visuals/iu/index.html",
    "href": "visuals/iu/index.html",
    "title": "Year-wise Percentage of Internet Users Globally",
    "section": "",
    "text": "The provided code reads data from the World Bank’s indicator for internet users (% of population), available at World Bank Internet Users Data, and processes it for visualization using Plotly. After reading the data, fill missing values by forward-filling along the rows to replace NaN values, ensuring a smoother time series representation. The cleaned data is then reshaped into a long format using pd.melt() to prepare it for plotting with Plotly. Non-numeric values are filtered out to ensure data integrity. This structured dataset, named melted_data, is now ready for visualization, providing insights into internet usage trends across different countries over time.\nLoad Python libraries commonly used for data analysis, and visualization.\n\n\nShow the code\nimport pandas as pd\nimport plotly.graph_objects as go\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nimport scipy\nimport kaleido\nfrom PIL import Image\nimport plotly.io as pio\nimport io\nfrom itables import init_notebook_mode\nimport os\nimport plotly.express as px\n\n\n\n\nShow the code\ninit_notebook_mode(all_interactive=True)\ndata = pd.read_csv(\"interdata.csv\")\n\n# front fill to replace NA (2016 -&gt; 2017 -&gt; 2018)\nclean_data = data.fillna(method = \"ffill\", axis=1)\n\n# structure data for plotly\nmelted_data = pd.melt(clean_data, id_vars=['Country Name', 'Country Code'], var_name='Year', value_name='Value')\n# remove rows with non-numeric values\nmelted_data = melted_data[pd.to_numeric(melted_data['Value'], errors='coerce').notnull()]\ndisplay(melted_data)\n\n\n\n\n\n\n\n\n    \n      \n      Country Name\n      Country Code\n      Year\n      Value\n    \n  Loading... (need help?)"
  },
  {
    "objectID": "visuals/iu/index.html#data",
    "href": "visuals/iu/index.html#data",
    "title": "Year-wise Percentage of Internet Users Globally",
    "section": "",
    "text": "The provided code reads data from the World Bank’s indicator for internet users (% of population), available at World Bank Internet Users Data, and processes it for visualization using Plotly. After reading the data, fill missing values by forward-filling along the rows to replace NaN values, ensuring a smoother time series representation. The cleaned data is then reshaped into a long format using pd.melt() to prepare it for plotting with Plotly. Non-numeric values are filtered out to ensure data integrity. This structured dataset, named melted_data, is now ready for visualization, providing insights into internet usage trends across different countries over time.\nLoad Python libraries commonly used for data analysis, and visualization.\n\n\nShow the code\nimport pandas as pd\nimport plotly.graph_objects as go\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nimport scipy\nimport kaleido\nfrom PIL import Image\nimport plotly.io as pio\nimport io\nfrom itables import init_notebook_mode\nimport os\nimport plotly.express as px\n\n\n\n\nShow the code\ninit_notebook_mode(all_interactive=True)\ndata = pd.read_csv(\"interdata.csv\")\n\n# front fill to replace NA (2016 -&gt; 2017 -&gt; 2018)\nclean_data = data.fillna(method = \"ffill\", axis=1)\n\n# structure data for plotly\nmelted_data = pd.melt(clean_data, id_vars=['Country Name', 'Country Code'], var_name='Year', value_name='Value')\n# remove rows with non-numeric values\nmelted_data = melted_data[pd.to_numeric(melted_data['Value'], errors='coerce').notnull()]\ndisplay(melted_data)\n\n\n\n\n\n\n\n\n    \n      \n      Country Name\n      Country Code\n      Year\n      Value\n    \n  Loading... (need help?)"
  },
  {
    "objectID": "visuals/iu/index.html#geospatial-visualization",
    "href": "visuals/iu/index.html#geospatial-visualization",
    "title": "Year-wise Percentage of Internet Users Globally",
    "section": "Geospatial visualization:",
    "text": "Geospatial visualization:\n\nInteractive:\nThe code below creates an animated choropleth map to visualize country-wise percentage of internet users globally over time. Animation controls allow users to play through the frames, visualizing changes over time. This interactive visualization offers insights into the evolution of internet usage across different countries globally, providing a dynamic representation of population trends.\n\n\nShow the code\nmelted_data['Value'] = pd.to_numeric(melted_data['Value'])\nmelted_data['Year'] = melted_data['Year'].astype(str)\nmelted_data['Country Code'] = melted_data['Country Code'].astype(str)\n\n\npio.renderers.default = \"notebook\"\nfrom urllib.request import urlopen\nimport json\nwith urlopen('https://raw.githubusercontent.com/johan/world.geo.json/master/countries.geo.json') as response:\n    countries = json.load(response)\n\nfig = px.choropleth_mapbox(\n    melted_data,\n    geojson=countries,\n    locations=\"Country Code\",\n    color_continuous_scale=\"Ice\",    \n    animation_frame=\"Year\",\n    color= 'Value',\n    labels={'Value': 'Percentage'},\n    mapbox_style=\"carto-positron\",  # Use a dark map style\n    zoom=1,  # Set the initial zoom level,\n    range_color = (0,100),\n    opacity=0.5,\n    hover_name=\"Country Name\",\n    height= 600,\n    width= 800,\n)\n\n    \nfig.update_layout(\n    title= \"Internet Users Globally\",\n    geo=dict(\n        showframe=False,\n        showcoastlines=False,\n    ),\n    paper_bgcolor='black',  # Set the overall background color to black\n    font=dict(color='white'),  # Set font color to white\n    height=500,  # Set the height of the plot interface\n    updatemenus=[dict(type='buttons',\n                      showactive=False,\n                      y=-0.15,\n                      x= -0.1,\n                      xanchor='left',\n                      yanchor='bottom')\n                 ],\n)\n\nfig.update_layout(\n    font=dict(\n        color=\"white\",  # Set font color to white\n        size=15,  # Set font size\n    ),\n)\nfig.update_layout(\n    hoverlabel=dict(\n        bgcolor=\"#18191A\",\n        font_size=12,\n        font_family=\"Arial\",\n    )\n)\n\nfig['layout']['sliders'][0]['pad'] = dict(r=70, t=0.0,)\n\nfig.show()\n\n\n                                                \n\n\n\n\nGIF:\nThis code snippet converts the above interactive plot into a GIF which offers a dynamic portrayal of global internet usage trends over time, emphasizing localized shifts in internet adoption rates.\n\n\nShow the code\nmaps = []\nfor year in melted_data['Year'].unique():\n    # Filter data for the current year\n    data_year = melted_data[melted_data['Year'] == year]\n    \n    # Create a Choropleth map for the current year\n    choropleth_map = go.Choroplethmapbox(\n        geojson=countries,\n        locations = data_year['Country Code'],\n        z = data_year['Value'],\n        text=data_year['Country Name'],\n        colorscale = 'Ice',\n        zmin = 0,\n        zmax = 100,\n        colorbar_title = 'Value',\n        name=str(year),\n        marker=dict(opacity=0.5),\n    )\n    fig = go.Figure(choropleth_map)\n    fig.update_layout(mapbox_style=\"carto-positron\",\n                      margin=dict(l=0, r=0, t=30, b=0),  # Set margin to 0 to remove padding\n                      title=f\"Internet users by year {year}\", \n                      paper_bgcolor='black',  # Set the overall background color to black\n                      font=dict(color='white'),  # Set font color to white\n                      height= 600,\n                      width= 1000,\n                      mapbox=dict(\n                        center=dict(lat=30, lon=0),  # Set the center of the map\n                        zoom=1,  # Set the initial zoom level\n                    )           \n                )\n    fig.update_layout(\n    font=dict(\n        color=\"white\",  # Set font color to white\n        size=15,        # Set font size\n        )\n    )  \n    # Append map to map list\n    maps.append(fig.to_image(format=\"png\", height= 600, width= 1000))\nimages_pil = []\n\n# Convert each PNG image byte to a PIL Image object\nfor image_bytes in maps:\n    image_pil = Image.open(io.BytesIO(image_bytes))\n    images_pil.append(image_pil)\n\n# create gif\nimages_pil[0].save(\"maps_animation.gif\",\n                   save_all = True,\n                   append_images = images_pil[1:],\n                   duration = 120,\n                   loop = 0,\n                   disposal=2,\n                   quality = 50)\n\n\n\n\n\nmaps_animation.gif"
  },
  {
    "objectID": "visuals/iu/index.html#barplot",
    "href": "visuals/iu/index.html#barplot",
    "title": "Year-wise Percentage of Internet Users Globally",
    "section": "Barplot:",
    "text": "Barplot:\nThis snippet processes country data from a CSV file, adjusts column names, merges it with additional data based on country names, then displays the combined dataset, effectively consolidating continent and country information.\n\n\nShow the code\ncountry_info = pd.read_csv(\"countryinfo.csv\")\ncountry_info.rename(columns={'name': 'Country Name'}, inplace=True)\nmerged_df = pd.merge(melted_data, country_info[['Country Name', 'region']], on='Country Name', how='left').dropna()\n\ndisplay(merged_df)\n\n\n\n\n    \n      \n      Country Name\n      Country Code\n      Year\n      Value\n      region\n    \n  Loading... (need help?)\n\n\n\n\n\n\nInteractive:\nThis code generates bar plots, depicting the internet usage percentage data against country with color encoding for continents. The plot includes animation for yearly changes.\n\n\nShow the code\n# Plotting using Plotly Express\nfig = px.bar(merged_df, x='Country Name', y='Value', color='region',\n                 title='Value by Country', template = 'plotly_dark',\n                 hover_data=['Country Code'], animation_frame=\"Year\")\nfig.update_xaxes(showticklabels=False, title_text='Countries')\nfig.update_traces(textfont_size=12, textangle=0, textposition=\"outside\", cliponaxis=False)\n\n# Customize layout\nfig.update_layout(\n    title=\"Internet users by year\",\n    yaxis_title=\"Percentage\",\n    font=dict(size=14),\n    height = 600\n\n)\nfig.update_layout(updatemenus=[dict(type='buttons',\n                  showactive=False,\n                  y=-0.34,\n                  x=-0.09,\n                  xanchor='left',\n                  yanchor='bottom')\n])\n\n# Show plot\nfig.show()\n\n\n                                                \n\n\n\n\nGIF\nA GIF is created from this interactive plot, capturing each frame’s visualization to showcase the dynamic evolution of internet usage across countries over time.\n\n\nShow the code\nbars = []\nfor year in merged_df['Year'].unique():\n    # Filter data for the current year\n    data_year = merged_df[merged_df['Year'] == year]\n    \n    # Create bars for the current year\n    bar = px.bar(data_year, x='Country Name', y='Value', color='region',\n                 title='Value by Country', template = 'plotly_dark',\n                 hover_data=['Country Code'])\n    bar.update_yaxes(range=[0, 100])\n    bar.update_xaxes(showticklabels=False)\n    bar.update_xaxes(tickangle=40)\n    bar.update_layout(height= 400)\n    bar.update_xaxes(title_text='Countries')\n    bar.update_yaxes(title_text='Percentage')\n    # Append bar to bar list\n    bars.append(go.Figure(data=bar).update_layout(title=f\"Internet users by year {year}\", font=dict(size=14)\n).to_image(format=\"png\"))\n    # Set labels for x-axis and y-axis\n    images_pil = []\n    \n# Convert each PNG image byte to a PIL Image object\nfor image_bytes in bars:\n    image_pil = Image.open(io.BytesIO(image_bytes))\n    images_pil.append(image_pil)\n\n# create gif\nimages_pil[0].save(\"bars_animation.gif\",\n                   save_all = True,\n                   append_images = images_pil[1:],\n                   duration = 100,\n                   loop = 0)  \n\n\n\n\n\nbars_animation.gif"
  },
  {
    "objectID": "visuals/prace/index.html",
    "href": "visuals/prace/index.html",
    "title": "Top FIFA national teams over the years",
    "section": "",
    "text": "The results of the bar chart race showcase the ranking dynamics of the top FIFA national teams over a period of time. The process behind this visualization is explained in detail below, including the methodology used to create the bar chart race animation."
  },
  {
    "objectID": "visuals/prace/index.html#data-wrangling",
    "href": "visuals/prace/index.html#data-wrangling",
    "title": "Top FIFA national teams over the years",
    "section": "Data wrangling",
    "text": "Data wrangling\nLoad necessary libraries for the project.\n\n\nShow the code\nimport pandas as pd\nfrom matplotlib.offsetbox import OffsetImage,AnnotationBbox\nimport plotly.graph_objects as go\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nimport scipy\nimport kaleido\nfrom PIL import Image\nimport plotly.io as pio\nimport io\nfrom itables import init_notebook_mode\nimport os\nimport plotly.express as px\nfrom pytrends.request import TrendReq\nimport bar_chart_race as bcr\nfrom datetime import datetime\nimport requests\nfrom io import BytesIO\nimport matplotlib.animation as anim\n\n\nPrepare the data for visualization by cleaning and organizing it. The organised data is shown below. Create a extended dataset with interpolated values for better animation using total points and ranks as x-axis and y-axis respectively\n\n\nShow the code\ninit_notebook_mode(all_interactive=True)\n\n# reading data and modeling it for plotting\ndata = pd.read_csv(\"fifarank.csv\")[['total_points', 'country_full', 'rank_date', 'rank']]\ndata['points'] = pd.to_numeric(data['total_points'])\ngroup_df = data.groupby('rank_date').apply(lambda x: x.nlargest(10, 'total_points'))\ngroup_df = group_df.reset_index(drop=True)\n# selecting after 1999\ngroup_df = group_df[10*5*12:]\nclean_df = group_df.pivot_table(index='rank_date', columns='country_full', values='total_points').fillna(method='ffill')\nclean_df = clean_df.reset_index()\nrank_df = group_df.pivot_table(index='rank_date', columns='country_full', values='rank').fillna(12)\nrank_df = rank_df.reset_index()\nrank_df.index = range(len(rank_df))   # Setting index to integers\n\n# expand and interpolate for transition\ndef prepare_data(df, steps=10):\n    df.index = df.index * steps\n    l = df.index[-1] + 1\n    df_expanded = df.reindex(range(l))\n    df_expanded['rank_date'] = df_expanded['rank_date'].fillna(method='ffill')\n    df_expanded = df_expanded.set_index('rank_date')\n    rank_expanded = df_expanded.rank(axis = 1, method = 'first', ascending = False)\n    df_expanded = df_expanded.interpolate()\n    rank_expanded = rank_expanded.interpolate()\n    return df_expanded, rank_expanded\n\ndf_expanded, rank_expanded = prepare_data(clean_df)\n\n\n\n\n\n\nRetrieve the flag data and utilize its country code to load flag images at a later stage in the project.\n\n\nShow the code\n# use it to read flag.png with 2 letter code\nflagcode = pd.read_csv(\"data.csv\")\nflagcode = flagcode._append([{'Name': 'England', 'Code': 'GB'},\n                             {'Name': 'Republic of Ireland', 'Code': 'IE'}, \n                             {'Name': 'Russia', 'Code': 'RU'},\n                             {'Name': 'Yugoslavia', 'Code': 'YU'},\n                             {'Name': 'USA', 'Code': 'US'},\n                             {'Name': 'Wales', 'Code': 'GB-WLS'}], ignore_index = True)\n\n# create colour map for the countries\ncols = plt.cm.tab20.colors + plt.cm.Dark2.colors + plt.cm.Set3.colors + plt.cm.tab20b.colors\nm = zip(np.array(clean_df.columns), cols)\ncolor_dict = {x[0]:x[1] for x in m}"
  },
  {
    "objectID": "visuals/prace/index.html#bar-chart-race",
    "href": "visuals/prace/index.html#bar-chart-race",
    "title": "Top FIFA national teams over the years",
    "section": "Bar chart race:",
    "text": "Bar chart race:\nInitially, plot a single frame to fine-tune its visual appearance and ensure it looks polished before proceeding with the rest. Also, examine first few frames to check the animation.\n\n\nShow the code\ni = 0\nys = rank_expanded.iloc[i][rank_expanded.iloc[i] != 0].sort_values().values[:10]\nwidths = df_expanded.iloc[i].sort_values(ascending=False).values[:10]\nlabels = df_expanded.iloc[i].sort_values(ascending=False).keys().values[:10]\n# Create a horizontal bar plot\nfig, ax = plt.subplots(nrows = 1,\n                       ncols = 1,\n                       figsize = (8, 5),\n                       facecolor = '#fefefe',\n                       dpi = 144)\nbars = ax.barh(y=ys, width=widths, edgecolor = \"#f7f7fa\",\n               color=[color_dict[country] for country in labels], height=0.8,\n               tick_label=[x.rjust(5, ' ') for x in labels], \n               alpha=0.8,)\nax.set_xlabel('Points', fontsize=14)\ndate = datetime.strptime(df_expanded.iloc[i].name, \"%Y-%m-%d\")\nax.set_title(f'Top 10 FIFA teams over the years', fontsize=20, fontweight = 'bold')\nplt.text(0.5, 0.5, date.strftime(\"%B %Y\"), ha='center', va='center', fontsize=25, \n          transform=ax.transAxes)\nfor bar in bars:\n        width = bar.get_width()\n        ax.annotate(f'{width:.0F}',\n                    xy = (width, bar.get_y() + bar.get_height() / 2),\n                    xytext = (-20, 0),\n                    textcoords = \"offset points\",\n                    fontsize = 12,\n                    ha = 'right',\n                    va = 'center')\n\n# use it to set flag to the right of the bars\ndef offset_image(coord, name, ax):\n    img = get_flag(name)\n    im = OffsetImage(img, zoom=0.5, alpha=0.8)\n    im.image.axes = ax\n    # Adjust x-coordinate to place the flag to the right end of the bar\n    index = np.where(labels == name)[0][0]\n    value = widths[index]\n    ab = AnnotationBbox(im, (value, coord), xybox=(0, 0), frameon=False,\n                        xycoords='data',  boxcoords=\"offset points\", pad=0)\n    ax.add_artist(ab)\n\n# get flag from folder\ndef get_flag(x):\n    fc = flagcode[flagcode['Name'] == x][\"Code\"].values[0]\n    img = plt.imread(f'./flags/{fc}.png')\n    return img\n\n# do it for every country \nfor label, y_value in zip(labels, ys):\n    offset_image(y_value, label, ax)\n\n# changes for visual\nax.invert_yaxis()\n[spine.set_visible(False) for spine in ax.spines.values()]\nax.tick_params(labelsize=8, length=0)\nax.tick_params(axis='x', labelsize=12)  \nax.tick_params(axis='y', labelsize=12,)\nax.set_facecolor('#eaeaf2')\nax.grid(True, color='white')\nax.set_axisbelow(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nComplete the bar plot race by adding transitions between frames, and ensuring a smooth and visually appealing presentation of the data.\n\n\nShow the code\ndef update(i, n=10):\n    ax.clear()\n    \n    for bar in ax.containers:\n        bar.remove()\n        \n    ys = rank_expanded.iloc[i][rank_expanded.iloc[i] != 0].sort_values().values[:n]\n    widths = df_expanded.iloc[i].sort_values(ascending=False).values[:n]\n    labels = df_expanded.iloc[i].sort_values(ascending=False).keys().values[:n]\n    bars = ax.barh(y = ys, height=0.8,\n                   width = widths,\n                   color=[color_dict[country] for country in labels], \n                   tick_label=[x.rjust(5, ' ') for x in labels], \n                   alpha = 0.8,\n                   edgecolor = \"#f7f7fa\",\n                  )\n    ax.set_xlabel('Points', fontsize=14)\n    date = datetime.strptime(df_expanded.iloc[i].name, \"%Y-%m-%d\") \n    ax.set_title(f'Top {n} FIFA teams over the years', fontsize=20, fontweight = 'bold')\n    ax.text(0.5, 0.5, date.strftime(\"%B %Y\"), ha='center', va='center', fontsize=25, \n         fontweight='bold', transform=ax.transAxes)\n    ax.tick_params(axis='x', labelsize=12)  \n    ax.tick_params(axis='y', labelsize=12)  \n    ax.set_axisbelow(True)\n    \n    # use it to set flag to the right of the bars\n    for bar in bars:\n        width = bar.get_width()\n        ax.annotate(f'{width:.0F}',\n                    xy = (width, bar.get_y() + bar.get_height() / 2),\n                    xytext = (-20, 0),\n                    textcoords = \"offset points\",\n                    fontsize = 12,\n                    ha = 'right',\n                    va = 'center')\n        \n    def offset_image(coord, name, bx):\n        img = get_flag(name)\n        im = OffsetImage(img, zoom=0.5, alpha=0.8)\n        im.image.axes = bx\n        # Adjust x-coordinate to place the flag to the right of the right end of the bar\n        index = np.where(labels == name)[0][0]\n        value = widths[index]\n        ab = AnnotationBbox(im, (value, coord),  xybox=(0, 0), frameon=False,\n                        xycoords='data',  boxcoords=\"offset points\", pad=0)\n        bx.add_artist(ab)\n        \n    # get flag from folder\n    def get_flag(x):\n        fc = flagcode[flagcode['Name'] == x][\"Code\"].values[0]\n        img = plt.imread(f'./flags/{fc}.png')\n        return img\n    \n    # do it for every country \n    for label, y_value in zip(labels, ys):\n        offset_image(y_value, label, ax)\n    \n    # improve visuals\n    ax.invert_yaxis()\n    [spine.set_visible(False) for spine in ax.spines.values()]\n    ax.grid(axis='x')\n    ax.grid(True, axis='x', color='white')\n    ax.tick_params(labelsize=8, length=0)\n    ax.tick_params(axis='x', labelsize=12)  \n    ax.tick_params(axis='y', labelsize=12,)\n    ax.set_facecolor('#eaeaf2')\n\n# changes for visual\nfig = plt.Figure(figsize=(10, 6), facecolor = '#fefefe')\nax = fig.add_subplot()\nu5mr_anim = anim.FuncAnimation(\n    fig = fig,\n    func = update,\n    frames = len(rank_expanded),\n    interval = 70)\nu5mr_anim.save('FIFA_world.gif')"
  },
  {
    "objectID": "visuals/prace/index.html#result",
    "href": "visuals/prace/index.html#result",
    "title": "Top FIFA national teams over the years",
    "section": "Result:",
    "text": "Result:\n\n\n\nBarChartRace"
  },
  {
    "objectID": "index.html#the-only-way-to-do-great-work-is-to-love-what-you-do.",
    "href": "index.html#the-only-way-to-do-great-work-is-to-love-what-you-do.",
    "title": "Arnab Das",
    "section": "The only way to do great work is to love what you do.",
    "text": "The only way to do great work is to love what you do."
  }
]